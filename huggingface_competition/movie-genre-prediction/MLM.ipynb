{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0138951d-9cc7-46c3-8743-2012c1dec11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34a56b30-6e9c-4343-a0cd-91d4d2255bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.23.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.28.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.12.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2023.1.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.13.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.3.5.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.5.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate) (23.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.70.13)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (5.4.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2019.11.28)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (18.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.14.0)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d131b07-e2d1-4c5e-a352-c5769f52331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_TXT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "074e5400-f278-4ee4-be3b-54de9852e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = pd.read_csv(\"data/movies_train.csv\")\n",
    "df_test = pd.read_csv(\"data/movies_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d4947c-0478-4454-badb-2b135b6349e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0a6117d-a15f-43e7-a6bb-adfb352d1430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89500 500\n"
     ]
    }
   ],
   "source": [
    "if SAVE_TXT:\n",
    "    df_tr[\"concat\"] = df_tr.apply(lambda x: f\"Title:{x.title} Text:{x.text}\", axis=1)\n",
    "    df_test[\"concat\"] = df_test.apply(lambda x: f\"Title:{x.title} Text:{x.text}\", axis=1)\n",
    "\n",
    "    ids = np.random.choice(list(range(len(df_tr))), size=500, replace=False)\n",
    "    ids.sort()\n",
    "\n",
    "    ids_tr = list(set(list(range(len(df_tr)))) - set(ids))\n",
    "\n",
    "    text_tr = df_tr.iloc[ids_tr].concat.values.tolist() + df_test.concat.values.tolist()\n",
    "    text_dev = df_tr.iloc[ids].concat.values.tolist()\n",
    "\n",
    "    print(len(text_tr), len(text_dev))\n",
    "\n",
    "\n",
    "    with open(\"data/movies_text_tr.txt\", \"w\") as f:\n",
    "        for t in text_tr:\n",
    "            f.write(t + '\\n')\n",
    "\n",
    "    with open(\"data/movies_text_dev.txt\", \"w\") as f:\n",
    "        for t in text_dev:\n",
    "            f.write(t + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b966d-a065-48ec-a08c-eda40c5f498e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d5341d9-5205-4180-a67f-f44f6ec63f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "07/17/2023 14:57:23 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "07/17/2023 14:57:23 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=steps,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=True,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=roberta-large-movies/runs/Jul17_14-57-22_ndom6pkvsr,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_accuracy,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=30.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=roberta-large-movies,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=roberta-large-movies,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "07/17/2023 14:57:23 - WARNING - datasets.builder - Using custom data configuration default-64609a9a3d747a5c\n",
      "07/17/2023 14:57:23 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-64609a9a3d747a5c/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad)\n",
      "Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-64609a9a3d747a5c/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad...\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 11351.30it/s]\n",
      "07/17/2023 14:57:23 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "07/17/2023 14:57:23 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|█████████████████████| 2/2 [00:00<00:00, 199.19it/s]\n",
      "07/17/2023 14:57:23 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
      "07/17/2023 14:57:23 - INFO - datasets.builder - Generating train split\n",
      "07/17/2023 14:57:23 - INFO - datasets.builder - Generating validation split\n",
      "07/17/2023 14:57:23 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-64609a9a3d747a5c/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1015.08it/s]\n",
      "[INFO|hub.py:600] 2023-07-17 14:57:23,550 >> https://huggingface.co/roberta-large/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmps_9mm6ry\n",
      "Downloading config.json: 100%|██████████████████| 482/482 [00:00<00:00, 551kB/s]\n",
      "[INFO|hub.py:613] 2023-07-17 14:57:23,592 >> storing https://huggingface.co/roberta-large/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
      "[INFO|hub.py:621] 2023-07-17 14:57:23,592 >> creating metadata file for /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
      "[INFO|configuration_utils.py:681] 2023-07-17 14:57:23,593 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
      "[INFO|configuration_utils.py:730] 2023-07-17 14:57:23,594 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:404] 2023-07-17 14:57:23,658 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:681] 2023-07-17 14:57:23,706 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
      "[INFO|configuration_utils.py:730] 2023-07-17 14:57:23,707 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|hub.py:600] 2023-07-17 14:57:23,799 >> https://huggingface.co/roberta-large/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpysjateup\n",
      "Downloading vocab.json: 100%|████████████████| 878k/878k [00:00<00:00, 24.3MB/s]\n",
      "[INFO|hub.py:613] 2023-07-17 14:57:23,881 >> storing https://huggingface.co/roberta-large/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|hub.py:621] 2023-07-17 14:57:23,881 >> creating metadata file for /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|hub.py:600] 2023-07-17 14:57:23,923 >> https://huggingface.co/roberta-large/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0dnzwj3n\n",
      "Downloading merges.txt: 100%|████████████████| 446k/446k [00:00<00:00, 19.3MB/s]\n",
      "[INFO|hub.py:613] 2023-07-17 14:57:23,991 >> storing https://huggingface.co/roberta-large/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|hub.py:621] 2023-07-17 14:57:23,991 >> creating metadata file for /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|hub.py:600] 2023-07-17 14:57:24,037 >> https://huggingface.co/roberta-large/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmph4npy4y6\n",
      "Downloading tokenizer.json: 100%|██████████| 1.29M/1.29M [00:00<00:00, 16.1MB/s]\n",
      "[INFO|hub.py:613] 2023-07-17 14:57:24,166 >> storing https://huggingface.co/roberta-large/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|hub.py:621] 2023-07-17 14:57:24,166 >> creating metadata file for /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1803] 2023-07-17 14:57:24,301 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1803] 2023-07-17 14:57:24,302 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1803] 2023-07-17 14:57:24,302 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1803] 2023-07-17 14:57:24,302 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1803] 2023-07-17 14:57:24,302 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1803] 2023-07-17 14:57:24,302 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:681] 2023-07-17 14:57:24,457 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
      "[INFO|configuration_utils.py:730] 2023-07-17 14:57:24,458 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|hub.py:600] 2023-07-17 14:57:24,606 >> https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_6ls5vey\n",
      "Downloading pytorch_model.bin: 100%|███████| 1.33G/1.33G [00:15<00:00, 94.7MB/s]\n",
      "[INFO|hub.py:613] 2023-07-17 14:57:39,698 >> storing https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
      "[INFO|hub.py:621] 2023-07-17 14:57:39,698 >> creating metadata file for /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
      "[INFO|modeling_utils.py:2041] 2023-07-17 14:57:39,698 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
      "[INFO|modeling_utils.py:2435] 2023-07-17 14:57:43,867 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "[INFO|modeling_utils.py:2443] 2023-07-17 14:57:43,867 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "Running tokenizer on dataset line_by_line:   0%|         | 0/90 [00:00<?, ?ba/s]07/17/2023 14:57:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-64609a9a3d747a5c/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-7ba14049e07d0cc9.arrow\n",
      "Running tokenizer on dataset line_by_line: 100%|█| 90/90 [00:04<00:00, 19.77ba/s\n",
      "Running tokenizer on dataset line_by_line:   0%|          | 0/1 [00:00<?, ?ba/s]07/17/2023 14:57:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-64609a9a3d747a5c/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-47375e1c672b6ec6.arrow\n",
      "Running tokenizer on dataset line_by_line: 100%|██| 1/1 [00:00<00:00, 38.42ba/s]\n",
      "07/17/2023 14:57:49 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.4.0/metrics/accuracy/accuracy.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp8wsr99u6\n",
      "Downloading builder script: 4.21kB [00:00, 4.01MB/s]                            \n",
      "07/17/2023 14:57:49 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.4.0/metrics/accuracy/accuracy.py in cache at /root/.cache/huggingface/datasets/downloads/7d5afacc2f0ddac3c52fcc7ab3d5537069e8ee349ad261467426c0ad809c4eaa.32b3507481ea2e26fd6a2b34c9976e9da377302faaf35089eb1cd971d41bb0ff.py\n",
      "07/17/2023 14:57:49 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/7d5afacc2f0ddac3c52fcc7ab3d5537069e8ee349ad261467426c0ad809c4eaa.32b3507481ea2e26fd6a2b34c9976e9da377302faaf35089eb1cd971d41bb0ff.py\n",
      "[INFO|trainer.py:555] 2023-07-17 14:57:50,080 >> Using cuda_amp half precision backend\n",
      "[INFO|trainer.py:722] 2023-07-17 14:57:50,082 >> The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1605] 2023-07-17 14:57:50,091 >> ***** Running training *****\n",
      "[INFO|trainer.py:1606] 2023-07-17 14:57:50,091 >>   Num examples = 89500\n",
      "[INFO|trainer.py:1607] 2023-07-17 14:57:50,091 >>   Num Epochs = 30\n",
      "[INFO|trainer.py:1608] 2023-07-17 14:57:50,091 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1609] 2023-07-17 14:57:50,091 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1610] 2023-07-17 14:57:50,091 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1611] 2023-07-17 14:57:50,091 >>   Total optimization steps = 83910\n",
      "{'loss': 1.7698, 'learning_rate': 4.970504111548088e-05, 'epoch': 0.18}         \n",
      "  1%|▏                                    | 500/83910 [01:31<4:24:43,  5.25it/s][INFO|trainer.py:722] 2023-07-17 14:59:21,712 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 14:59:21,714 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 14:59:21,714 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 14:59:21,714 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 47.00it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 43.62it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 40.72it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 39.38it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 38.78it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 39.74it/s]\u001b[A07/17/2023 14:59:22 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.6167851686477661, 'eval_accuracy': 0.6738421395955643, 'eval_runtime': 0.8246, 'eval_samples_per_second': 606.37, 'eval_steps_per_second': 38.808, 'epoch': 0.18}\n",
      "  1%|▏                                    | 500/83910 [01:32<4:24:43,  5.25it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 39.74it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 14:59:22,540 >> Saving model checkpoint to roberta-large-movies/checkpoint-500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 14:59:22,541 >> Configuration saved in roberta-large-movies/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 14:59:24,199 >> Model weights saved in roberta-large-movies/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 14:59:24,200 >> tokenizer config file saved in roberta-large-movies/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 14:59:24,200 >> Special tokens file saved in roberta-large-movies/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 1.7761, 'learning_rate': 4.94082946013586e-05, 'epoch': 0.36}          \n",
      "  1%|▍                                   | 1000/83910 [03:08<4:15:50,  5.40it/s][INFO|trainer.py:722] 2023-07-17 15:00:58,851 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:00:58,852 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:00:58,852 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:00:58,852 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.45it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.27it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.33it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.11it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.23it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.44it/s]\u001b[A07/17/2023 15:00:59 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.6522468328475952, 'eval_accuracy': 0.6829508196721311, 'eval_runtime': 0.7873, 'eval_samples_per_second': 635.049, 'eval_steps_per_second': 40.643, 'epoch': 0.36}\n",
      "  1%|▍                                   | 1000/83910 [03:09<4:15:50,  5.40it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.44it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:00:59,642 >> Saving model checkpoint to roberta-large-movies/checkpoint-1000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:00:59,643 >> Configuration saved in roberta-large-movies/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:01:01,476 >> Model weights saved in roberta-large-movies/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:01:01,477 >> tokenizer config file saved in roberta-large-movies/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:01:01,478 >> Special tokens file saved in roberta-large-movies/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 1.7626, 'learning_rate': 4.9110356334167565e-05, 'epoch': 0.54}        \n",
      "  2%|▋                                   | 1500/83910 [04:46<4:05:34,  5.59it/s][INFO|trainer.py:722] 2023-07-17 15:02:36,462 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:02:36,463 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:02:36,463 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:02:36,463 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.41it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.09it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.22it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.23it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.51it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.62it/s]\u001b[A07/17/2023 15:02:37 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.6534239053726196, 'eval_accuracy': 0.6660117878192534, 'eval_runtime': 0.7869, 'eval_samples_per_second': 635.425, 'eval_steps_per_second': 40.667, 'epoch': 0.54}\n",
      "  2%|▋                                   | 1500/83910 [04:47<4:05:34,  5.59it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.62it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:02:37,252 >> Saving model checkpoint to roberta-large-movies/checkpoint-1500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:02:37,253 >> Configuration saved in roberta-large-movies/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:02:39,038 >> Model weights saved in roberta-large-movies/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:02:39,039 >> tokenizer config file saved in roberta-large-movies/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:02:39,039 >> Special tokens file saved in roberta-large-movies/checkpoint-1500/special_tokens_map.json\n",
      "{'loss': 1.7602, 'learning_rate': 4.8812418066976524e-05, 'epoch': 0.72}        \n",
      "  2%|▊                                   | 2000/83910 [06:23<4:14:48,  5.36it/s][INFO|trainer.py:722] 2023-07-17 15:04:14,039 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:04:14,040 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:04:14,040 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:04:14,040 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.33it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.31it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.43it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.48it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.72it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.56it/s]\u001b[A07/17/2023 15:04:14 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.6575504541397095, 'eval_accuracy': 0.6787299419597133, 'eval_runtime': 0.7882, 'eval_samples_per_second': 634.385, 'eval_steps_per_second': 40.601, 'epoch': 0.72}\n",
      "  2%|▊                                   | 2000/83910 [06:24<4:14:48,  5.36it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.56it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:04:14,830 >> Saving model checkpoint to roberta-large-movies/checkpoint-2000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:04:14,831 >> Configuration saved in roberta-large-movies/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:04:16,637 >> Model weights saved in roberta-large-movies/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:04:16,638 >> tokenizer config file saved in roberta-large-movies/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:04:16,638 >> Special tokens file saved in roberta-large-movies/checkpoint-2000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:04:20,147 >> Deleting older checkpoint [roberta-large-movies/checkpoint-500] due to args.save_total_limit\n",
      "{'loss': 1.7587, 'learning_rate': 4.851447979978549e-05, 'epoch': 0.89}         \n",
      "  3%|█                                   | 2500/83910 [08:00<4:00:21,  5.64it/s][INFO|trainer.py:722] 2023-07-17 15:05:50,990 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:05:50,991 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:05:50,992 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:05:50,992 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.21it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.65it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.78it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.37it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.43it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.18it/s]\u001b[A07/17/2023 15:05:51 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.6266298294067383, 'eval_accuracy': 0.6772697150430749, 'eval_runtime': 0.7893, 'eval_samples_per_second': 633.509, 'eval_steps_per_second': 40.545, 'epoch': 0.89}\n",
      "  3%|█                                   | 2500/83910 [08:01<4:00:21,  5.64it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.18it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:05:51,783 >> Saving model checkpoint to roberta-large-movies/checkpoint-2500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:05:51,783 >> Configuration saved in roberta-large-movies/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:05:53,491 >> Model weights saved in roberta-large-movies/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:05:53,491 >> tokenizer config file saved in roberta-large-movies/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:05:53,492 >> Special tokens file saved in roberta-large-movies/checkpoint-2500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:05:57,381 >> Deleting older checkpoint [roberta-large-movies/checkpoint-1500] due to args.save_total_limit\n",
      "{'loss': 1.7047, 'learning_rate': 4.821654153259445e-05, 'epoch': 1.07}         \n",
      "  4%|█▎                                  | 3000/83910 [09:39<4:16:00,  5.27it/s][INFO|trainer.py:722] 2023-07-17 15:07:29,560 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:07:29,562 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:07:29,562 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:07:29,562 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 43.41it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 42.85it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.15it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.52it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.89it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 40.90it/s]\u001b[A07/17/2023 15:07:30 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.605985164642334, 'eval_accuracy': 0.6851971557853911, 'eval_runtime': 0.8181, 'eval_samples_per_second': 611.179, 'eval_steps_per_second': 39.115, 'epoch': 1.07}\n",
      "  4%|█▎                                  | 3000/83910 [09:40<4:16:00,  5.27it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.90it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:07:30,382 >> Saving model checkpoint to roberta-large-movies/checkpoint-3000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:07:30,383 >> Configuration saved in roberta-large-movies/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:07:32,073 >> Model weights saved in roberta-large-movies/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:07:32,074 >> tokenizer config file saved in roberta-large-movies/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:07:32,074 >> Special tokens file saved in roberta-large-movies/checkpoint-3000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:07:35,961 >> Deleting older checkpoint [roberta-large-movies/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 1.6782, 'learning_rate': 4.791860326540341e-05, 'epoch': 1.25}         \n",
      "  4%|█▌                                  | 3500/83910 [11:17<4:08:43,  5.39it/s][INFO|trainer.py:722] 2023-07-17 15:09:07,960 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:09:07,961 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:09:07,962 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:09:07,962 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.23it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 42.86it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 40.45it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 40.81it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.08it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 40.33it/s]\u001b[A07/17/2023 15:09:08 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.599035382270813, 'eval_accuracy': 0.6906354515050167, 'eval_runtime': 0.8184, 'eval_samples_per_second': 610.967, 'eval_steps_per_second': 39.102, 'epoch': 1.25}\n",
      "  4%|█▌                                  | 3500/83910 [11:18<4:08:43,  5.39it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.33it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:09:08,782 >> Saving model checkpoint to roberta-large-movies/checkpoint-3500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:09:08,783 >> Configuration saved in roberta-large-movies/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:09:10,522 >> Model weights saved in roberta-large-movies/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:09:10,523 >> tokenizer config file saved in roberta-large-movies/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:09:10,523 >> Special tokens file saved in roberta-large-movies/checkpoint-3500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:09:14,550 >> Deleting older checkpoint [roberta-large-movies/checkpoint-2000] due to args.save_total_limit\n",
      "{'loss': 1.6733, 'learning_rate': 4.7620664998212375e-05, 'epoch': 1.43}        \n",
      "  5%|█▋                                  | 4000/83910 [12:55<4:00:35,  5.54it/s][INFO|trainer.py:722] 2023-07-17 15:10:46,075 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:10:46,076 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:10:46,076 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:10:46,076 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.48it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.25it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.59it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.44it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 38.98it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 38.18it/s]\u001b[A07/17/2023 15:10:46 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.5377483367919922, 'eval_accuracy': 0.6967426710097719, 'eval_runtime': 0.819, 'eval_samples_per_second': 610.521, 'eval_steps_per_second': 39.073, 'epoch': 1.43}\n",
      "  5%|█▋                                  | 4000/83910 [12:56<4:00:35,  5.54it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.18it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:10:46,897 >> Saving model checkpoint to roberta-large-movies/checkpoint-4000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:10:46,898 >> Configuration saved in roberta-large-movies/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:10:48,584 >> Model weights saved in roberta-large-movies/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:10:48,585 >> tokenizer config file saved in roberta-large-movies/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:10:48,585 >> Special tokens file saved in roberta-large-movies/checkpoint-4000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:10:52,183 >> Deleting older checkpoint [roberta-large-movies/checkpoint-2500] due to args.save_total_limit\n",
      "{'loss': 1.6664, 'learning_rate': 4.7322726731021334e-05, 'epoch': 1.61}        \n",
      "  5%|█▉                                  | 4500/83910 [14:33<3:56:44,  5.59it/s][INFO|trainer.py:722] 2023-07-17 15:12:23,343 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:12:23,345 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:12:23,345 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:12:23,345 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.13it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.05it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.27it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.10it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.43it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.56it/s]\u001b[A07/17/2023 15:12:24 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.6434643268585205, 'eval_accuracy': 0.6746607762701168, 'eval_runtime': 0.7966, 'eval_samples_per_second': 627.631, 'eval_steps_per_second': 40.168, 'epoch': 1.61}\n",
      "  5%|█▉                                  | 4500/83910 [14:34<3:56:44,  5.59it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.56it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:12:24,147 >> Saving model checkpoint to roberta-large-movies/checkpoint-4500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:12:24,150 >> Configuration saved in roberta-large-movies/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:12:25,876 >> Model weights saved in roberta-large-movies/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:12:25,877 >> tokenizer config file saved in roberta-large-movies/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:12:25,877 >> Special tokens file saved in roberta-large-movies/checkpoint-4500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:12:29,606 >> Deleting older checkpoint [roberta-large-movies/checkpoint-3000] due to args.save_total_limit\n",
      "{'loss': 1.6719, 'learning_rate': 4.70247884638303e-05, 'epoch': 1.79}          \n",
      "  6%|██▏                                 | 5000/83910 [16:11<3:55:10,  5.59it/s][INFO|trainer.py:722] 2023-07-17 15:14:01,805 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:14:01,807 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:14:01,807 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:14:01,807 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 46.71it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.64it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.28it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.98it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.40it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.66it/s]\u001b[A07/17/2023 15:14:02 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.483905553817749, 'eval_accuracy': 0.6907181571815718, 'eval_runtime': 0.7989, 'eval_samples_per_second': 625.841, 'eval_steps_per_second': 40.054, 'epoch': 1.79}\n",
      "  6%|██▏                                 | 5000/83910 [16:12<3:55:10,  5.59it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.66it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:14:02,608 >> Saving model checkpoint to roberta-large-movies/checkpoint-5000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:14:02,609 >> Configuration saved in roberta-large-movies/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:14:04,293 >> Model weights saved in roberta-large-movies/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:14:04,294 >> tokenizer config file saved in roberta-large-movies/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:14:04,294 >> Special tokens file saved in roberta-large-movies/checkpoint-5000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:14:08,078 >> Deleting older checkpoint [roberta-large-movies/checkpoint-3500] due to args.save_total_limit\n",
      "{'loss': 1.6502, 'learning_rate': 4.672685019663926e-05, 'epoch': 1.97}         \n",
      "  7%|██▎                                 | 5500/83910 [17:49<4:04:01,  5.36it/s][INFO|trainer.py:722] 2023-07-17 15:15:39,970 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:15:39,972 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:15:39,972 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:15:39,972 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 47.94it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 43.67it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 40.00it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 38.87it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 38.72it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 39.77it/s]\u001b[A07/17/2023 15:15:40 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.535127878189087, 'eval_accuracy': 0.6896661367249602, 'eval_runtime': 0.823, 'eval_samples_per_second': 607.558, 'eval_steps_per_second': 38.884, 'epoch': 1.97}\n",
      "  7%|██▎                                 | 5500/83910 [17:50<4:04:01,  5.36it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 39.77it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:15:40,797 >> Saving model checkpoint to roberta-large-movies/checkpoint-5500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:15:40,798 >> Configuration saved in roberta-large-movies/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:15:42,463 >> Model weights saved in roberta-large-movies/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:15:42,464 >> tokenizer config file saved in roberta-large-movies/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:15:42,464 >> Special tokens file saved in roberta-large-movies/checkpoint-5500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:15:46,398 >> Deleting older checkpoint [roberta-large-movies/checkpoint-4500] due to args.save_total_limit\n",
      "{'loss': 1.6233, 'learning_rate': 4.642891192944822e-05, 'epoch': 2.15}         \n",
      "  7%|██▌                                 | 6000/83910 [19:27<3:57:35,  5.47it/s][INFO|trainer.py:722] 2023-07-17 15:17:17,533 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:17:17,535 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:17:17,535 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:17:17,535 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.20it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.90it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.31it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.16it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.22it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.53it/s]\u001b[A07/17/2023 15:17:18 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.6817570924758911, 'eval_accuracy': 0.6763219939373526, 'eval_runtime': 0.7881, 'eval_samples_per_second': 634.403, 'eval_steps_per_second': 40.602, 'epoch': 2.15}\n",
      "  7%|██▌                                 | 6000/83910 [19:28<3:57:35,  5.47it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.53it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:17:18,325 >> Saving model checkpoint to roberta-large-movies/checkpoint-6000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:17:18,326 >> Configuration saved in roberta-large-movies/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:17:20,053 >> Model weights saved in roberta-large-movies/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:17:20,053 >> tokenizer config file saved in roberta-large-movies/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:17:20,054 >> Special tokens file saved in roberta-large-movies/checkpoint-6000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:17:24,267 >> Deleting older checkpoint [roberta-large-movies/checkpoint-5000] due to args.save_total_limit\n",
      "{'loss': 1.6127, 'learning_rate': 4.6130973662257184e-05, 'epoch': 2.32}        \n",
      "  8%|██▊                                 | 6500/83910 [21:05<3:54:53,  5.49it/s][INFO|trainer.py:722] 2023-07-17 15:18:55,302 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:18:55,303 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:18:55,303 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:18:55,303 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.87it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.20it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.22it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.02it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.35it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.65it/s]\u001b[A07/17/2023 15:18:56 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.5865211486816406, 'eval_accuracy': 0.685335059889932, 'eval_runtime': 0.787, 'eval_samples_per_second': 635.291, 'eval_steps_per_second': 40.659, 'epoch': 2.32}\n",
      "  8%|██▊                                 | 6500/83910 [21:05<3:54:53,  5.49it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.65it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:18:56,092 >> Saving model checkpoint to roberta-large-movies/checkpoint-6500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:18:56,093 >> Configuration saved in roberta-large-movies/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:18:57,849 >> Model weights saved in roberta-large-movies/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:18:57,850 >> tokenizer config file saved in roberta-large-movies/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:18:57,850 >> Special tokens file saved in roberta-large-movies/checkpoint-6500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:19:01,846 >> Deleting older checkpoint [roberta-large-movies/checkpoint-5500] due to args.save_total_limit\n",
      "{'loss': 1.6274, 'learning_rate': 4.5833035395066143e-05, 'epoch': 2.5}         \n",
      "  8%|███                                 | 7000/83910 [22:42<3:49:34,  5.58it/s][INFO|trainer.py:722] 2023-07-17 15:20:33,019 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:20:33,020 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:20:33,020 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:20:33,020 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.80it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.69it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.97it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.99it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.61it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 40.85it/s]\u001b[A07/17/2023 15:20:33 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.5004233121871948, 'eval_accuracy': 0.7003633961017509, 'eval_runtime': 0.8009, 'eval_samples_per_second': 624.318, 'eval_steps_per_second': 39.956, 'epoch': 2.5}\n",
      "  8%|███                                 | 7000/83910 [22:43<3:49:34,  5.58it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.85it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:20:33,823 >> Saving model checkpoint to roberta-large-movies/checkpoint-7000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:20:33,824 >> Configuration saved in roberta-large-movies/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:20:35,575 >> Model weights saved in roberta-large-movies/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:20:35,575 >> tokenizer config file saved in roberta-large-movies/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:20:35,576 >> Special tokens file saved in roberta-large-movies/checkpoint-7000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:20:39,403 >> Deleting older checkpoint [roberta-large-movies/checkpoint-4000] due to args.save_total_limit\n",
      "{'loss': 1.601, 'learning_rate': 4.553628888094387e-05, 'epoch': 2.68}          \n",
      "  9%|███▏                                | 7500/83910 [24:20<3:39:46,  5.79it/s][INFO|trainer.py:722] 2023-07-17 15:22:10,544 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:22:10,546 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:22:10,546 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:22:10,546 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.16it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.60it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.98it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.99it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.38it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.63it/s]\u001b[A07/17/2023 15:22:11 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.452188491821289, 'eval_accuracy': 0.6929970129439097, 'eval_runtime': 0.7898, 'eval_samples_per_second': 633.056, 'eval_steps_per_second': 40.516, 'epoch': 2.68}\n",
      "  9%|███▏                                | 7500/83910 [24:21<3:39:46,  5.79it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.63it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:22:11,337 >> Saving model checkpoint to roberta-large-movies/checkpoint-7500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:22:11,338 >> Configuration saved in roberta-large-movies/checkpoint-7500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:22:13,096 >> Model weights saved in roberta-large-movies/checkpoint-7500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:22:13,097 >> tokenizer config file saved in roberta-large-movies/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:22:13,097 >> Special tokens file saved in roberta-large-movies/checkpoint-7500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:22:16,975 >> Deleting older checkpoint [roberta-large-movies/checkpoint-6000] due to args.save_total_limit\n",
      "{'loss': 1.6123, 'learning_rate': 4.523835061375284e-05, 'epoch': 2.86}         \n",
      " 10%|███▍                                | 8000/83910 [25:58<3:45:11,  5.62it/s][INFO|trainer.py:722] 2023-07-17 15:23:48,199 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:23:48,200 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:23:48,200 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:23:48,200 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.20it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.61it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.79it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.79it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.15it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 37.65it/s]\u001b[A07/17/2023 15:23:49 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.5370689630508423, 'eval_accuracy': 0.689419795221843, 'eval_runtime': 0.8546, 'eval_samples_per_second': 585.05, 'eval_steps_per_second': 37.443, 'epoch': 2.86}\n",
      " 10%|███▍                                | 8000/83910 [25:58<3:45:11,  5.62it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 37.65it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:23:49,059 >> Saving model checkpoint to roberta-large-movies/checkpoint-8000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:23:49,063 >> Configuration saved in roberta-large-movies/checkpoint-8000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:23:50,759 >> Model weights saved in roberta-large-movies/checkpoint-8000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:23:50,760 >> tokenizer config file saved in roberta-large-movies/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:23:50,760 >> Special tokens file saved in roberta-large-movies/checkpoint-8000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:23:54,653 >> Deleting older checkpoint [roberta-large-movies/checkpoint-6500] due to args.save_total_limit\n",
      "{'loss': 1.6074, 'learning_rate': 4.4940412346561796e-05, 'epoch': 3.04}        \n",
      " 10%|███▋                                | 8500/83910 [27:35<3:50:56,  5.44it/s][INFO|trainer.py:722] 2023-07-17 15:25:26,041 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:25:26,042 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:25:26,042 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:25:26,042 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.61it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.70it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.05it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.16it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.40it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 39.40it/s]\u001b[A07/17/2023 15:25:26 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.5342369079589844, 'eval_accuracy': 0.6952157912345266, 'eval_runtime': 0.8214, 'eval_samples_per_second': 608.68, 'eval_steps_per_second': 38.956, 'epoch': 3.04}\n",
      " 10%|███▋                                | 8500/83910 [27:36<3:50:56,  5.44it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 39.40it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:25:26,867 >> Saving model checkpoint to roberta-large-movies/checkpoint-8500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:25:26,873 >> Configuration saved in roberta-large-movies/checkpoint-8500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:25:28,554 >> Model weights saved in roberta-large-movies/checkpoint-8500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:25:28,555 >> tokenizer config file saved in roberta-large-movies/checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:25:28,555 >> Special tokens file saved in roberta-large-movies/checkpoint-8500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:25:32,330 >> Deleting older checkpoint [roberta-large-movies/checkpoint-7500] due to args.save_total_limit\n",
      "{'loss': 1.563, 'learning_rate': 4.4642474079370755e-05, 'epoch': 3.22}         \n",
      " 11%|███▊                                | 9000/83910 [29:13<3:49:55,  5.43it/s][INFO|trainer.py:722] 2023-07-17 15:27:03,541 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:27:03,542 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:27:03,543 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:27:03,543 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.97it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.36it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.28it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.15it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.48it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 37.26it/s]\u001b[A07/17/2023 15:27:04 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.568178415298462, 'eval_accuracy': 0.6875834445927904, 'eval_runtime': 0.8488, 'eval_samples_per_second': 589.06, 'eval_steps_per_second': 37.7, 'epoch': 3.22}\n",
      " 11%|███▊                                | 9000/83910 [29:14<3:49:55,  5.43it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 37.26it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:27:04,401 >> Saving model checkpoint to roberta-large-movies/checkpoint-9000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:27:04,420 >> Configuration saved in roberta-large-movies/checkpoint-9000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:27:06,102 >> Model weights saved in roberta-large-movies/checkpoint-9000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:27:06,103 >> tokenizer config file saved in roberta-large-movies/checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:27:06,104 >> Special tokens file saved in roberta-large-movies/checkpoint-9000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:27:09,758 >> Deleting older checkpoint [roberta-large-movies/checkpoint-8000] due to args.save_total_limit\n",
      "{'loss': 1.5746, 'learning_rate': 4.4344535812179714e-05, 'epoch': 3.4}         \n",
      " 11%|████                                | 9500/83910 [30:51<3:42:05,  5.58it/s][INFO|trainer.py:722] 2023-07-17 15:28:41,468 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:28:41,469 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:28:41,469 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:28:41,469 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.10it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.59it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 37.19it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:00<00:00, 35.69it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:00<00:00, 36.59it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 38.38it/s]\u001b[A07/17/2023 15:28:42 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.5704632997512817, 'eval_accuracy': 0.6957663275352806, 'eval_runtime': 0.852, 'eval_samples_per_second': 586.84, 'eval_steps_per_second': 37.558, 'epoch': 3.4}\n",
      " 11%|████                                | 9500/83910 [30:52<3:42:05,  5.58it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.38it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:28:42,323 >> Saving model checkpoint to roberta-large-movies/checkpoint-9500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:28:42,325 >> Configuration saved in roberta-large-movies/checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:28:43,993 >> Model weights saved in roberta-large-movies/checkpoint-9500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:28:43,993 >> tokenizer config file saved in roberta-large-movies/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:28:43,994 >> Special tokens file saved in roberta-large-movies/checkpoint-9500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:28:47,775 >> Deleting older checkpoint [roberta-large-movies/checkpoint-8500] due to args.save_total_limit\n",
      "{'loss': 1.5539, 'learning_rate': 4.404778929805745e-05, 'epoch': 3.58}         \n",
      " 12%|████▏                              | 10000/83910 [32:29<3:45:21,  5.47it/s][INFO|trainer.py:722] 2023-07-17 15:30:19,191 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:30:19,192 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:30:19,192 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:30:19,192 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.32it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.32it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.90it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.96it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 38.53it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 37.22it/s]\u001b[A07/17/2023 15:30:20 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4710707664489746, 'eval_accuracy': 0.7040711597673623, 'eval_runtime': 0.85, 'eval_samples_per_second': 588.248, 'eval_steps_per_second': 37.648, 'epoch': 3.58}\n",
      " 12%|████▏                              | 10000/83910 [32:29<3:45:21,  5.47it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 37.22it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:30:20,044 >> Saving model checkpoint to roberta-large-movies/checkpoint-10000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:30:20,046 >> Configuration saved in roberta-large-movies/checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:30:21,761 >> Model weights saved in roberta-large-movies/checkpoint-10000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:30:21,762 >> tokenizer config file saved in roberta-large-movies/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:30:21,762 >> Special tokens file saved in roberta-large-movies/checkpoint-10000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:30:25,689 >> Deleting older checkpoint [roberta-large-movies/checkpoint-7000] due to args.save_total_limit\n",
      "{'loss': 1.578, 'learning_rate': 4.374985103086641e-05, 'epoch': 3.75}          \n",
      " 13%|████▍                              | 10500/83910 [34:07<3:41:36,  5.52it/s][INFO|trainer.py:722] 2023-07-17 15:31:57,217 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:31:57,218 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:31:57,218 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:31:57,218 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.44it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.88it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.35it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.81it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.99it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 32.88it/s]\u001b[A07/17/2023 15:31:58 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.5465725660324097, 'eval_accuracy': 0.6888888888888889, 'eval_runtime': 0.8902, 'eval_samples_per_second': 561.645, 'eval_steps_per_second': 35.945, 'epoch': 3.75}\n",
      " 13%|████▍                              | 10500/83910 [34:07<3:41:36,  5.52it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 32.88it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:31:58,110 >> Saving model checkpoint to roberta-large-movies/checkpoint-10500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:31:58,111 >> Configuration saved in roberta-large-movies/checkpoint-10500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:31:59,786 >> Model weights saved in roberta-large-movies/checkpoint-10500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:31:59,787 >> tokenizer config file saved in roberta-large-movies/checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:31:59,787 >> Special tokens file saved in roberta-large-movies/checkpoint-10500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:32:03,672 >> Deleting older checkpoint [roberta-large-movies/checkpoint-9000] due to args.save_total_limit\n",
      "{'loss': 1.5492, 'learning_rate': 4.345191276367537e-05, 'epoch': 3.93}         \n",
      " 13%|████▌                              | 11000/83910 [35:44<3:35:35,  5.64it/s][INFO|trainer.py:722] 2023-07-17 15:33:35,121 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:33:35,122 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:33:35,122 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:33:35,122 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.54it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.18it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.76it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.67it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 38.34it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 37.27it/s]\u001b[A07/17/2023 15:33:35 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4628891944885254, 'eval_accuracy': 0.6968894771674388, 'eval_runtime': 0.8368, 'eval_samples_per_second': 597.487, 'eval_steps_per_second': 38.239, 'epoch': 3.93}\n",
      " 13%|████▌                              | 11000/83910 [35:45<3:35:35,  5.64it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 37.27it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:33:35,960 >> Saving model checkpoint to roberta-large-movies/checkpoint-11000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:33:35,961 >> Configuration saved in roberta-large-movies/checkpoint-11000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:33:37,685 >> Model weights saved in roberta-large-movies/checkpoint-11000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:33:37,685 >> tokenizer config file saved in roberta-large-movies/checkpoint-11000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:33:37,685 >> Special tokens file saved in roberta-large-movies/checkpoint-11000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:33:41,489 >> Deleting older checkpoint [roberta-large-movies/checkpoint-9500] due to args.save_total_limit\n",
      "{'loss': 1.5291, 'learning_rate': 4.3153974496484326e-05, 'epoch': 4.11}        \n",
      " 14%|████▊                              | 11500/83910 [37:22<3:34:13,  5.63it/s][INFO|trainer.py:722] 2023-07-17 15:35:13,029 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:35:13,031 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:35:13,031 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:35:13,031 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.03it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.86it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.95it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 34.09it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 33.62it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 36.14it/s]\u001b[A07/17/2023 15:35:13 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4264894723892212, 'eval_accuracy': 0.7200132538104705, 'eval_runtime': 0.8798, 'eval_samples_per_second': 568.319, 'eval_steps_per_second': 36.372, 'epoch': 4.11}\n",
      " 14%|████▊                              | 11500/83910 [37:23<3:34:13,  5.63it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 36.14it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:35:13,912 >> Saving model checkpoint to roberta-large-movies/checkpoint-11500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:35:13,913 >> Configuration saved in roberta-large-movies/checkpoint-11500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:35:15,620 >> Model weights saved in roberta-large-movies/checkpoint-11500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:35:15,620 >> tokenizer config file saved in roberta-large-movies/checkpoint-11500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:35:15,621 >> Special tokens file saved in roberta-large-movies/checkpoint-11500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:35:19,397 >> Deleting older checkpoint [roberta-large-movies/checkpoint-10000] due to args.save_total_limit\n",
      "{'loss': 1.5079, 'learning_rate': 4.285603622929329e-05, 'epoch': 4.29}         \n",
      " 14%|█████                              | 12000/83910 [39:00<3:49:55,  5.21it/s][INFO|trainer.py:722] 2023-07-17 15:36:50,913 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:36:50,914 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:36:50,914 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:36:50,914 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.59it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.44it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.39it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.37it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 39.72it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 38.72it/s]\u001b[A07/17/2023 15:36:51 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.5052707195281982, 'eval_accuracy': 0.6966074313408723, 'eval_runtime': 0.8186, 'eval_samples_per_second': 610.796, 'eval_steps_per_second': 39.091, 'epoch': 4.29}\n",
      " 14%|█████                              | 12000/83910 [39:01<3:49:55,  5.21it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.72it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:36:51,736 >> Saving model checkpoint to roberta-large-movies/checkpoint-12000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:36:51,756 >> Configuration saved in roberta-large-movies/checkpoint-12000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:36:53,381 >> Model weights saved in roberta-large-movies/checkpoint-12000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:36:53,382 >> tokenizer config file saved in roberta-large-movies/checkpoint-12000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:36:53,382 >> Special tokens file saved in roberta-large-movies/checkpoint-12000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:36:57,229 >> Deleting older checkpoint [roberta-large-movies/checkpoint-10500] due to args.save_total_limit\n",
      "{'loss': 1.5283, 'learning_rate': 4.255809796210226e-05, 'epoch': 4.47}         \n",
      " 15%|█████▏                             | 12500/83910 [40:38<3:31:26,  5.63it/s][INFO|trainer.py:722] 2023-07-17 15:38:28,360 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:38:28,362 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:38:28,362 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:38:28,362 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.57it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 43.73it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.44it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.37it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.87it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.03it/s]\u001b[A07/17/2023 15:38:29 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.5257039070129395, 'eval_accuracy': 0.6902654867256637, 'eval_runtime': 0.8002, 'eval_samples_per_second': 624.861, 'eval_steps_per_second': 39.991, 'epoch': 4.47}\n",
      " 15%|█████▏                             | 12500/83910 [40:39<3:31:26,  5.63it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.03it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:38:29,164 >> Saving model checkpoint to roberta-large-movies/checkpoint-12500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:38:29,165 >> Configuration saved in roberta-large-movies/checkpoint-12500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:38:30,913 >> Model weights saved in roberta-large-movies/checkpoint-12500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:38:30,914 >> tokenizer config file saved in roberta-large-movies/checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:38:30,914 >> Special tokens file saved in roberta-large-movies/checkpoint-12500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:38:34,776 >> Deleting older checkpoint [roberta-large-movies/checkpoint-11000] due to args.save_total_limit\n",
      "{'loss': 1.5141, 'learning_rate': 4.226015969491122e-05, 'epoch': 4.65}         \n",
      " 15%|█████▍                             | 13000/83910 [42:16<3:42:51,  5.30it/s][INFO|trainer.py:722] 2023-07-17 15:40:06,172 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:40:06,174 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:40:06,174 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:40:06,174 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.52it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.89it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.94it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.80it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.08it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 35.78it/s]\u001b[A07/17/2023 15:40:07 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.5063292980194092, 'eval_accuracy': 0.6949898442789438, 'eval_runtime': 0.8654, 'eval_samples_per_second': 577.759, 'eval_steps_per_second': 36.977, 'epoch': 4.65}\n",
      " 15%|█████▍                             | 13000/83910 [42:16<3:42:51,  5.30it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 35.78it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:40:07,042 >> Saving model checkpoint to roberta-large-movies/checkpoint-13000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:40:07,043 >> Configuration saved in roberta-large-movies/checkpoint-13000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:40:08,776 >> Model weights saved in roberta-large-movies/checkpoint-13000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:40:08,778 >> tokenizer config file saved in roberta-large-movies/checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:40:08,778 >> Special tokens file saved in roberta-large-movies/checkpoint-13000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:40:12,617 >> Deleting older checkpoint [roberta-large-movies/checkpoint-12000] due to args.save_total_limit\n",
      "{'loss': 1.4979, 'learning_rate': 4.1962221427720176e-05, 'epoch': 4.83}        \n",
      " 16%|█████▋                             | 13500/83910 [43:54<3:33:38,  5.49it/s][INFO|trainer.py:722] 2023-07-17 15:41:44,125 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:41:44,127 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:41:44,127 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:41:44,128 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 47.89it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 43.80it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.65it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.62it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.03it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.16it/s]\u001b[A07/17/2023 15:41:44 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.5636450052261353, 'eval_accuracy': 0.6955945677376615, 'eval_runtime': 0.8149, 'eval_samples_per_second': 613.582, 'eval_steps_per_second': 39.269, 'epoch': 4.83}\n",
      " 16%|█████▋                             | 13500/83910 [43:54<3:33:38,  5.49it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.16it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:41:44,946 >> Saving model checkpoint to roberta-large-movies/checkpoint-13500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:41:44,950 >> Configuration saved in roberta-large-movies/checkpoint-13500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:41:46,613 >> Model weights saved in roberta-large-movies/checkpoint-13500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:41:46,614 >> tokenizer config file saved in roberta-large-movies/checkpoint-13500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:41:46,614 >> Special tokens file saved in roberta-large-movies/checkpoint-13500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:41:50,344 >> Deleting older checkpoint [roberta-large-movies/checkpoint-12500] due to args.save_total_limit\n",
      "{'loss': 1.5294, 'learning_rate': 4.1664283160529136e-05, 'epoch': 5.01}        \n",
      " 17%|█████▊                             | 14000/83910 [45:32<3:27:56,  5.60it/s][INFO|trainer.py:722] 2023-07-17 15:43:22,193 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:43:22,194 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:43:22,194 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:43:22,194 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.68it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 43.00it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 37.99it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:00<00:00, 38.29it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:00<00:00, 38.62it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 39.69it/s]\u001b[A07/17/2023 15:43:23 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.587847113609314, 'eval_accuracy': 0.6835193696651346, 'eval_runtime': 0.8296, 'eval_samples_per_second': 602.733, 'eval_steps_per_second': 38.575, 'epoch': 5.01}\n",
      " 17%|█████▊                             | 14000/83910 [45:32<3:27:56,  5.60it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 39.69it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:43:23,036 >> Saving model checkpoint to roberta-large-movies/checkpoint-14000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:43:23,037 >> Configuration saved in roberta-large-movies/checkpoint-14000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:43:24,770 >> Model weights saved in roberta-large-movies/checkpoint-14000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:43:24,770 >> tokenizer config file saved in roberta-large-movies/checkpoint-14000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:43:24,771 >> Special tokens file saved in roberta-large-movies/checkpoint-14000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:43:28,551 >> Deleting older checkpoint [roberta-large-movies/checkpoint-13000] due to args.save_total_limit\n",
      "{'loss': 1.4641, 'learning_rate': 4.13663448933381e-05, 'epoch': 5.18}          \n",
      " 17%|██████                             | 14500/83910 [47:09<3:25:47,  5.62it/s][INFO|trainer.py:722] 2023-07-17 15:44:59,693 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:44:59,694 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:44:59,694 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:44:59,694 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.96it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.04it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.15it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.23it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.41it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.03it/s]\u001b[A07/17/2023 15:45:00 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.5574804544448853, 'eval_accuracy': 0.6962067807989258, 'eval_runtime': 0.81, 'eval_samples_per_second': 617.287, 'eval_steps_per_second': 39.506, 'epoch': 5.18}\n",
      " 17%|██████                             | 14500/83910 [47:10<3:25:47,  5.62it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.03it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:45:00,511 >> Saving model checkpoint to roberta-large-movies/checkpoint-14500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:45:00,518 >> Configuration saved in roberta-large-movies/checkpoint-14500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:45:02,296 >> Model weights saved in roberta-large-movies/checkpoint-14500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:45:02,297 >> tokenizer config file saved in roberta-large-movies/checkpoint-14500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:45:02,297 >> Special tokens file saved in roberta-large-movies/checkpoint-14500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:45:06,176 >> Deleting older checkpoint [roberta-large-movies/checkpoint-13500] due to args.save_total_limit\n",
      "{'loss': 1.4754, 'learning_rate': 4.106840662614707e-05, 'epoch': 5.36}         \n",
      " 18%|██████▎                            | 15000/83910 [48:47<3:28:06,  5.52it/s][INFO|trainer.py:722] 2023-07-17 15:46:37,558 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:46:37,560 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:46:37,560 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:46:37,560 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.38it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.43it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.96it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.04it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.21it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 38.91it/s]\u001b[A07/17/2023 15:46:38 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4779187440872192, 'eval_accuracy': 0.7006847081838931, 'eval_runtime': 0.8312, 'eval_samples_per_second': 601.557, 'eval_steps_per_second': 38.5, 'epoch': 5.36}\n",
      " 18%|██████▎                            | 15000/83910 [48:48<3:28:06,  5.52it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.91it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:46:38,396 >> Saving model checkpoint to roberta-large-movies/checkpoint-15000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:46:38,399 >> Configuration saved in roberta-large-movies/checkpoint-15000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:46:40,124 >> Model weights saved in roberta-large-movies/checkpoint-15000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:46:40,125 >> tokenizer config file saved in roberta-large-movies/checkpoint-15000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:46:40,125 >> Special tokens file saved in roberta-large-movies/checkpoint-15000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:46:43,835 >> Deleting older checkpoint [roberta-large-movies/checkpoint-14000] due to args.save_total_limit\n",
      "{'loss': 1.4696, 'learning_rate': 4.077046835895603e-05, 'epoch': 5.54}         \n",
      " 18%|██████▍                            | 15500/83910 [50:24<3:30:07,  5.43it/s][INFO|trainer.py:722] 2023-07-17 15:48:14,946 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:48:14,947 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:48:14,947 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:48:14,947 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.96it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.10it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.38it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.90it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.12it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.40it/s]\u001b[A07/17/2023 15:48:15 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.451996922492981, 'eval_accuracy': 0.6965271015903928, 'eval_runtime': 0.7909, 'eval_samples_per_second': 632.19, 'eval_steps_per_second': 40.46, 'epoch': 5.54}\n",
      " 18%|██████▍                            | 15500/83910 [50:25<3:30:07,  5.43it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.40it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:48:15,740 >> Saving model checkpoint to roberta-large-movies/checkpoint-15500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:48:15,741 >> Configuration saved in roberta-large-movies/checkpoint-15500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:48:17,468 >> Model weights saved in roberta-large-movies/checkpoint-15500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:48:17,469 >> tokenizer config file saved in roberta-large-movies/checkpoint-15500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:48:17,469 >> Special tokens file saved in roberta-large-movies/checkpoint-15500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:48:22,242 >> Deleting older checkpoint [roberta-large-movies/checkpoint-14500] due to args.save_total_limit\n",
      "{'loss': 1.4655, 'learning_rate': 4.0472530091764986e-05, 'epoch': 5.72}        \n",
      " 19%|██████▋                            | 16000/83910 [52:03<3:45:21,  5.02it/s][INFO|trainer.py:722] 2023-07-17 15:49:53,737 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:49:53,739 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:49:53,739 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:49:53,739 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.57it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.93it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.05it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.98it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.01it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 38.00it/s]\u001b[A07/17/2023 15:49:54 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.6320295333862305, 'eval_accuracy': 0.683049147442327, 'eval_runtime': 0.8309, 'eval_samples_per_second': 601.76, 'eval_steps_per_second': 38.513, 'epoch': 5.72}\n",
      " 19%|██████▋                            | 16000/83910 [52:04<3:45:21,  5.02it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.00it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:49:54,571 >> Saving model checkpoint to roberta-large-movies/checkpoint-16000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:49:54,572 >> Configuration saved in roberta-large-movies/checkpoint-16000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:49:56,257 >> Model weights saved in roberta-large-movies/checkpoint-16000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:49:56,257 >> tokenizer config file saved in roberta-large-movies/checkpoint-16000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:49:56,257 >> Special tokens file saved in roberta-large-movies/checkpoint-16000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:49:59,930 >> Deleting older checkpoint [roberta-large-movies/checkpoint-15000] due to args.save_total_limit\n",
      "{'loss': 1.4792, 'learning_rate': 4.0174591824573945e-05, 'epoch': 5.9}         \n",
      " 20%|██████▉                            | 16500/83910 [53:41<3:16:39,  5.71it/s][INFO|trainer.py:722] 2023-07-17 15:51:31,680 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:51:31,682 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:51:31,682 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:51:31,682 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 47.69it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 36.24it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:00<00:00, 34.92it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:00<00:00, 37.32it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:00<00:00, 38.07it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 39.45it/s]\u001b[A07/17/2023 15:51:32 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.415226697921753, 'eval_accuracy': 0.7134165866154338, 'eval_runtime': 0.8575, 'eval_samples_per_second': 583.097, 'eval_steps_per_second': 37.318, 'epoch': 5.9}\n",
      " 20%|██████▉                            | 16500/83910 [53:42<3:16:39,  5.71it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 39.45it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:51:32,541 >> Saving model checkpoint to roberta-large-movies/checkpoint-16500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:51:32,543 >> Configuration saved in roberta-large-movies/checkpoint-16500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:51:34,219 >> Model weights saved in roberta-large-movies/checkpoint-16500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:51:34,220 >> tokenizer config file saved in roberta-large-movies/checkpoint-16500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:51:34,220 >> Special tokens file saved in roberta-large-movies/checkpoint-16500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:51:37,886 >> Deleting older checkpoint [roberta-large-movies/checkpoint-15500] due to args.save_total_limit\n",
      "{'loss': 1.4379, 'learning_rate': 3.98772494339173e-05, 'epoch': 6.08}          \n",
      " 20%|███████                            | 17000/83910 [55:19<3:28:44,  5.34it/s][INFO|trainer.py:722] 2023-07-17 15:53:09,596 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:53:09,597 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:53:09,598 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:53:09,598 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 39.96it/s]\u001b[A\n",
      " 28%|████████████▍                               | 9/32 [00:00<00:00, 38.45it/s]\u001b[A\n",
      " 41%|█████████████████▍                         | 13/32 [00:00<00:00, 35.76it/s]\u001b[A\n",
      " 56%|████████████████████████▏                  | 18/32 [00:00<00:00, 38.22it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:00<00:00, 39.28it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 40.42it/s]\u001b[A07/17/2023 15:53:10 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4900156259536743, 'eval_accuracy': 0.7041935483870968, 'eval_runtime': 0.8413, 'eval_samples_per_second': 594.352, 'eval_steps_per_second': 38.039, 'epoch': 6.08}\n",
      " 20%|███████                            | 17000/83910 [55:20<3:28:44,  5.34it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.42it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:53:10,440 >> Saving model checkpoint to roberta-large-movies/checkpoint-17000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:53:10,441 >> Configuration saved in roberta-large-movies/checkpoint-17000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:53:12,117 >> Model weights saved in roberta-large-movies/checkpoint-17000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:53:12,118 >> tokenizer config file saved in roberta-large-movies/checkpoint-17000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:53:12,118 >> Special tokens file saved in roberta-large-movies/checkpoint-17000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:53:15,956 >> Deleting older checkpoint [roberta-large-movies/checkpoint-16000] due to args.save_total_limit\n",
      "{'loss': 1.4281, 'learning_rate': 3.957931116672626e-05, 'epoch': 6.26}         \n",
      " 21%|███████▎                           | 17500/83910 [56:57<3:18:58,  5.56it/s][INFO|trainer.py:722] 2023-07-17 15:54:47,810 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:54:47,812 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:54:47,812 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:54:47,812 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.58it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 33.93it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:00<00:00, 33.81it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:00<00:00, 36.55it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 37.77it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 39.06it/s]\u001b[A07/17/2023 15:54:48 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.5407416820526123, 'eval_accuracy': 0.6989864864864865, 'eval_runtime': 0.8677, 'eval_samples_per_second': 576.232, 'eval_steps_per_second': 36.879, 'epoch': 6.26}\n",
      " 21%|███████▎                           | 17500/83910 [56:58<3:18:58,  5.56it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 39.06it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:54:48,695 >> Saving model checkpoint to roberta-large-movies/checkpoint-17500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:54:48,696 >> Configuration saved in roberta-large-movies/checkpoint-17500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:54:50,407 >> Model weights saved in roberta-large-movies/checkpoint-17500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:54:50,408 >> tokenizer config file saved in roberta-large-movies/checkpoint-17500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:54:50,408 >> Special tokens file saved in roberta-large-movies/checkpoint-17500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:54:54,392 >> Deleting older checkpoint [roberta-large-movies/checkpoint-16500] due to args.save_total_limit\n",
      "{'loss': 1.436, 'learning_rate': 3.928137289953522e-05, 'epoch': 6.44}          \n",
      " 21%|███████▌                           | 18000/83910 [58:36<3:26:58,  5.31it/s][INFO|trainer.py:722] 2023-07-17 15:56:26,151 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:56:26,153 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:56:26,153 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:56:26,153 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 46.46it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 36.31it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:00<00:00, 35.81it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:00<00:00, 38.41it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:00<00:00, 38.78it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 39.92it/s]\u001b[A07/17/2023 15:56:26 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.534258246421814, 'eval_accuracy': 0.6914175506268081, 'eval_runtime': 0.843, 'eval_samples_per_second': 593.143, 'eval_steps_per_second': 37.961, 'epoch': 6.44}\n",
      " 21%|███████▌                           | 18000/83910 [58:36<3:26:58,  5.31it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 39.92it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:56:27,007 >> Saving model checkpoint to roberta-large-movies/checkpoint-18000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:56:27,008 >> Configuration saved in roberta-large-movies/checkpoint-18000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:56:28,658 >> Model weights saved in roberta-large-movies/checkpoint-18000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:56:28,659 >> tokenizer config file saved in roberta-large-movies/checkpoint-18000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:56:28,659 >> Special tokens file saved in roberta-large-movies/checkpoint-18000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:56:32,338 >> Deleting older checkpoint [roberta-large-movies/checkpoint-17000] due to args.save_total_limit\n",
      "{'loss': 1.4342, 'learning_rate': 3.8983434632344176e-05, 'epoch': 6.61}        \n",
      " 22%|███████▎                         | 18500/83910 [1:00:13<3:34:27,  5.08it/s][INFO|trainer.py:722] 2023-07-17 15:58:03,441 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:58:03,443 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:58:03,443 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:58:03,443 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.35it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.26it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.14it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.18it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.55it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.62it/s]\u001b[A07/17/2023 15:58:04 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.5323561429977417, 'eval_accuracy': 0.7023696682464455, 'eval_runtime': 0.7874, 'eval_samples_per_second': 635.024, 'eval_steps_per_second': 40.642, 'epoch': 6.61}\n",
      " 22%|███████▎                         | 18500/83910 [1:00:14<3:34:27,  5.08it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.62it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:58:04,232 >> Saving model checkpoint to roberta-large-movies/checkpoint-18500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:58:04,233 >> Configuration saved in roberta-large-movies/checkpoint-18500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:58:05,956 >> Model weights saved in roberta-large-movies/checkpoint-18500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:58:05,957 >> tokenizer config file saved in roberta-large-movies/checkpoint-18500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:58:05,957 >> Special tokens file saved in roberta-large-movies/checkpoint-18500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:58:09,737 >> Deleting older checkpoint [roberta-large-movies/checkpoint-17500] due to args.save_total_limit\n",
      "{'loss': 1.4176, 'learning_rate': 3.868549636515314e-05, 'epoch': 6.79}         \n",
      " 23%|███████▍                         | 19000/83910 [1:01:50<3:31:15,  5.12it/s][INFO|trainer.py:722] 2023-07-17 15:59:41,008 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 15:59:41,009 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 15:59:41,009 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 15:59:41,009 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.74it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.81it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.63it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.59it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 37.23it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 35.83it/s]\u001b[A07/17/2023 15:59:41 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4485751390457153, 'eval_accuracy': 0.7132913490222075, 'eval_runtime': 0.8567, 'eval_samples_per_second': 583.665, 'eval_steps_per_second': 37.355, 'epoch': 6.79}\n",
      " 23%|███████▍                         | 19000/83910 [1:01:51<3:31:15,  5.12it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 35.83it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 15:59:41,867 >> Saving model checkpoint to roberta-large-movies/checkpoint-19000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 15:59:41,868 >> Configuration saved in roberta-large-movies/checkpoint-19000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 15:59:43,630 >> Model weights saved in roberta-large-movies/checkpoint-19000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 15:59:43,631 >> tokenizer config file saved in roberta-large-movies/checkpoint-19000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 15:59:43,631 >> Special tokens file saved in roberta-large-movies/checkpoint-19000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 15:59:47,709 >> Deleting older checkpoint [roberta-large-movies/checkpoint-18000] due to args.save_total_limit\n",
      "{'loss': 1.4308, 'learning_rate': 3.838755809796211e-05, 'epoch': 6.97}         \n",
      " 23%|███████▋                         | 19500/83910 [1:03:28<3:15:04,  5.50it/s][INFO|trainer.py:722] 2023-07-17 16:01:18,753 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:01:18,754 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:01:18,754 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:01:18,755 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.82it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.97it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.25it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.21it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.40it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.59it/s]\u001b[A07/17/2023 16:01:19 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4598056077957153, 'eval_accuracy': 0.7031503734978889, 'eval_runtime': 0.79, 'eval_samples_per_second': 632.872, 'eval_steps_per_second': 40.504, 'epoch': 6.97}\n",
      " 23%|███████▋                         | 19500/83910 [1:03:29<3:15:04,  5.50it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.59it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:01:19,551 >> Saving model checkpoint to roberta-large-movies/checkpoint-19500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:01:19,552 >> Configuration saved in roberta-large-movies/checkpoint-19500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:01:21,230 >> Model weights saved in roberta-large-movies/checkpoint-19500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:01:21,230 >> tokenizer config file saved in roberta-large-movies/checkpoint-19500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:01:21,230 >> Special tokens file saved in roberta-large-movies/checkpoint-19500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:01:25,117 >> Deleting older checkpoint [roberta-large-movies/checkpoint-18500] due to args.save_total_limit\n",
      "{'loss': 1.4014, 'learning_rate': 3.809021570730545e-05, 'epoch': 7.15}         \n",
      " 24%|███████▊                         | 20000/83910 [1:05:06<3:15:58,  5.44it/s][INFO|trainer.py:722] 2023-07-17 16:02:56,635 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:02:56,636 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:02:56,636 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:02:56,636 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.61it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.89it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.04it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.14it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 39.36it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 37.61it/s]\u001b[A07/17/2023 16:02:57 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.575023889541626, 'eval_accuracy': 0.6938435940099834, 'eval_runtime': 0.8292, 'eval_samples_per_second': 603.024, 'eval_steps_per_second': 38.594, 'epoch': 7.15}\n",
      " 24%|███████▊                         | 20000/83910 [1:05:07<3:15:58,  5.44it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 37.61it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:02:57,467 >> Saving model checkpoint to roberta-large-movies/checkpoint-20000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:02:57,468 >> Configuration saved in roberta-large-movies/checkpoint-20000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:02:59,148 >> Model weights saved in roberta-large-movies/checkpoint-20000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:02:59,149 >> tokenizer config file saved in roberta-large-movies/checkpoint-20000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:02:59,149 >> Special tokens file saved in roberta-large-movies/checkpoint-20000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:03:02,798 >> Deleting older checkpoint [roberta-large-movies/checkpoint-19000] due to args.save_total_limit\n",
      "{'loss': 1.3661, 'learning_rate': 3.779227744011441e-05, 'epoch': 7.33}         \n",
      " 24%|████████                         | 20500/83910 [1:06:44<3:12:06,  5.50it/s][INFO|trainer.py:722] 2023-07-17 16:04:34,417 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:04:34,418 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:04:34,419 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:04:34,419 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.62it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.94it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 39.24it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 37.80it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 38.14it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 39.28it/s]\u001b[A07/17/2023 16:04:35 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.5403505563735962, 'eval_accuracy': 0.6985221674876847, 'eval_runtime': 0.8319, 'eval_samples_per_second': 601.063, 'eval_steps_per_second': 38.468, 'epoch': 7.33}\n",
      " 24%|████████                         | 20500/83910 [1:06:45<3:12:06,  5.50it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 39.28it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:04:35,252 >> Saving model checkpoint to roberta-large-movies/checkpoint-20500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:04:35,253 >> Configuration saved in roberta-large-movies/checkpoint-20500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:04:36,974 >> Model weights saved in roberta-large-movies/checkpoint-20500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:04:36,975 >> tokenizer config file saved in roberta-large-movies/checkpoint-20500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:04:36,975 >> Special tokens file saved in roberta-large-movies/checkpoint-20500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:04:40,890 >> Deleting older checkpoint [roberta-large-movies/checkpoint-19500] due to args.save_total_limit\n",
      "{'loss': 1.3857, 'learning_rate': 3.7494935049457754e-05, 'epoch': 7.51}        \n",
      " 25%|████████▎                        | 21000/83910 [1:08:22<3:14:00,  5.40it/s][INFO|trainer.py:722] 2023-07-17 16:06:12,289 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:06:12,290 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:06:12,291 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:06:12,291 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.71it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.85it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.85it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.58it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.92it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 40.94it/s]\u001b[A07/17/2023 16:06:13 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4692307710647583, 'eval_accuracy': 0.7037155669442665, 'eval_runtime': 0.8177, 'eval_samples_per_second': 611.5, 'eval_steps_per_second': 39.136, 'epoch': 7.51}\n",
      " 25%|████████▎                        | 21000/83910 [1:08:22<3:14:00,  5.40it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.94it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:06:13,124 >> Saving model checkpoint to roberta-large-movies/checkpoint-21000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:06:13,134 >> Configuration saved in roberta-large-movies/checkpoint-21000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:06:14,806 >> Model weights saved in roberta-large-movies/checkpoint-21000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:06:14,806 >> tokenizer config file saved in roberta-large-movies/checkpoint-21000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:06:14,807 >> Special tokens file saved in roberta-large-movies/checkpoint-21000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:06:18,596 >> Deleting older checkpoint [roberta-large-movies/checkpoint-20000] due to args.save_total_limit\n",
      "{'loss': 1.3846, 'learning_rate': 3.719699678226672e-05, 'epoch': 7.69}         \n",
      " 26%|████████▍                        | 21500/83910 [1:09:59<3:09:47,  5.48it/s][INFO|trainer.py:722] 2023-07-17 16:07:49,769 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:07:49,770 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:07:49,770 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:07:49,770 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.75it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.30it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.04it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.13it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.41it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.58it/s]\u001b[A07/17/2023 16:07:50 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.5511342287063599, 'eval_accuracy': 0.6941445861956166, 'eval_runtime': 0.7898, 'eval_samples_per_second': 633.076, 'eval_steps_per_second': 40.517, 'epoch': 7.69}\n",
      " 26%|████████▍                        | 21500/83910 [1:10:00<3:09:47,  5.48it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.58it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:07:50,563 >> Saving model checkpoint to roberta-large-movies/checkpoint-21500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:07:50,566 >> Configuration saved in roberta-large-movies/checkpoint-21500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:07:52,246 >> Model weights saved in roberta-large-movies/checkpoint-21500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:07:52,247 >> tokenizer config file saved in roberta-large-movies/checkpoint-21500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:07:52,247 >> Special tokens file saved in roberta-large-movies/checkpoint-21500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:07:56,205 >> Deleting older checkpoint [roberta-large-movies/checkpoint-20500] due to args.save_total_limit\n",
      "{'loss': 1.3867, 'learning_rate': 3.689905851507568e-05, 'epoch': 7.87}         \n",
      " 26%|████████▋                        | 22000/83910 [1:11:37<3:07:26,  5.50it/s][INFO|trainer.py:722] 2023-07-17 16:09:27,781 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:09:27,782 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:09:27,782 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:09:27,782 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 47.43it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.70it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 37.91it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:00<00:00, 36.89it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:00<00:00, 37.63it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 39.22it/s]\u001b[A07/17/2023 16:09:28 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.5321439504623413, 'eval_accuracy': 0.6925124792013311, 'eval_runtime': 0.8379, 'eval_samples_per_second': 596.713, 'eval_steps_per_second': 38.19, 'epoch': 7.87}\n",
      " 26%|████████▋                        | 22000/83910 [1:11:38<3:07:26,  5.50it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 39.22it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:09:28,622 >> Saving model checkpoint to roberta-large-movies/checkpoint-22000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:09:28,623 >> Configuration saved in roberta-large-movies/checkpoint-22000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:09:30,333 >> Model weights saved in roberta-large-movies/checkpoint-22000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:09:30,334 >> tokenizer config file saved in roberta-large-movies/checkpoint-22000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:09:30,334 >> Special tokens file saved in roberta-large-movies/checkpoint-22000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:09:34,156 >> Deleting older checkpoint [roberta-large-movies/checkpoint-21000] due to args.save_total_limit\n",
      "{'loss': 1.3658, 'learning_rate': 3.660112024788464e-05, 'epoch': 8.04}         \n",
      " 27%|████████▊                        | 22500/83910 [1:13:15<3:02:18,  5.61it/s][INFO|trainer.py:722] 2023-07-17 16:11:05,439 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:11:05,440 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:11:05,440 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:11:05,440 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.58it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.36it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.60it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.69it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.74it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 39.46it/s]\u001b[A07/17/2023 16:11:06 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.5499885082244873, 'eval_accuracy': 0.7020917678812416, 'eval_runtime': 0.8209, 'eval_samples_per_second': 609.075, 'eval_steps_per_second': 38.981, 'epoch': 8.04}\n",
      " 27%|████████▊                        | 22500/83910 [1:13:16<3:02:18,  5.61it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 39.46it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:11:06,267 >> Saving model checkpoint to roberta-large-movies/checkpoint-22500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:11:06,272 >> Configuration saved in roberta-large-movies/checkpoint-22500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:11:07,940 >> Model weights saved in roberta-large-movies/checkpoint-22500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:11:07,941 >> tokenizer config file saved in roberta-large-movies/checkpoint-22500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:11:07,941 >> Special tokens file saved in roberta-large-movies/checkpoint-22500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:11:11,793 >> Deleting older checkpoint [roberta-large-movies/checkpoint-21500] due to args.save_total_limit\n",
      "{'loss': 1.3406, 'learning_rate': 3.6303181980693604e-05, 'epoch': 8.22}        \n",
      " 27%|█████████                        | 23000/83910 [1:14:53<3:05:39,  5.47it/s][INFO|trainer.py:722] 2023-07-17 16:12:43,182 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:12:43,183 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:12:43,183 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:12:43,183 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.12it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.57it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.81it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.20it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 38.80it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 37.61it/s]\u001b[A07/17/2023 16:12:44 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.523918628692627, 'eval_accuracy': 0.6959503592423253, 'eval_runtime': 0.8298, 'eval_samples_per_second': 602.525, 'eval_steps_per_second': 38.562, 'epoch': 8.22}\n",
      " 27%|█████████                        | 23000/83910 [1:14:53<3:05:39,  5.47it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 37.61it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:12:44,014 >> Saving model checkpoint to roberta-large-movies/checkpoint-23000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:12:44,015 >> Configuration saved in roberta-large-movies/checkpoint-23000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:12:45,672 >> Model weights saved in roberta-large-movies/checkpoint-23000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:12:45,673 >> tokenizer config file saved in roberta-large-movies/checkpoint-23000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:12:45,673 >> Special tokens file saved in roberta-large-movies/checkpoint-23000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:12:49,434 >> Deleting older checkpoint [roberta-large-movies/checkpoint-22000] due to args.save_total_limit\n",
      "{'loss': 1.3405, 'learning_rate': 3.600524371350256e-05, 'epoch': 8.4}          \n",
      " 28%|█████████▏                       | 23500/83910 [1:16:30<2:58:12,  5.65it/s][INFO|trainer.py:722] 2023-07-17 16:14:20,970 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:14:20,972 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:14:20,972 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:14:20,972 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.41it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.74it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.74it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 40.62it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 38.15it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 36.07it/s]\u001b[A07/17/2023 16:14:21 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4414023160934448, 'eval_accuracy': 0.7055256064690026, 'eval_runtime': 0.8516, 'eval_samples_per_second': 587.105, 'eval_steps_per_second': 37.575, 'epoch': 8.4}\n",
      " 28%|█████████▏                       | 23500/83910 [1:16:31<2:58:12,  5.65it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 36.07it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:14:21,828 >> Saving model checkpoint to roberta-large-movies/checkpoint-23500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:14:21,829 >> Configuration saved in roberta-large-movies/checkpoint-23500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:14:23,521 >> Model weights saved in roberta-large-movies/checkpoint-23500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:14:23,521 >> tokenizer config file saved in roberta-large-movies/checkpoint-23500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:14:23,522 >> Special tokens file saved in roberta-large-movies/checkpoint-23500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:14:27,255 >> Deleting older checkpoint [roberta-large-movies/checkpoint-22500] due to args.save_total_limit\n",
      "{'loss': 1.3373, 'learning_rate': 3.570730544631153e-05, 'epoch': 8.58}         \n",
      " 29%|█████████▍                       | 24000/83910 [1:18:08<2:58:26,  5.60it/s][INFO|trainer.py:722] 2023-07-17 16:15:58,439 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:15:58,440 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:15:58,440 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:15:58,440 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.32it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.69it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.14it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.14it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.31it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.41it/s]\u001b[A07/17/2023 16:15:59 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.599377155303955, 'eval_accuracy': 0.6784238957737527, 'eval_runtime': 0.791, 'eval_samples_per_second': 632.109, 'eval_steps_per_second': 40.455, 'epoch': 8.58}\n",
      " 29%|█████████▍                       | 24000/83910 [1:18:09<2:58:26,  5.60it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.41it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:15:59,246 >> Saving model checkpoint to roberta-large-movies/checkpoint-24000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:15:59,248 >> Configuration saved in roberta-large-movies/checkpoint-24000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:16:01,380 >> Model weights saved in roberta-large-movies/checkpoint-24000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:16:01,381 >> tokenizer config file saved in roberta-large-movies/checkpoint-24000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:16:01,381 >> Special tokens file saved in roberta-large-movies/checkpoint-24000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:16:05,551 >> Deleting older checkpoint [roberta-large-movies/checkpoint-23000] due to args.save_total_limit\n",
      "{'loss': 1.3527, 'learning_rate': 3.540936717912049e-05, 'epoch': 8.76}         \n",
      " 29%|█████████▋                       | 24500/83910 [1:19:47<3:01:20,  5.46it/s][INFO|trainer.py:722] 2023-07-17 16:17:37,210 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:17:37,212 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:17:37,212 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:17:37,212 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 40.65it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 35.21it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:00<00:00, 36.17it/s]\u001b[A\n",
      " 56%|████████████████████████▏                  | 18/32 [00:00<00:00, 37.22it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:00<00:00, 38.38it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 39.35it/s]\u001b[A07/17/2023 16:17:38 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.5105814933776855, 'eval_accuracy': 0.6970387243735763, 'eval_runtime': 0.8594, 'eval_samples_per_second': 581.797, 'eval_steps_per_second': 37.235, 'epoch': 8.76}\n",
      " 29%|█████████▋                       | 24500/83910 [1:19:47<3:01:20,  5.46it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 39.35it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:17:38,073 >> Saving model checkpoint to roberta-large-movies/checkpoint-24500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:17:38,074 >> Configuration saved in roberta-large-movies/checkpoint-24500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:17:39,758 >> Model weights saved in roberta-large-movies/checkpoint-24500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:17:39,759 >> tokenizer config file saved in roberta-large-movies/checkpoint-24500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:17:39,759 >> Special tokens file saved in roberta-large-movies/checkpoint-24500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:17:43,714 >> Deleting older checkpoint [roberta-large-movies/checkpoint-23500] due to args.save_total_limit\n",
      "{'loss': 1.3436, 'learning_rate': 3.511142891192945e-05, 'epoch': 8.94}         \n",
      " 30%|█████████▊                       | 25000/83910 [1:21:25<3:01:41,  5.40it/s][INFO|trainer.py:722] 2023-07-17 16:19:15,171 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:19:15,172 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:19:15,172 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:19:15,172 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.24it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.23it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.35it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 40.33it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 36.37it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 36.97it/s]\u001b[A07/17/2023 16:19:16 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.471426010131836, 'eval_accuracy': 0.7079758500158881, 'eval_runtime': 0.8427, 'eval_samples_per_second': 593.355, 'eval_steps_per_second': 37.975, 'epoch': 8.94}\n",
      " 30%|█████████▊                       | 25000/83910 [1:21:25<3:01:41,  5.40it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 36.97it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:19:16,016 >> Saving model checkpoint to roberta-large-movies/checkpoint-25000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:19:16,017 >> Configuration saved in roberta-large-movies/checkpoint-25000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:19:17,686 >> Model weights saved in roberta-large-movies/checkpoint-25000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:19:17,687 >> tokenizer config file saved in roberta-large-movies/checkpoint-25000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:19:17,687 >> Special tokens file saved in roberta-large-movies/checkpoint-25000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:19:21,486 >> Deleting older checkpoint [roberta-large-movies/checkpoint-24000] due to args.save_total_limit\n",
      "{'loss': 1.3069, 'learning_rate': 3.4813490644738414e-05, 'epoch': 9.12}        \n",
      " 30%|██████████                       | 25500/83910 [1:23:03<3:06:46,  5.21it/s][INFO|trainer.py:722] 2023-07-17 16:20:53,303 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:20:53,305 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:20:53,305 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:20:53,305 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.65it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.23it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 40.74it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 35.64it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 36.37it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 37.78it/s]\u001b[A07/17/2023 16:20:54 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4990392923355103, 'eval_accuracy': 0.6953099376844867, 'eval_runtime': 0.8575, 'eval_samples_per_second': 583.12, 'eval_steps_per_second': 37.32, 'epoch': 9.12}\n",
      " 30%|██████████                       | 25500/83910 [1:23:04<3:06:46,  5.21it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 37.78it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:20:54,164 >> Saving model checkpoint to roberta-large-movies/checkpoint-25500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:20:54,165 >> Configuration saved in roberta-large-movies/checkpoint-25500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:20:55,842 >> Model weights saved in roberta-large-movies/checkpoint-25500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:20:55,842 >> tokenizer config file saved in roberta-large-movies/checkpoint-25500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:20:55,843 >> Special tokens file saved in roberta-large-movies/checkpoint-25500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:20:59,604 >> Deleting older checkpoint [roberta-large-movies/checkpoint-24500] due to args.save_total_limit\n",
      "{'loss': 1.2969, 'learning_rate': 3.451555237754737e-05, 'epoch': 9.3}          \n",
      " 31%|██████████▏                      | 26000/83910 [1:24:41<2:53:56,  5.55it/s][INFO|trainer.py:722] 2023-07-17 16:22:31,314 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:22:31,315 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:22:31,315 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:22:31,315 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.82it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 38.56it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:00<00:00, 37.82it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:00<00:00, 38.95it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:00<00:00, 39.18it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 40.00it/s]\u001b[A07/17/2023 16:22:32 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4809668064117432, 'eval_accuracy': 0.6964285714285714, 'eval_runtime': 0.8312, 'eval_samples_per_second': 601.512, 'eval_steps_per_second': 38.497, 'epoch': 9.3}\n",
      " 31%|██████████▏                      | 26000/83910 [1:24:42<2:53:56,  5.55it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.00it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:22:32,148 >> Saving model checkpoint to roberta-large-movies/checkpoint-26000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:22:32,150 >> Configuration saved in roberta-large-movies/checkpoint-26000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:22:33,827 >> Model weights saved in roberta-large-movies/checkpoint-26000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:22:33,828 >> tokenizer config file saved in roberta-large-movies/checkpoint-26000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:22:33,828 >> Special tokens file saved in roberta-large-movies/checkpoint-26000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:22:37,680 >> Deleting older checkpoint [roberta-large-movies/checkpoint-25000] due to args.save_total_limit\n",
      "{'loss': 1.3009, 'learning_rate': 3.421761411035634e-05, 'epoch': 9.47}         \n",
      " 32%|██████████▍                      | 26500/83910 [1:26:19<2:50:08,  5.62it/s][INFO|trainer.py:722] 2023-07-17 16:24:09,335 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:24:09,336 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:24:09,336 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:24:09,336 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.52it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 38.49it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:00<00:00, 35.70it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:00<00:00, 37.71it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:00<00:00, 38.29it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 39.46it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.22it/s]\u001b[A07/17/2023 16:24:10 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.5964903831481934, 'eval_accuracy': 0.6875602700096431, 'eval_runtime': 0.8752, 'eval_samples_per_second': 571.296, 'eval_steps_per_second': 36.563, 'epoch': 9.47}\n",
      " 32%|██████████▍                      | 26500/83910 [1:26:20<2:50:08,  5.62it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.22it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:24:10,213 >> Saving model checkpoint to roberta-large-movies/checkpoint-26500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:24:10,214 >> Configuration saved in roberta-large-movies/checkpoint-26500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:24:11,887 >> Model weights saved in roberta-large-movies/checkpoint-26500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:24:11,888 >> tokenizer config file saved in roberta-large-movies/checkpoint-26500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:24:11,888 >> Special tokens file saved in roberta-large-movies/checkpoint-26500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:24:15,690 >> Deleting older checkpoint [roberta-large-movies/checkpoint-25500] due to args.save_total_limit\n",
      "{'loss': 1.3227, 'learning_rate': 3.392086759623406e-05, 'epoch': 9.65}         \n",
      " 32%|██████████▌                      | 27000/83910 [1:27:56<2:49:38,  5.59it/s][INFO|trainer.py:722] 2023-07-17 16:25:46,986 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:25:46,988 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:25:46,988 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:25:46,988 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.04it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.97it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.31it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.28it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.44it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.22it/s]\u001b[A07/17/2023 16:25:47 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.429559588432312, 'eval_accuracy': 0.7013662979830839, 'eval_runtime': 0.7904, 'eval_samples_per_second': 632.561, 'eval_steps_per_second': 40.484, 'epoch': 9.65}\n",
      " 32%|██████████▌                      | 27000/83910 [1:27:57<2:49:38,  5.59it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.22it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:25:47,780 >> Saving model checkpoint to roberta-large-movies/checkpoint-27000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:25:47,781 >> Configuration saved in roberta-large-movies/checkpoint-27000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:25:49,565 >> Model weights saved in roberta-large-movies/checkpoint-27000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:25:49,566 >> tokenizer config file saved in roberta-large-movies/checkpoint-27000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:25:49,566 >> Special tokens file saved in roberta-large-movies/checkpoint-27000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:25:53,241 >> Deleting older checkpoint [roberta-large-movies/checkpoint-26000] due to args.save_total_limit\n",
      "{'loss': 1.3259, 'learning_rate': 3.3622929329043025e-05, 'epoch': 9.83}        \n",
      " 33%|██████████▊                      | 27500/83910 [1:29:34<2:50:56,  5.50it/s][INFO|trainer.py:722] 2023-07-17 16:27:24,438 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:27:24,439 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:27:24,440 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:27:24,440 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.08it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.30it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.56it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.59it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.68it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 40.03it/s]\u001b[A07/17/2023 16:27:25 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.413652777671814, 'eval_accuracy': 0.7189224277831873, 'eval_runtime': 0.8134, 'eval_samples_per_second': 614.697, 'eval_steps_per_second': 39.341, 'epoch': 9.83}\n",
      " 33%|██████████▊                      | 27500/83910 [1:29:35<2:50:56,  5.50it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.03it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:27:25,263 >> Saving model checkpoint to roberta-large-movies/checkpoint-27500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:27:25,272 >> Configuration saved in roberta-large-movies/checkpoint-27500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:27:27,025 >> Model weights saved in roberta-large-movies/checkpoint-27500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:27:27,026 >> tokenizer config file saved in roberta-large-movies/checkpoint-27500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:27:27,026 >> Special tokens file saved in roberta-large-movies/checkpoint-27500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:27:30,708 >> Deleting older checkpoint [roberta-large-movies/checkpoint-26500] due to args.save_total_limit\n",
      "{'loss': 1.3131, 'learning_rate': 3.3324991061851985e-05, 'epoch': 10.01}       \n",
      " 33%|███████████                      | 28000/83910 [1:31:11<2:52:40,  5.40it/s][INFO|trainer.py:722] 2023-07-17 16:29:01,901 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:29:01,903 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:29:01,903 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:29:01,903 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.30it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.38it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.60it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.57it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.89it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.26it/s]\u001b[A07/17/2023 16:29:02 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.534200668334961, 'eval_accuracy': 0.7019570099454604, 'eval_runtime': 0.8056, 'eval_samples_per_second': 620.653, 'eval_steps_per_second': 39.722, 'epoch': 10.01}\n",
      " 33%|███████████                      | 28000/83910 [1:31:12<2:52:40,  5.40it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.26it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:29:02,719 >> Saving model checkpoint to roberta-large-movies/checkpoint-28000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:29:02,728 >> Configuration saved in roberta-large-movies/checkpoint-28000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:29:04,437 >> Model weights saved in roberta-large-movies/checkpoint-28000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:29:04,437 >> tokenizer config file saved in roberta-large-movies/checkpoint-28000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:29:04,438 >> Special tokens file saved in roberta-large-movies/checkpoint-28000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:29:08,114 >> Deleting older checkpoint [roberta-large-movies/checkpoint-27000] due to args.save_total_limit\n",
      "{'loss': 1.271, 'learning_rate': 3.3027052794660944e-05, 'epoch': 10.19}        \n",
      " 34%|███████████▏                     | 28500/83910 [1:32:49<2:48:18,  5.49it/s][INFO|trainer.py:722] 2023-07-17 16:30:39,353 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:30:39,354 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:30:39,354 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:30:39,355 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.37it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.32it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.63it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.51it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.77it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.77it/s]\u001b[A07/17/2023 16:30:40 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.470828890800476, 'eval_accuracy': 0.711340206185567, 'eval_runtime': 0.7815, 'eval_samples_per_second': 639.779, 'eval_steps_per_second': 40.946, 'epoch': 10.19}\n",
      " 34%|███████████▏                     | 28500/83910 [1:32:50<2:48:18,  5.49it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.77it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:30:40,151 >> Saving model checkpoint to roberta-large-movies/checkpoint-28500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:30:40,163 >> Configuration saved in roberta-large-movies/checkpoint-28500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:30:41,905 >> Model weights saved in roberta-large-movies/checkpoint-28500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:30:41,906 >> tokenizer config file saved in roberta-large-movies/checkpoint-28500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:30:41,906 >> Special tokens file saved in roberta-large-movies/checkpoint-28500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:30:45,693 >> Deleting older checkpoint [roberta-large-movies/checkpoint-27500] due to args.save_total_limit\n",
      "{'loss': 1.2684, 'learning_rate': 3.272911452746991e-05, 'epoch': 10.37}        \n",
      " 35%|███████████▍                     | 29000/83910 [1:34:26<3:00:00,  5.08it/s][INFO|trainer.py:722] 2023-07-17 16:32:16,750 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:32:16,751 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:32:16,751 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:32:16,751 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.46it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.39it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.49it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.19it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.48it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.42it/s]\u001b[A07/17/2023 16:32:17 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4341672658920288, 'eval_accuracy': 0.7045747422680413, 'eval_runtime': 0.7954, 'eval_samples_per_second': 628.629, 'eval_steps_per_second': 40.232, 'epoch': 10.37}\n",
      " 35%|███████████▍                     | 29000/83910 [1:34:27<3:00:00,  5.08it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.42it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:32:17,548 >> Saving model checkpoint to roberta-large-movies/checkpoint-29000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:32:17,549 >> Configuration saved in roberta-large-movies/checkpoint-29000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:32:19,430 >> Model weights saved in roberta-large-movies/checkpoint-29000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:32:19,430 >> tokenizer config file saved in roberta-large-movies/checkpoint-29000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:32:19,431 >> Special tokens file saved in roberta-large-movies/checkpoint-29000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:32:23,291 >> Deleting older checkpoint [roberta-large-movies/checkpoint-28000] due to args.save_total_limit\n",
      "{'loss': 1.2767, 'learning_rate': 3.2431176260278876e-05, 'epoch': 10.55}       \n",
      " 35%|███████████▌                     | 29500/83910 [1:36:04<2:40:57,  5.63it/s][INFO|trainer.py:722] 2023-07-17 16:33:55,052 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:33:55,053 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:33:55,053 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:33:55,053 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.12it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 40.76it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 38.26it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 39.71it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 39.98it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 40.62it/s]\u001b[A07/17/2023 16:33:55 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4703407287597656, 'eval_accuracy': 0.709353000335233, 'eval_runtime': 0.8179, 'eval_samples_per_second': 611.351, 'eval_steps_per_second': 39.126, 'epoch': 10.55}\n",
      " 35%|███████████▌                     | 29500/83910 [1:36:05<2:40:57,  5.63it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.62it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:33:55,872 >> Saving model checkpoint to roberta-large-movies/checkpoint-29500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:33:55,873 >> Configuration saved in roberta-large-movies/checkpoint-29500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:33:57,533 >> Model weights saved in roberta-large-movies/checkpoint-29500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:33:57,534 >> tokenizer config file saved in roberta-large-movies/checkpoint-29500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:33:57,534 >> Special tokens file saved in roberta-large-movies/checkpoint-29500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:34:01,136 >> Deleting older checkpoint [roberta-large-movies/checkpoint-28500] due to args.save_total_limit\n",
      "{'loss': 1.2861, 'learning_rate': 3.2133237993087835e-05, 'epoch': 10.73}       \n",
      " 36%|███████████▊                     | 30000/83910 [1:37:41<2:43:02,  5.51it/s][INFO|trainer.py:722] 2023-07-17 16:35:31,977 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:35:31,978 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:35:31,978 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:35:31,978 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.25it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.14it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.32it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.15it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.43it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.65it/s]\u001b[A07/17/2023 16:35:32 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3323109149932861, 'eval_accuracy': 0.7308937823834197, 'eval_runtime': 0.7855, 'eval_samples_per_second': 636.523, 'eval_steps_per_second': 40.737, 'epoch': 10.73}\n",
      " 36%|███████████▊                     | 30000/83910 [1:37:42<2:43:02,  5.51it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.65it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:35:32,765 >> Saving model checkpoint to roberta-large-movies/checkpoint-30000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:35:32,766 >> Configuration saved in roberta-large-movies/checkpoint-30000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:35:34,489 >> Model weights saved in roberta-large-movies/checkpoint-30000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:35:34,490 >> tokenizer config file saved in roberta-large-movies/checkpoint-30000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:35:34,490 >> Special tokens file saved in roberta-large-movies/checkpoint-30000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:35:38,221 >> Deleting older checkpoint [roberta-large-movies/checkpoint-11500] due to args.save_total_limit\n",
      "{'loss': 1.2617, 'learning_rate': 3.1835299725896794e-05, 'epoch': 10.9}        \n",
      " 36%|███████████▉                     | 30500/83910 [1:39:19<2:42:30,  5.48it/s][INFO|trainer.py:722] 2023-07-17 16:37:09,526 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:37:09,527 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:37:09,527 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:37:09,527 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.35it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.02it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.41it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.29it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.57it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.65it/s]\u001b[A07/17/2023 16:37:10 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4562044143676758, 'eval_accuracy': 0.7003344481605351, 'eval_runtime': 0.7951, 'eval_samples_per_second': 628.826, 'eval_steps_per_second': 40.245, 'epoch': 10.9}\n",
      " 36%|███████████▉                     | 30500/83910 [1:39:20<2:42:30,  5.48it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.65it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:37:10,327 >> Saving model checkpoint to roberta-large-movies/checkpoint-30500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:37:10,331 >> Configuration saved in roberta-large-movies/checkpoint-30500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:37:12,048 >> Model weights saved in roberta-large-movies/checkpoint-30500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:37:12,049 >> tokenizer config file saved in roberta-large-movies/checkpoint-30500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:37:12,049 >> Special tokens file saved in roberta-large-movies/checkpoint-30500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:37:15,886 >> Deleting older checkpoint [roberta-large-movies/checkpoint-29000] due to args.save_total_limit\n",
      "{'loss': 1.2551, 'learning_rate': 3.153736145870575e-05, 'epoch': 11.08}        \n",
      " 37%|████████████▏                    | 31000/83910 [1:40:57<2:41:08,  5.47it/s][INFO|trainer.py:722] 2023-07-17 16:38:47,557 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:38:47,559 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:38:47,559 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:38:47,559 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.95it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.90it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 34.72it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:00<00:00, 34.79it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:00<00:00, 36.28it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 38.09it/s]\u001b[A07/17/2023 16:38:48 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4361472129821777, 'eval_accuracy': 0.7169689119170984, 'eval_runtime': 0.8647, 'eval_samples_per_second': 578.22, 'eval_steps_per_second': 37.006, 'epoch': 11.08}\n",
      " 37%|████████████▏                    | 31000/83910 [1:40:58<2:41:08,  5.47it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.09it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:38:48,425 >> Saving model checkpoint to roberta-large-movies/checkpoint-31000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:38:48,426 >> Configuration saved in roberta-large-movies/checkpoint-31000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:38:50,135 >> Model weights saved in roberta-large-movies/checkpoint-31000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:38:50,136 >> tokenizer config file saved in roberta-large-movies/checkpoint-31000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:38:50,136 >> Special tokens file saved in roberta-large-movies/checkpoint-31000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:38:54,090 >> Deleting older checkpoint [roberta-large-movies/checkpoint-29500] due to args.save_total_limit\n",
      "{'loss': 1.2404, 'learning_rate': 3.124001906804911e-05, 'epoch': 11.26}        \n",
      " 38%|████████████▍                    | 31500/83910 [1:42:34<2:31:47,  5.75it/s][INFO|trainer.py:722] 2023-07-17 16:40:25,107 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:40:25,109 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:40:25,109 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:40:25,109 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.95it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.95it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.23it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.06it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.20it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.29it/s]\u001b[A07/17/2023 16:40:25 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4536628723144531, 'eval_accuracy': 0.7034617896799478, 'eval_runtime': 0.7907, 'eval_samples_per_second': 632.325, 'eval_steps_per_second': 40.469, 'epoch': 11.26}\n",
      " 38%|████████████▍                    | 31500/83910 [1:42:35<2:31:47,  5.75it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.29it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:40:25,902 >> Saving model checkpoint to roberta-large-movies/checkpoint-31500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:40:25,903 >> Configuration saved in roberta-large-movies/checkpoint-31500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:40:27,651 >> Model weights saved in roberta-large-movies/checkpoint-31500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:40:27,652 >> tokenizer config file saved in roberta-large-movies/checkpoint-31500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:40:27,652 >> Special tokens file saved in roberta-large-movies/checkpoint-31500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:40:31,344 >> Deleting older checkpoint [roberta-large-movies/checkpoint-30500] due to args.save_total_limit\n",
      "{'loss': 1.2562, 'learning_rate': 3.0942080800858066e-05, 'epoch': 11.44}       \n",
      " 38%|████████████▌                    | 32000/83910 [1:44:13<2:35:23,  5.57it/s][INFO|trainer.py:722] 2023-07-17 16:42:03,498 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:42:03,499 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:42:03,499 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:42:03,499 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.36it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.01it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.46it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.06it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.21it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.24it/s]\u001b[A07/17/2023 16:42:04 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4038574695587158, 'eval_accuracy': 0.7132209980557356, 'eval_runtime': 0.7924, 'eval_samples_per_second': 631.001, 'eval_steps_per_second': 40.384, 'epoch': 11.44}\n",
      " 38%|████████████▌                    | 32000/83910 [1:44:14<2:35:23,  5.57it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.24it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:42:04,293 >> Saving model checkpoint to roberta-large-movies/checkpoint-32000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:42:04,294 >> Configuration saved in roberta-large-movies/checkpoint-32000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:42:05,958 >> Model weights saved in roberta-large-movies/checkpoint-32000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:42:05,959 >> tokenizer config file saved in roberta-large-movies/checkpoint-32000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:42:05,959 >> Special tokens file saved in roberta-large-movies/checkpoint-32000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:42:10,204 >> Deleting older checkpoint [roberta-large-movies/checkpoint-31000] due to args.save_total_limit\n",
      "{'loss': 1.2489, 'learning_rate': 3.0644142533667025e-05, 'epoch': 11.62}       \n",
      " 39%|████████████▊                    | 32500/83910 [1:45:52<2:39:25,  5.37it/s][INFO|trainer.py:722] 2023-07-17 16:43:42,509 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:43:42,511 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:43:42,511 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:43:42,511 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 47.81it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 43.94it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.47it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.57it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.95it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 40.60it/s]\u001b[A07/17/2023 16:43:43 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4372212886810303, 'eval_accuracy': 0.706418918918919, 'eval_runtime': 0.8024, 'eval_samples_per_second': 623.122, 'eval_steps_per_second': 39.88, 'epoch': 11.62}\n",
      " 39%|████████████▊                    | 32500/83910 [1:45:53<2:39:25,  5.37it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.60it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:43:43,343 >> Saving model checkpoint to roberta-large-movies/checkpoint-32500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:43:43,345 >> Configuration saved in roberta-large-movies/checkpoint-32500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:43:45,035 >> Model weights saved in roberta-large-movies/checkpoint-32500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:43:45,036 >> tokenizer config file saved in roberta-large-movies/checkpoint-32500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:43:45,036 >> Special tokens file saved in roberta-large-movies/checkpoint-32500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:43:48,492 >> Deleting older checkpoint [roberta-large-movies/checkpoint-31500] due to args.save_total_limit\n",
      "{'loss': 1.2406, 'learning_rate': 3.0346204266475984e-05, 'epoch': 11.8}        \n",
      " 39%|████████████▉                    | 33000/83910 [1:47:29<2:29:21,  5.68it/s][INFO|trainer.py:722] 2023-07-17 16:45:20,096 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:45:20,097 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:45:20,097 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:45:20,097 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.14it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.26it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.55it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 39.58it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 36.84it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 37.05it/s]\u001b[A07/17/2023 16:45:20 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4926137924194336, 'eval_accuracy': 0.7087442472057857, 'eval_runtime': 0.8525, 'eval_samples_per_second': 586.532, 'eval_steps_per_second': 37.538, 'epoch': 11.8}\n",
      " 39%|████████████▉                    | 33000/83910 [1:47:30<2:29:21,  5.68it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 37.05it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:45:20,951 >> Saving model checkpoint to roberta-large-movies/checkpoint-33000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:45:20,952 >> Configuration saved in roberta-large-movies/checkpoint-33000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:45:22,763 >> Model weights saved in roberta-large-movies/checkpoint-33000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:45:22,763 >> tokenizer config file saved in roberta-large-movies/checkpoint-33000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:45:22,764 >> Special tokens file saved in roberta-large-movies/checkpoint-33000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:45:26,671 >> Deleting older checkpoint [roberta-large-movies/checkpoint-32000] due to args.save_total_limit\n",
      "{'loss': 1.2285, 'learning_rate': 3.0048265999284947e-05, 'epoch': 11.98}       \n",
      " 40%|█████████████▏                   | 33500/83910 [1:49:08<2:43:53,  5.13it/s][INFO|trainer.py:722] 2023-07-17 16:46:58,356 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:46:58,358 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:46:58,358 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:46:58,358 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.06it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.56it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 38.88it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 39.81it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.03it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 40.69it/s]\u001b[A07/17/2023 16:46:59 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4080321788787842, 'eval_accuracy': 0.7152005392652511, 'eval_runtime': 0.8108, 'eval_samples_per_second': 616.703, 'eval_steps_per_second': 39.469, 'epoch': 11.98}\n",
      " 40%|█████████████▏                   | 33500/83910 [1:49:09<2:43:53,  5.13it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.69it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:46:59,171 >> Saving model checkpoint to roberta-large-movies/checkpoint-33500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:46:59,172 >> Configuration saved in roberta-large-movies/checkpoint-33500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:47:00,788 >> Model weights saved in roberta-large-movies/checkpoint-33500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:47:00,789 >> tokenizer config file saved in roberta-large-movies/checkpoint-33500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:47:00,789 >> Special tokens file saved in roberta-large-movies/checkpoint-33500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:47:04,523 >> Deleting older checkpoint [roberta-large-movies/checkpoint-32500] due to args.save_total_limit\n",
      "{'loss': 1.2213, 'learning_rate': 2.9750327732093913e-05, 'epoch': 12.16}       \n",
      " 41%|█████████████▎                   | 34000/83910 [1:50:46<2:34:15,  5.39it/s][INFO|trainer.py:722] 2023-07-17 16:48:36,257 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:48:36,259 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:48:36,259 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:48:36,259 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 47.92it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.31it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 40.76it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 36.98it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 36.57it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 38.26it/s]\u001b[A07/17/2023 16:48:37 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.403072476387024, 'eval_accuracy': 0.7170240415854451, 'eval_runtime': 0.8459, 'eval_samples_per_second': 591.089, 'eval_steps_per_second': 37.83, 'epoch': 12.16}\n",
      " 41%|█████████████▎                   | 34000/83910 [1:50:46<2:34:15,  5.39it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.26it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:48:37,106 >> Saving model checkpoint to roberta-large-movies/checkpoint-34000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:48:37,107 >> Configuration saved in roberta-large-movies/checkpoint-34000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:48:38,819 >> Model weights saved in roberta-large-movies/checkpoint-34000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:48:38,819 >> tokenizer config file saved in roberta-large-movies/checkpoint-34000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:48:38,820 >> Special tokens file saved in roberta-large-movies/checkpoint-34000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:48:42,937 >> Deleting older checkpoint [roberta-large-movies/checkpoint-33000] due to args.save_total_limit\n",
      "{'loss': 1.1998, 'learning_rate': 2.9452389464902875e-05, 'epoch': 12.33}       \n",
      " 41%|█████████████▌                   | 34500/83910 [1:52:24<2:27:22,  5.59it/s][INFO|trainer.py:722] 2023-07-17 16:50:14,761 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:50:14,763 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:50:14,763 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:50:14,763 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.91it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.14it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.26it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.25it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.42it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.22it/s]\u001b[A07/17/2023 16:50:15 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3541438579559326, 'eval_accuracy': 0.7222584856396866, 'eval_runtime': 0.7909, 'eval_samples_per_second': 632.16, 'eval_steps_per_second': 40.458, 'epoch': 12.33}\n",
      " 41%|█████████████▌                   | 34500/83910 [1:52:25<2:27:22,  5.59it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.22it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:50:15,556 >> Saving model checkpoint to roberta-large-movies/checkpoint-34500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:50:15,557 >> Configuration saved in roberta-large-movies/checkpoint-34500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:50:17,280 >> Model weights saved in roberta-large-movies/checkpoint-34500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:50:17,280 >> tokenizer config file saved in roberta-large-movies/checkpoint-34500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:50:17,281 >> Special tokens file saved in roberta-large-movies/checkpoint-34500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:50:21,126 >> Deleting older checkpoint [roberta-large-movies/checkpoint-33500] due to args.save_total_limit\n",
      "{'loss': 1.2184, 'learning_rate': 2.9154451197711835e-05, 'epoch': 12.51}       \n",
      " 42%|█████████████▊                   | 35000/83910 [1:54:02<2:22:55,  5.70it/s][INFO|trainer.py:722] 2023-07-17 16:51:52,857 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:51:52,858 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:51:52,858 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:51:52,858 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.13it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.99it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.77it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 33.95it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 34.51it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 36.67it/s]\u001b[A07/17/2023 16:51:53 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3629957437515259, 'eval_accuracy': 0.7308441558441559, 'eval_runtime': 0.8716, 'eval_samples_per_second': 573.677, 'eval_steps_per_second': 36.715, 'epoch': 12.51}\n",
      " 42%|█████████████▊                   | 35000/83910 [1:54:03<2:22:55,  5.70it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 36.67it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:51:53,731 >> Saving model checkpoint to roberta-large-movies/checkpoint-35000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:51:53,732 >> Configuration saved in roberta-large-movies/checkpoint-35000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:51:55,394 >> Model weights saved in roberta-large-movies/checkpoint-35000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:51:55,395 >> tokenizer config file saved in roberta-large-movies/checkpoint-35000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:51:55,395 >> Special tokens file saved in roberta-large-movies/checkpoint-35000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:51:58,852 >> Deleting older checkpoint [roberta-large-movies/checkpoint-34000] due to args.save_total_limit\n",
      "{'loss': 1.2195, 'learning_rate': 2.8856512930520797e-05, 'epoch': 12.69}       \n",
      " 42%|█████████████▉                   | 35500/83910 [1:55:40<2:26:48,  5.50it/s][INFO|trainer.py:722] 2023-07-17 16:53:30,167 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:53:30,170 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:53:30,170 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:53:30,170 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.14it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.19it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.89it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.49it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.61it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 36.31it/s]\u001b[A07/17/2023 16:53:31 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.312456488609314, 'eval_accuracy': 0.7281362594169669, 'eval_runtime': 0.852, 'eval_samples_per_second': 586.847, 'eval_steps_per_second': 37.558, 'epoch': 12.69}\n",
      " 42%|█████████████▉                   | 35500/83910 [1:55:40<2:26:48,  5.50it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 36.31it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:53:31,023 >> Saving model checkpoint to roberta-large-movies/checkpoint-35500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:53:31,024 >> Configuration saved in roberta-large-movies/checkpoint-35500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:53:32,679 >> Model weights saved in roberta-large-movies/checkpoint-35500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:53:32,679 >> tokenizer config file saved in roberta-large-movies/checkpoint-35500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:53:32,679 >> Special tokens file saved in roberta-large-movies/checkpoint-35500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:53:36,607 >> Deleting older checkpoint [roberta-large-movies/checkpoint-34500] due to args.save_total_limit\n",
      "{'loss': 1.2178, 'learning_rate': 2.8558574663329756e-05, 'epoch': 12.87}       \n",
      " 43%|██████████████▏                  | 36000/83910 [1:57:18<2:23:56,  5.55it/s][INFO|trainer.py:722] 2023-07-17 16:55:08,281 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:55:08,283 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:55:08,283 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:55:08,283 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.16it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.22it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.71it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.85it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 36.13it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 35.56it/s]\u001b[A07/17/2023 16:55:09 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4257023334503174, 'eval_accuracy': 0.7119236883942767, 'eval_runtime': 0.8597, 'eval_samples_per_second': 581.571, 'eval_steps_per_second': 37.221, 'epoch': 12.87}\n",
      " 43%|██████████████▏                  | 36000/83910 [1:57:19<2:23:56,  5.55it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 35.56it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:55:09,144 >> Saving model checkpoint to roberta-large-movies/checkpoint-36000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:55:09,145 >> Configuration saved in roberta-large-movies/checkpoint-36000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:55:10,887 >> Model weights saved in roberta-large-movies/checkpoint-36000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:55:10,888 >> tokenizer config file saved in roberta-large-movies/checkpoint-36000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:55:10,888 >> Special tokens file saved in roberta-large-movies/checkpoint-36000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:55:14,704 >> Deleting older checkpoint [roberta-large-movies/checkpoint-35000] due to args.save_total_limit\n",
      "{'loss': 1.1918, 'learning_rate': 2.8260636396138722e-05, 'epoch': 13.05}       \n",
      " 43%|██████████████▎                  | 36500/83910 [1:58:55<2:24:00,  5.49it/s][INFO|trainer.py:722] 2023-07-17 16:56:45,912 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:56:45,913 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:56:45,913 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:56:45,913 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.34it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.27it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.27it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.09it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.39it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 36.82it/s]\u001b[A07/17/2023 16:56:46 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4108035564422607, 'eval_accuracy': 0.7152686762778506, 'eval_runtime': 0.9192, 'eval_samples_per_second': 543.96, 'eval_steps_per_second': 34.813, 'epoch': 13.05}\n",
      " 43%|██████████████▎                  | 36500/83910 [1:58:56<2:24:00,  5.49it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 36.82it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:56:46,835 >> Saving model checkpoint to roberta-large-movies/checkpoint-36500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:56:46,836 >> Configuration saved in roberta-large-movies/checkpoint-36500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:56:48,502 >> Model weights saved in roberta-large-movies/checkpoint-36500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:56:48,502 >> tokenizer config file saved in roberta-large-movies/checkpoint-36500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:56:48,503 >> Special tokens file saved in roberta-large-movies/checkpoint-36500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:56:52,934 >> Deleting older checkpoint [roberta-large-movies/checkpoint-35500] due to args.save_total_limit\n",
      "{'loss': 1.1664, 'learning_rate': 2.7963294005482066e-05, 'epoch': 13.23}       \n",
      " 44%|██████████████▌                  | 37000/83910 [2:00:33<2:18:35,  5.64it/s][INFO|trainer.py:722] 2023-07-17 16:58:23,999 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 16:58:24,001 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 16:58:24,001 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 16:58:24,001 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.20it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.78it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.54it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.98it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.24it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.52it/s]\u001b[A07/17/2023 16:58:24 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3577048778533936, 'eval_accuracy': 0.7226588081204977, 'eval_runtime': 0.7887, 'eval_samples_per_second': 633.948, 'eval_steps_per_second': 40.573, 'epoch': 13.23}\n",
      " 44%|██████████████▌                  | 37000/83910 [2:00:34<2:18:35,  5.64it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.52it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 16:58:24,791 >> Saving model checkpoint to roberta-large-movies/checkpoint-37000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 16:58:24,792 >> Configuration saved in roberta-large-movies/checkpoint-37000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 16:58:26,549 >> Model weights saved in roberta-large-movies/checkpoint-37000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 16:58:26,550 >> tokenizer config file saved in roberta-large-movies/checkpoint-37000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 16:58:26,550 >> Special tokens file saved in roberta-large-movies/checkpoint-37000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 16:58:30,368 >> Deleting older checkpoint [roberta-large-movies/checkpoint-36000] due to args.save_total_limit\n",
      "{'loss': 1.1754, 'learning_rate': 2.7665355738291028e-05, 'epoch': 13.41}       \n",
      " 45%|██████████████▋                  | 37500/83910 [2:02:11<2:22:09,  5.44it/s][INFO|trainer.py:722] 2023-07-17 17:00:01,736 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:00:01,738 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:00:01,738 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:00:01,738 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.87it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.69it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.75it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 38.50it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 36.72it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 37.53it/s]\u001b[A07/17/2023 17:00:02 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.377700924873352, 'eval_accuracy': 0.720593191776205, 'eval_runtime': 0.8445, 'eval_samples_per_second': 592.06, 'eval_steps_per_second': 37.892, 'epoch': 13.41}\n",
      " 45%|██████████████▋                  | 37500/83910 [2:02:12<2:22:09,  5.44it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 37.53it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:00:02,584 >> Saving model checkpoint to roberta-large-movies/checkpoint-37500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:00:02,585 >> Configuration saved in roberta-large-movies/checkpoint-37500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:00:04,281 >> Model weights saved in roberta-large-movies/checkpoint-37500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:00:04,281 >> tokenizer config file saved in roberta-large-movies/checkpoint-37500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:00:04,282 >> Special tokens file saved in roberta-large-movies/checkpoint-37500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:00:08,078 >> Deleting older checkpoint [roberta-large-movies/checkpoint-36500] due to args.save_total_limit\n",
      "{'loss': 1.1855, 'learning_rate': 2.7367417471099987e-05, 'epoch': 13.59}       \n",
      " 45%|██████████████▉                  | 38000/83910 [2:03:50<2:26:56,  5.21it/s][INFO|trainer.py:722] 2023-07-17 17:01:40,194 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:01:40,196 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:01:40,197 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:01:40,197 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 46.11it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 43.98it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.72it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.88it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.25it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.26it/s]\u001b[A07/17/2023 17:01:41 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.350059151649475, 'eval_accuracy': 0.7354008578027054, 'eval_runtime': 0.8109, 'eval_samples_per_second': 616.607, 'eval_steps_per_second': 39.463, 'epoch': 13.59}\n",
      " 45%|██████████████▉                  | 38000/83910 [2:03:50<2:26:56,  5.21it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.26it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:01:41,009 >> Saving model checkpoint to roberta-large-movies/checkpoint-38000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:01:41,010 >> Configuration saved in roberta-large-movies/checkpoint-38000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:01:42,716 >> Model weights saved in roberta-large-movies/checkpoint-38000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:01:42,717 >> tokenizer config file saved in roberta-large-movies/checkpoint-38000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:01:42,717 >> Special tokens file saved in roberta-large-movies/checkpoint-38000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:01:46,641 >> Deleting older checkpoint [roberta-large-movies/checkpoint-30000] due to args.save_total_limit\n",
      "{'loss': 1.1644, 'learning_rate': 2.7070075080443334e-05, 'epoch': 13.76}       \n",
      " 46%|███████████████▏                 | 38500/83910 [2:05:28<2:20:34,  5.38it/s][INFO|trainer.py:722] 2023-07-17 17:03:18,233 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:03:18,235 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:03:18,235 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:03:18,235 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.29it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.14it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.22it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.10it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.98it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 37.01it/s]\u001b[A07/17/2023 17:03:19 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.374656081199646, 'eval_accuracy': 0.7206685953069752, 'eval_runtime': 0.8397, 'eval_samples_per_second': 595.482, 'eval_steps_per_second': 38.111, 'epoch': 13.76}\n",
      " 46%|███████████████▏                 | 38500/83910 [2:05:28<2:20:34,  5.38it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 37.01it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:03:19,076 >> Saving model checkpoint to roberta-large-movies/checkpoint-38500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:03:19,077 >> Configuration saved in roberta-large-movies/checkpoint-38500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:03:20,775 >> Model weights saved in roberta-large-movies/checkpoint-38500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:03:20,775 >> tokenizer config file saved in roberta-large-movies/checkpoint-38500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:03:20,776 >> Special tokens file saved in roberta-large-movies/checkpoint-38500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:03:24,412 >> Deleting older checkpoint [roberta-large-movies/checkpoint-37000] due to args.save_total_limit\n",
      "{'loss': 1.1709, 'learning_rate': 2.6772136813252297e-05, 'epoch': 13.94}       \n",
      " 46%|███████████████▎                 | 39000/83910 [2:07:05<2:18:10,  5.42it/s][INFO|trainer.py:722] 2023-07-17 17:04:55,548 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:04:55,550 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:04:55,550 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:04:55,550 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.40it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.13it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.66it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.69it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.89it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 40.94it/s]\u001b[A07/17/2023 17:04:56 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3703839778900146, 'eval_accuracy': 0.7183739837398374, 'eval_runtime': 0.8025, 'eval_samples_per_second': 623.038, 'eval_steps_per_second': 39.874, 'epoch': 13.94}\n",
      " 46%|███████████████▎                 | 39000/83910 [2:07:06<2:18:10,  5.42it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.94it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:04:56,361 >> Saving model checkpoint to roberta-large-movies/checkpoint-39000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:04:56,368 >> Configuration saved in roberta-large-movies/checkpoint-39000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:04:58,097 >> Model weights saved in roberta-large-movies/checkpoint-39000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:04:58,097 >> tokenizer config file saved in roberta-large-movies/checkpoint-39000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:04:58,098 >> Special tokens file saved in roberta-large-movies/checkpoint-39000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:05:01,938 >> Deleting older checkpoint [roberta-large-movies/checkpoint-37500] due to args.save_total_limit\n",
      "{'loss': 1.1613, 'learning_rate': 2.6474198546061256e-05, 'epoch': 14.12}       \n",
      " 47%|███████████████▌                 | 39500/83910 [2:08:43<2:11:50,  5.61it/s][INFO|trainer.py:722] 2023-07-17 17:06:33,320 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:06:33,321 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:06:33,321 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:06:33,321 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.67it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.72it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.15it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.17it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.18it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 36.51it/s]\u001b[A07/17/2023 17:06:34 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4306718111038208, 'eval_accuracy': 0.7246875, 'eval_runtime': 0.8499, 'eval_samples_per_second': 588.275, 'eval_steps_per_second': 37.65, 'epoch': 14.12}\n",
      " 47%|███████████████▌                 | 39500/83910 [2:08:44<2:11:50,  5.61it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 36.51it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:06:34,173 >> Saving model checkpoint to roberta-large-movies/checkpoint-39500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:06:34,174 >> Configuration saved in roberta-large-movies/checkpoint-39500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:06:35,901 >> Model weights saved in roberta-large-movies/checkpoint-39500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:06:35,902 >> tokenizer config file saved in roberta-large-movies/checkpoint-39500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:06:35,902 >> Special tokens file saved in roberta-large-movies/checkpoint-39500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:06:39,761 >> Deleting older checkpoint [roberta-large-movies/checkpoint-38500] due to args.save_total_limit\n",
      "{'loss': 1.1443, 'learning_rate': 2.617626027887022e-05, 'epoch': 14.3}         \n",
      " 48%|███████████████▋                 | 40000/83910 [2:10:20<2:12:17,  5.53it/s][INFO|trainer.py:722] 2023-07-17 17:08:10,809 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:08:10,810 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:08:10,810 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:08:10,810 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.43it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.18it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.48it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.29it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.39it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.25it/s]\u001b[A07/17/2023 17:08:11 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3189983367919922, 'eval_accuracy': 0.7220978573712824, 'eval_runtime': 0.7903, 'eval_samples_per_second': 632.651, 'eval_steps_per_second': 40.49, 'epoch': 14.3}\n",
      " 48%|███████████████▋                 | 40000/83910 [2:10:21<2:12:17,  5.53it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.25it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:08:11,602 >> Saving model checkpoint to roberta-large-movies/checkpoint-40000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:08:11,604 >> Configuration saved in roberta-large-movies/checkpoint-40000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:08:13,338 >> Model weights saved in roberta-large-movies/checkpoint-40000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:08:13,339 >> tokenizer config file saved in roberta-large-movies/checkpoint-40000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:08:13,339 >> Special tokens file saved in roberta-large-movies/checkpoint-40000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:08:17,123 >> Deleting older checkpoint [roberta-large-movies/checkpoint-39000] due to args.save_total_limit\n",
      "{'loss': 1.1356, 'learning_rate': 2.5878322011679178e-05, 'epoch': 14.48}       \n",
      " 48%|███████████████▉                 | 40500/83910 [2:11:58<2:07:21,  5.68it/s][INFO|trainer.py:722] 2023-07-17 17:09:48,224 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:09:48,226 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:09:48,226 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:09:48,226 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.84it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.93it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.45it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.77it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.17it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.27it/s]\u001b[A07/17/2023 17:09:49 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3287793397903442, 'eval_accuracy': 0.7331329325317302, 'eval_runtime': 0.7921, 'eval_samples_per_second': 631.257, 'eval_steps_per_second': 40.4, 'epoch': 14.48}\n",
      " 48%|███████████████▉                 | 40500/83910 [2:11:58<2:07:21,  5.68it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.27it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:09:49,020 >> Saving model checkpoint to roberta-large-movies/checkpoint-40500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:09:49,021 >> Configuration saved in roberta-large-movies/checkpoint-40500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:09:50,782 >> Model weights saved in roberta-large-movies/checkpoint-40500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:09:50,783 >> tokenizer config file saved in roberta-large-movies/checkpoint-40500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:09:50,783 >> Special tokens file saved in roberta-large-movies/checkpoint-40500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:09:54,650 >> Deleting older checkpoint [roberta-large-movies/checkpoint-39500] due to args.save_total_limit\n",
      "{'loss': 1.1493, 'learning_rate': 2.5580383744488147e-05, 'epoch': 14.66}       \n",
      " 49%|████████████████                 | 41000/83910 [2:13:36<2:15:31,  5.28it/s][INFO|trainer.py:722] 2023-07-17 17:11:26,466 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:11:26,467 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:11:26,468 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:11:26,468 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████▌                                      | 4/32 [00:00<00:00, 37.19it/s]\u001b[A\n",
      " 25%|███████████                                 | 8/32 [00:00<00:00, 35.65it/s]\u001b[A\n",
      " 38%|████████████████▏                          | 12/32 [00:00<00:00, 37.24it/s]\u001b[A\n",
      " 53%|██████████████████████▊                    | 17/32 [00:00<00:00, 38.89it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 22/32 [00:00<00:00, 39.93it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▎      | 27/32 [00:00<00:00, 40.20it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.54it/s]\u001b[A07/17/2023 17:11:27 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3504801988601685, 'eval_accuracy': 0.7240227196792516, 'eval_runtime': 0.8432, 'eval_samples_per_second': 592.975, 'eval_steps_per_second': 37.95, 'epoch': 14.66}\n",
      " 49%|████████████████                 | 41000/83910 [2:13:37<2:15:31,  5.28it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.54it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:11:27,312 >> Saving model checkpoint to roberta-large-movies/checkpoint-41000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:11:27,313 >> Configuration saved in roberta-large-movies/checkpoint-41000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:11:28,988 >> Model weights saved in roberta-large-movies/checkpoint-41000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:11:28,989 >> tokenizer config file saved in roberta-large-movies/checkpoint-41000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:11:28,989 >> Special tokens file saved in roberta-large-movies/checkpoint-41000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:11:32,929 >> Deleting older checkpoint [roberta-large-movies/checkpoint-40000] due to args.save_total_limit\n",
      "{'loss': 1.1417, 'learning_rate': 2.5283041353831487e-05, 'epoch': 14.84}       \n",
      " 49%|████████████████▎                | 41500/83910 [2:15:14<2:07:55,  5.53it/s][INFO|trainer.py:722] 2023-07-17 17:13:04,664 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:13:04,666 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:13:04,666 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:13:04,666 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.86it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 38.10it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:00<00:00, 37.39it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:00<00:00, 39.28it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:00<00:00, 39.48it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 40.39it/s]\u001b[A07/17/2023 17:13:05 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.31459379196167, 'eval_accuracy': 0.7320369149637442, 'eval_runtime': 0.8272, 'eval_samples_per_second': 604.463, 'eval_steps_per_second': 38.686, 'epoch': 14.84}\n",
      " 49%|████████████████▎                | 41500/83910 [2:15:15<2:07:55,  5.53it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.39it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:13:05,495 >> Saving model checkpoint to roberta-large-movies/checkpoint-41500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:13:05,495 >> Configuration saved in roberta-large-movies/checkpoint-41500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:13:07,176 >> Model weights saved in roberta-large-movies/checkpoint-41500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:13:07,177 >> tokenizer config file saved in roberta-large-movies/checkpoint-41500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:13:07,177 >> Special tokens file saved in roberta-large-movies/checkpoint-41500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:13:11,157 >> Deleting older checkpoint [roberta-large-movies/checkpoint-40500] due to args.save_total_limit\n",
      "{'loss': 1.1349, 'learning_rate': 2.498569896317483e-05, 'epoch': 15.02}        \n",
      " 50%|████████████████▌                | 42000/83910 [2:16:52<2:06:52,  5.51it/s][INFO|trainer.py:722] 2023-07-17 17:14:42,760 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:14:42,761 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:14:42,761 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:14:42,761 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.90it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.05it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.19it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.23it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 35.22it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 35.29it/s]\u001b[A07/17/2023 17:14:43 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3545522689819336, 'eval_accuracy': 0.7333114107201578, 'eval_runtime': 0.8634, 'eval_samples_per_second': 579.106, 'eval_steps_per_second': 37.063, 'epoch': 15.02}\n",
      " 50%|████████████████▌                | 42000/83910 [2:16:53<2:06:52,  5.51it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 35.29it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:14:43,626 >> Saving model checkpoint to roberta-large-movies/checkpoint-42000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:14:43,628 >> Configuration saved in roberta-large-movies/checkpoint-42000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:14:45,278 >> Model weights saved in roberta-large-movies/checkpoint-42000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:14:45,279 >> tokenizer config file saved in roberta-large-movies/checkpoint-42000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:14:45,279 >> Special tokens file saved in roberta-large-movies/checkpoint-42000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:14:49,677 >> Deleting older checkpoint [roberta-large-movies/checkpoint-41000] due to args.save_total_limit\n",
      "{'loss': 1.1169, 'learning_rate': 2.4687760695983793e-05, 'epoch': 15.19}       \n",
      " 51%|████████████████▋                | 42500/83910 [2:18:31<2:03:39,  5.58it/s][INFO|trainer.py:722] 2023-07-17 17:16:21,347 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:16:21,349 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:16:21,349 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:16:21,349 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 47.67it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.32it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.53it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.48it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 38.10it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 35.60it/s]\u001b[A07/17/2023 17:16:22 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.37086021900177, 'eval_accuracy': 0.7246922024623803, 'eval_runtime': 0.8611, 'eval_samples_per_second': 580.685, 'eval_steps_per_second': 37.164, 'epoch': 15.19}\n",
      " 51%|████████████████▋                | 42500/83910 [2:18:32<2:03:39,  5.58it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 35.60it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:16:22,211 >> Saving model checkpoint to roberta-large-movies/checkpoint-42500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:16:22,212 >> Configuration saved in roberta-large-movies/checkpoint-42500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:16:23,964 >> Model weights saved in roberta-large-movies/checkpoint-42500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:16:23,965 >> tokenizer config file saved in roberta-large-movies/checkpoint-42500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:16:23,965 >> Special tokens file saved in roberta-large-movies/checkpoint-42500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:16:27,838 >> Deleting older checkpoint [roberta-large-movies/checkpoint-41500] due to args.save_total_limit\n",
      "{'loss': 1.1187, 'learning_rate': 2.4390418305327136e-05, 'epoch': 15.37}       \n",
      " 51%|████████████████▉                | 43000/83910 [2:20:09<2:02:21,  5.57it/s][INFO|trainer.py:722] 2023-07-17 17:17:59,204 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:17:59,205 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:17:59,206 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:17:59,206 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.43it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 43.98it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.55it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.44it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.49it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 40.86it/s]\u001b[A07/17/2023 17:18:00 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4242717027664185, 'eval_accuracy': 0.7217795484727756, 'eval_runtime': 0.8265, 'eval_samples_per_second': 604.985, 'eval_steps_per_second': 38.719, 'epoch': 15.37}\n",
      " 51%|████████████████▉                | 43000/83910 [2:20:09<2:02:21,  5.57it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.86it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:18:00,035 >> Saving model checkpoint to roberta-large-movies/checkpoint-43000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:18:00,041 >> Configuration saved in roberta-large-movies/checkpoint-43000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:18:01,772 >> Model weights saved in roberta-large-movies/checkpoint-43000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:18:01,773 >> tokenizer config file saved in roberta-large-movies/checkpoint-43000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:18:01,773 >> Special tokens file saved in roberta-large-movies/checkpoint-43000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:18:05,563 >> Deleting older checkpoint [roberta-large-movies/checkpoint-42000] due to args.save_total_limit\n",
      "{'loss': 1.118, 'learning_rate': 2.4092480038136102e-05, 'epoch': 15.55}        \n",
      " 52%|█████████████████                | 43500/83910 [2:21:47<2:03:35,  5.45it/s][INFO|trainer.py:722] 2023-07-17 17:19:37,316 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:19:37,318 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:19:37,318 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:19:37,318 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.48it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 40.61it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 36.46it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 38.45it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 38.73it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 39.64it/s]\u001b[A07/17/2023 17:19:38 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3835431337356567, 'eval_accuracy': 0.7264245251582806, 'eval_runtime': 0.8374, 'eval_samples_per_second': 597.064, 'eval_steps_per_second': 38.212, 'epoch': 15.55}\n",
      " 52%|█████████████████                | 43500/83910 [2:21:48<2:03:35,  5.45it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 39.64it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:19:38,157 >> Saving model checkpoint to roberta-large-movies/checkpoint-43500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:19:38,159 >> Configuration saved in roberta-large-movies/checkpoint-43500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:19:39,809 >> Model weights saved in roberta-large-movies/checkpoint-43500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:19:39,810 >> tokenizer config file saved in roberta-large-movies/checkpoint-43500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:19:39,810 >> Special tokens file saved in roberta-large-movies/checkpoint-43500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:19:43,672 >> Deleting older checkpoint [roberta-large-movies/checkpoint-42500] due to args.save_total_limit\n",
      "{'loss': 1.1165, 'learning_rate': 2.379454177094506e-05, 'epoch': 15.73}        \n",
      " 52%|█████████████████▎               | 44000/83910 [2:23:25<2:00:59,  5.50it/s][INFO|trainer.py:722] 2023-07-17 17:21:15,524 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:21:15,526 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:21:15,526 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:21:15,526 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.69it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.28it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 38.50it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:00<00:00, 35.86it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:00<00:00, 36.94it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 38.51it/s]\u001b[A07/17/2023 17:21:16 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3239895105361938, 'eval_accuracy': 0.7253818654533637, 'eval_runtime': 0.8499, 'eval_samples_per_second': 588.29, 'eval_steps_per_second': 37.651, 'epoch': 15.73}\n",
      " 52%|█████████████████▎               | 44000/83910 [2:23:26<2:00:59,  5.50it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.51it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:21:16,377 >> Saving model checkpoint to roberta-large-movies/checkpoint-44000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:21:16,378 >> Configuration saved in roberta-large-movies/checkpoint-44000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:21:18,099 >> Model weights saved in roberta-large-movies/checkpoint-44000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:21:18,100 >> tokenizer config file saved in roberta-large-movies/checkpoint-44000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:21:18,101 >> Special tokens file saved in roberta-large-movies/checkpoint-44000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:21:21,637 >> Deleting older checkpoint [roberta-large-movies/checkpoint-43000] due to args.save_total_limit\n",
      "{'loss': 1.114, 'learning_rate': 2.3496603503754024e-05, 'epoch': 15.91}        \n",
      " 53%|█████████████████▌               | 44500/83910 [2:25:02<2:01:41,  5.40it/s][INFO|trainer.py:722] 2023-07-17 17:22:52,975 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:22:52,976 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:22:52,976 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:22:52,976 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.05it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.95it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.10it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.80it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.93it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 38.35it/s]\u001b[A07/17/2023 17:22:53 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3263858556747437, 'eval_accuracy': 0.7382113821138211, 'eval_runtime': 0.8424, 'eval_samples_per_second': 593.546, 'eval_steps_per_second': 37.987, 'epoch': 15.91}\n",
      " 53%|█████████████████▌               | 44500/83910 [2:25:03<2:01:41,  5.40it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.35it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:22:53,820 >> Saving model checkpoint to roberta-large-movies/checkpoint-44500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:22:53,821 >> Configuration saved in roberta-large-movies/checkpoint-44500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:22:55,532 >> Model weights saved in roberta-large-movies/checkpoint-44500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:22:55,533 >> tokenizer config file saved in roberta-large-movies/checkpoint-44500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:22:55,533 >> Special tokens file saved in roberta-large-movies/checkpoint-44500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:22:59,673 >> Deleting older checkpoint [roberta-large-movies/checkpoint-38000] due to args.save_total_limit\n",
      "{'loss': 1.105, 'learning_rate': 2.3198665236562986e-05, 'epoch': 16.09}        \n",
      " 54%|█████████████████▋               | 45000/83910 [2:26:40<1:54:46,  5.65it/s][INFO|trainer.py:722] 2023-07-17 17:24:31,047 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:24:31,048 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:24:31,048 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:24:31,048 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.02it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.00it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.03it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.92it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 39.64it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 36.51it/s]\u001b[A07/17/2023 17:24:31 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3213739395141602, 'eval_accuracy': 0.7333548804137039, 'eval_runtime': 0.8677, 'eval_samples_per_second': 576.224, 'eval_steps_per_second': 36.878, 'epoch': 16.09}\n",
      " 54%|█████████████████▋               | 45000/83910 [2:26:41<1:54:46,  5.65it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 36.51it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:24:31,918 >> Saving model checkpoint to roberta-large-movies/checkpoint-45000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:24:31,919 >> Configuration saved in roberta-large-movies/checkpoint-45000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:24:33,591 >> Model weights saved in roberta-large-movies/checkpoint-45000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:24:33,591 >> tokenizer config file saved in roberta-large-movies/checkpoint-45000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:24:33,592 >> Special tokens file saved in roberta-large-movies/checkpoint-45000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:24:37,476 >> Deleting older checkpoint [roberta-large-movies/checkpoint-43500] due to args.save_total_limit\n",
      "{'loss': 1.0924, 'learning_rate': 2.2900726969371946e-05, 'epoch': 16.27}       \n",
      " 54%|█████████████████▉               | 45500/83910 [2:28:18<1:56:43,  5.48it/s][INFO|trainer.py:722] 2023-07-17 17:26:09,001 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:26:09,003 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:26:09,003 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:26:09,003 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.70it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.81it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.97it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.89it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.01it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 31.85it/s]\u001b[A07/17/2023 17:26:09 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.384667992591858, 'eval_accuracy': 0.7282392026578073, 'eval_runtime': 0.9421, 'eval_samples_per_second': 530.704, 'eval_steps_per_second': 33.965, 'epoch': 16.27}\n",
      " 54%|█████████████████▉               | 45500/83910 [2:28:19<1:56:43,  5.48it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 31.85it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:26:09,947 >> Saving model checkpoint to roberta-large-movies/checkpoint-45500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:26:09,948 >> Configuration saved in roberta-large-movies/checkpoint-45500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:26:11,571 >> Model weights saved in roberta-large-movies/checkpoint-45500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:26:11,572 >> tokenizer config file saved in roberta-large-movies/checkpoint-45500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:26:11,572 >> Special tokens file saved in roberta-large-movies/checkpoint-45500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:26:15,490 >> Deleting older checkpoint [roberta-large-movies/checkpoint-44000] due to args.save_total_limit\n",
      "{'loss': 1.0915, 'learning_rate': 2.260278870218091e-05, 'epoch': 16.45}        \n",
      " 55%|██████████████████               | 46000/83910 [2:29:56<1:59:58,  5.27it/s][INFO|trainer.py:722] 2023-07-17 17:27:46,746 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:27:46,748 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:27:46,748 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:27:46,748 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.45it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.54it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.72it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.57it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.00it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.36it/s]\u001b[A07/17/2023 17:27:47 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3603721857070923, 'eval_accuracy': 0.7317073170731707, 'eval_runtime': 0.7951, 'eval_samples_per_second': 628.874, 'eval_steps_per_second': 40.248, 'epoch': 16.45}\n",
      " 55%|██████████████████               | 46000/83910 [2:29:57<1:59:58,  5.27it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.36it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:27:47,545 >> Saving model checkpoint to roberta-large-movies/checkpoint-46000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:27:47,546 >> Configuration saved in roberta-large-movies/checkpoint-46000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:27:49,279 >> Model weights saved in roberta-large-movies/checkpoint-46000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:27:49,280 >> tokenizer config file saved in roberta-large-movies/checkpoint-46000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:27:49,280 >> Special tokens file saved in roberta-large-movies/checkpoint-46000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:27:53,177 >> Deleting older checkpoint [roberta-large-movies/checkpoint-45000] due to args.save_total_limit\n",
      "{'loss': 1.0968, 'learning_rate': 2.230485043498987e-05, 'epoch': 16.62}        \n",
      " 55%|██████████████████▎              | 46500/83910 [2:31:34<1:53:20,  5.50it/s][INFO|trainer.py:722] 2023-07-17 17:29:24,639 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:29:24,641 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:29:24,641 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:29:24,641 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 47.94it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.63it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 40.92it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 33.43it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 34.30it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 36.56it/s]\u001b[A07/17/2023 17:29:25 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3539705276489258, 'eval_accuracy': 0.7319177173191772, 'eval_runtime': 0.8815, 'eval_samples_per_second': 567.187, 'eval_steps_per_second': 36.3, 'epoch': 16.62}\n",
      " 55%|██████████████████▎              | 46500/83910 [2:31:35<1:53:20,  5.50it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 36.56it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:29:25,524 >> Saving model checkpoint to roberta-large-movies/checkpoint-46500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:29:25,525 >> Configuration saved in roberta-large-movies/checkpoint-46500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:29:27,200 >> Model weights saved in roberta-large-movies/checkpoint-46500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:29:27,201 >> tokenizer config file saved in roberta-large-movies/checkpoint-46500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:29:27,201 >> Special tokens file saved in roberta-large-movies/checkpoint-46500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:29:30,994 >> Deleting older checkpoint [roberta-large-movies/checkpoint-45500] due to args.save_total_limit\n",
      "{'loss': 1.0772, 'learning_rate': 2.2006912167798833e-05, 'epoch': 16.8}        \n",
      " 56%|██████████████████▍              | 47000/83910 [2:33:12<1:51:55,  5.50it/s][INFO|trainer.py:722] 2023-07-17 17:31:02,623 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:31:02,624 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:31:02,624 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:31:02,624 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.83it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.90it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.93it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 39.53it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 37.75it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 38.39it/s]\u001b[A07/17/2023 17:31:03 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2475004196166992, 'eval_accuracy': 0.7306332369013179, 'eval_runtime': 0.8301, 'eval_samples_per_second': 602.308, 'eval_steps_per_second': 38.548, 'epoch': 16.8}\n",
      " 56%|██████████████████▍              | 47000/83910 [2:33:13<1:51:55,  5.50it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.39it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:31:03,456 >> Saving model checkpoint to roberta-large-movies/checkpoint-47000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:31:03,458 >> Configuration saved in roberta-large-movies/checkpoint-47000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:31:05,141 >> Model weights saved in roberta-large-movies/checkpoint-47000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:31:05,142 >> tokenizer config file saved in roberta-large-movies/checkpoint-47000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:31:05,142 >> Special tokens file saved in roberta-large-movies/checkpoint-47000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:31:09,007 >> Deleting older checkpoint [roberta-large-movies/checkpoint-46000] due to args.save_total_limit\n",
      "{'loss': 1.0975, 'learning_rate': 2.1708973900607796e-05, 'epoch': 16.98}       \n",
      " 57%|██████████████████▋              | 47500/83910 [2:34:50<1:48:29,  5.59it/s][INFO|trainer.py:722] 2023-07-17 17:32:40,935 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:32:40,937 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:32:40,937 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:32:40,938 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 42.41it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 42.00it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 40.34it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 40.43it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.10it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 40.43it/s]\u001b[A07/17/2023 17:32:41 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2635700702667236, 'eval_accuracy': 0.7448207826372903, 'eval_runtime': 0.8269, 'eval_samples_per_second': 604.655, 'eval_steps_per_second': 38.698, 'epoch': 16.98}\n",
      " 57%|██████████████████▋              | 47500/83910 [2:34:51<1:48:29,  5.59it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.43it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:32:41,766 >> Saving model checkpoint to roberta-large-movies/checkpoint-47500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:32:41,767 >> Configuration saved in roberta-large-movies/checkpoint-47500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:32:43,465 >> Model weights saved in roberta-large-movies/checkpoint-47500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:32:43,466 >> tokenizer config file saved in roberta-large-movies/checkpoint-47500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:32:43,466 >> Special tokens file saved in roberta-large-movies/checkpoint-47500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:32:47,242 >> Deleting older checkpoint [roberta-large-movies/checkpoint-44500] due to args.save_total_limit\n",
      "{'loss': 1.0708, 'learning_rate': 2.1411035633416755e-05, 'epoch': 17.16}       \n",
      " 57%|██████████████████▉              | 48000/83910 [2:36:28<1:49:48,  5.45it/s][INFO|trainer.py:722] 2023-07-17 17:34:18,690 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:34:18,691 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:34:18,691 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:34:18,691 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 47.70it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 43.87it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.27it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 38.21it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 32.96it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 33.78it/s]\u001b[A07/17/2023 17:34:19 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4056382179260254, 'eval_accuracy': 0.7182085648904871, 'eval_runtime': 0.8973, 'eval_samples_per_second': 557.236, 'eval_steps_per_second': 35.663, 'epoch': 17.16}\n",
      " 57%|██████████████████▉              | 48000/83910 [2:36:29<1:49:48,  5.45it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 33.78it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:34:19,590 >> Saving model checkpoint to roberta-large-movies/checkpoint-48000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:34:19,591 >> Configuration saved in roberta-large-movies/checkpoint-48000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:34:21,290 >> Model weights saved in roberta-large-movies/checkpoint-48000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:34:21,291 >> tokenizer config file saved in roberta-large-movies/checkpoint-48000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:34:21,291 >> Special tokens file saved in roberta-large-movies/checkpoint-48000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:34:24,880 >> Deleting older checkpoint [roberta-large-movies/checkpoint-46500] due to args.save_total_limit\n",
      "{'loss': 1.0654, 'learning_rate': 2.111309736622572e-05, 'epoch': 17.34}        \n",
      " 58%|███████████████████              | 48500/83910 [2:38:06<1:49:08,  5.41it/s][INFO|trainer.py:722] 2023-07-17 17:35:56,555 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:35:56,557 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:35:56,557 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:35:56,557 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.94it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.74it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.11it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 37.72it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 36.61it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 38.35it/s]\u001b[A07/17/2023 17:35:57 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3769292831420898, 'eval_accuracy': 0.727630285152409, 'eval_runtime': 0.8377, 'eval_samples_per_second': 596.886, 'eval_steps_per_second': 38.201, 'epoch': 17.34}\n",
      " 58%|███████████████████              | 48500/83910 [2:38:07<1:49:08,  5.41it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.35it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:35:57,396 >> Saving model checkpoint to roberta-large-movies/checkpoint-48500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:35:57,397 >> Configuration saved in roberta-large-movies/checkpoint-48500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:35:59,109 >> Model weights saved in roberta-large-movies/checkpoint-48500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:35:59,109 >> tokenizer config file saved in roberta-large-movies/checkpoint-48500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:35:59,110 >> Special tokens file saved in roberta-large-movies/checkpoint-48500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:36:02,967 >> Deleting older checkpoint [roberta-large-movies/checkpoint-47000] due to args.save_total_limit\n",
      "{'loss': 1.0676, 'learning_rate': 2.081515909903468e-05, 'epoch': 17.52}        \n",
      " 58%|███████████████████▎             | 49000/83910 [2:39:44<1:47:27,  5.41it/s][INFO|trainer.py:722] 2023-07-17 17:37:34,336 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:37:34,338 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:37:34,338 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:37:34,338 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.72it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.61it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.07it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.04it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.28it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.37it/s]\u001b[A07/17/2023 17:37:35 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.33571457862854, 'eval_accuracy': 0.7224234441883438, 'eval_runtime': 0.7909, 'eval_samples_per_second': 632.166, 'eval_steps_per_second': 40.459, 'epoch': 17.52}\n",
      " 58%|███████████████████▎             | 49000/83910 [2:39:45<1:47:27,  5.41it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.37it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:37:35,131 >> Saving model checkpoint to roberta-large-movies/checkpoint-49000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:37:35,132 >> Configuration saved in roberta-large-movies/checkpoint-49000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:37:36,923 >> Model weights saved in roberta-large-movies/checkpoint-49000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:37:36,923 >> tokenizer config file saved in roberta-large-movies/checkpoint-49000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:37:36,924 >> Special tokens file saved in roberta-large-movies/checkpoint-49000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:37:40,773 >> Deleting older checkpoint [roberta-large-movies/checkpoint-48000] due to args.save_total_limit\n",
      "{'loss': 1.0507, 'learning_rate': 2.0517220831843643e-05, 'epoch': 17.7}        \n",
      " 59%|███████████████████▍             | 49500/83910 [2:41:21<1:47:46,  5.32it/s][INFO|trainer.py:722] 2023-07-17 17:39:11,882 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:39:11,883 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:39:11,883 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:39:11,883 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.04it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.83it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.02it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.55it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.00it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.22it/s]\u001b[A07/17/2023 17:39:12 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4087713956832886, 'eval_accuracy': 0.712369109947644, 'eval_runtime': 0.7955, 'eval_samples_per_second': 628.504, 'eval_steps_per_second': 40.224, 'epoch': 17.7}\n",
      " 59%|███████████████████▍             | 49500/83910 [2:41:22<1:47:46,  5.32it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.22it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:39:12,681 >> Saving model checkpoint to roberta-large-movies/checkpoint-49500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:39:12,681 >> Configuration saved in roberta-large-movies/checkpoint-49500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:39:14,395 >> Model weights saved in roberta-large-movies/checkpoint-49500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:39:14,396 >> tokenizer config file saved in roberta-large-movies/checkpoint-49500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:39:14,396 >> Special tokens file saved in roberta-large-movies/checkpoint-49500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:39:18,221 >> Deleting older checkpoint [roberta-large-movies/checkpoint-48500] due to args.save_total_limit\n",
      "{'loss': 1.0424, 'learning_rate': 2.0219282564652605e-05, 'epoch': 17.88}       \n",
      " 60%|███████████████████▋             | 50000/83910 [2:42:59<1:41:55,  5.55it/s][INFO|trainer.py:722] 2023-07-17 17:40:49,215 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:40:49,217 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:40:49,217 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:40:49,217 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.82it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.09it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.24it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.21it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.40it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.32it/s]\u001b[A07/17/2023 17:40:50 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3146371841430664, 'eval_accuracy': 0.7314667515112949, 'eval_runtime': 0.7881, 'eval_samples_per_second': 634.428, 'eval_steps_per_second': 40.603, 'epoch': 17.88}\n",
      " 60%|███████████████████▋             | 50000/83910 [2:42:59<1:41:55,  5.55it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.32it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:40:50,007 >> Saving model checkpoint to roberta-large-movies/checkpoint-50000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:40:50,008 >> Configuration saved in roberta-large-movies/checkpoint-50000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:40:51,770 >> Model weights saved in roberta-large-movies/checkpoint-50000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:40:51,771 >> tokenizer config file saved in roberta-large-movies/checkpoint-50000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:40:51,771 >> Special tokens file saved in roberta-large-movies/checkpoint-50000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:40:55,662 >> Deleting older checkpoint [roberta-large-movies/checkpoint-49000] due to args.save_total_limit\n",
      "{'loss': 1.0524, 'learning_rate': 1.9921344297461568e-05, 'epoch': 18.06}       \n",
      " 60%|███████████████████▊             | 50500/83910 [2:44:36<1:38:41,  5.64it/s][INFO|trainer.py:722] 2023-07-17 17:42:27,119 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:42:27,121 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:42:27,121 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:42:27,121 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.04it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.70it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.06it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.93it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 35.60it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 35.79it/s]\u001b[A07/17/2023 17:42:27 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.28960382938385, 'eval_accuracy': 0.7393395319012503, 'eval_runtime': 0.8581, 'eval_samples_per_second': 582.683, 'eval_steps_per_second': 37.292, 'epoch': 18.06}\n",
      " 60%|███████████████████▊             | 50500/83910 [2:44:37<1:38:41,  5.64it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 35.79it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:42:27,980 >> Saving model checkpoint to roberta-large-movies/checkpoint-50500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:42:27,981 >> Configuration saved in roberta-large-movies/checkpoint-50500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:42:29,639 >> Model weights saved in roberta-large-movies/checkpoint-50500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:42:29,640 >> tokenizer config file saved in roberta-large-movies/checkpoint-50500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:42:29,640 >> Special tokens file saved in roberta-large-movies/checkpoint-50500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:42:33,471 >> Deleting older checkpoint [roberta-large-movies/checkpoint-49500] due to args.save_total_limit\n",
      "{'loss': 1.0349, 'learning_rate': 1.962340603027053e-05, 'epoch': 18.23}        \n",
      " 61%|████████████████████             | 51000/83910 [2:46:14<1:40:18,  5.47it/s][INFO|trainer.py:722] 2023-07-17 17:44:04,452 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:44:04,453 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:44:04,454 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:44:04,454 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.12it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.33it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.24it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.92it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.32it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.23it/s]\u001b[A07/17/2023 17:44:05 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3986730575561523, 'eval_accuracy': 0.7191558441558441, 'eval_runtime': 0.7904, 'eval_samples_per_second': 632.599, 'eval_steps_per_second': 40.486, 'epoch': 18.23}\n",
      " 61%|████████████████████             | 51000/83910 [2:46:15<1:40:18,  5.47it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.23it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:44:05,245 >> Saving model checkpoint to roberta-large-movies/checkpoint-51000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:44:05,246 >> Configuration saved in roberta-large-movies/checkpoint-51000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:44:07,035 >> Model weights saved in roberta-large-movies/checkpoint-51000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:44:07,036 >> tokenizer config file saved in roberta-large-movies/checkpoint-51000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:44:07,036 >> Special tokens file saved in roberta-large-movies/checkpoint-51000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:44:10,776 >> Deleting older checkpoint [roberta-large-movies/checkpoint-50000] due to args.save_total_limit\n",
      "{'loss': 1.0217, 'learning_rate': 1.932546776307949e-05, 'epoch': 18.41}        \n",
      " 61%|████████████████████▎            | 51500/83910 [2:47:52<1:53:49,  4.75it/s][INFO|trainer.py:722] 2023-07-17 17:45:42,347 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:45:42,349 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:45:42,349 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:45:42,349 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.90it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.97it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 38.74it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:00<00:00, 34.84it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:00<00:00, 36.08it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 38.06it/s]\u001b[A07/17/2023 17:45:43 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2937612533569336, 'eval_accuracy': 0.7380645161290322, 'eval_runtime': 0.8575, 'eval_samples_per_second': 583.089, 'eval_steps_per_second': 37.318, 'epoch': 18.41}\n",
      " 61%|████████████████████▎            | 51500/83910 [2:47:53<1:53:49,  4.75it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.06it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:45:43,208 >> Saving model checkpoint to roberta-large-movies/checkpoint-51500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:45:43,209 >> Configuration saved in roberta-large-movies/checkpoint-51500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:45:44,999 >> Model weights saved in roberta-large-movies/checkpoint-51500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:45:44,999 >> tokenizer config file saved in roberta-large-movies/checkpoint-51500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:45:45,000 >> Special tokens file saved in roberta-large-movies/checkpoint-51500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:45:48,771 >> Deleting older checkpoint [roberta-large-movies/checkpoint-50500] due to args.save_total_limit\n",
      "{'loss': 1.0238, 'learning_rate': 1.9028125372422833e-05, 'epoch': 18.59}       \n",
      " 62%|████████████████████▍            | 52000/83910 [2:49:30<1:38:02,  5.43it/s][INFO|trainer.py:722] 2023-07-17 17:47:20,314 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:47:20,315 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:47:20,315 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:47:20,315 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.37it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.29it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.49it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 38.50it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 36.19it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 37.72it/s]\u001b[A07/17/2023 17:47:21 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.296163558959961, 'eval_accuracy': 0.738654650788542, 'eval_runtime': 0.8423, 'eval_samples_per_second': 593.617, 'eval_steps_per_second': 37.992, 'epoch': 18.59}\n",
      " 62%|████████████████████▍            | 52000/83910 [2:49:31<1:38:02,  5.43it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 37.72it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:47:21,159 >> Saving model checkpoint to roberta-large-movies/checkpoint-52000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:47:21,160 >> Configuration saved in roberta-large-movies/checkpoint-52000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:47:22,874 >> Model weights saved in roberta-large-movies/checkpoint-52000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:47:22,875 >> tokenizer config file saved in roberta-large-movies/checkpoint-52000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:47:22,876 >> Special tokens file saved in roberta-large-movies/checkpoint-52000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:47:26,843 >> Deleting older checkpoint [roberta-large-movies/checkpoint-51000] due to args.save_total_limit\n",
      "{'loss': 1.0292, 'learning_rate': 1.87301871052318e-05, 'epoch': 18.77}         \n",
      " 63%|████████████████████▋            | 52500/83910 [2:51:08<1:33:35,  5.59it/s][INFO|trainer.py:722] 2023-07-17 17:48:58,383 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:48:58,385 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:48:58,385 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:48:58,385 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.22it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.83it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.09it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 40.12it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 38.45it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 39.40it/s]\u001b[A07/17/2023 17:48:59 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3194587230682373, 'eval_accuracy': 0.737131757850437, 'eval_runtime': 0.8232, 'eval_samples_per_second': 607.358, 'eval_steps_per_second': 38.871, 'epoch': 18.77}\n",
      " 63%|████████████████████▋            | 52500/83910 [2:51:09<1:33:35,  5.59it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 39.40it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:48:59,210 >> Saving model checkpoint to roberta-large-movies/checkpoint-52500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:48:59,211 >> Configuration saved in roberta-large-movies/checkpoint-52500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:49:00,854 >> Model weights saved in roberta-large-movies/checkpoint-52500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:49:00,855 >> tokenizer config file saved in roberta-large-movies/checkpoint-52500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:49:00,855 >> Special tokens file saved in roberta-large-movies/checkpoint-52500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:49:04,590 >> Deleting older checkpoint [roberta-large-movies/checkpoint-51500] due to args.save_total_limit\n",
      "{'loss': 1.0426, 'learning_rate': 1.8433440591109523e-05, 'epoch': 18.95}       \n",
      " 63%|████████████████████▊            | 53000/83910 [2:52:45<1:35:31,  5.39it/s][INFO|trainer.py:722] 2023-07-17 17:50:35,789 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:50:35,790 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:50:35,791 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:50:35,791 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.01it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.57it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.84it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.83it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.93it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.90it/s]\u001b[A07/17/2023 17:50:36 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2835460901260376, 'eval_accuracy': 0.7411687025420931, 'eval_runtime': 0.7859, 'eval_samples_per_second': 636.221, 'eval_steps_per_second': 40.718, 'epoch': 18.95}\n",
      " 63%|████████████████████▊            | 53000/83910 [2:52:46<1:35:31,  5.39it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.90it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:50:36,582 >> Saving model checkpoint to roberta-large-movies/checkpoint-53000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:50:36,592 >> Configuration saved in roberta-large-movies/checkpoint-53000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:50:38,298 >> Model weights saved in roberta-large-movies/checkpoint-53000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:50:38,300 >> tokenizer config file saved in roberta-large-movies/checkpoint-53000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:50:38,300 >> Special tokens file saved in roberta-large-movies/checkpoint-53000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:50:42,000 >> Deleting older checkpoint [roberta-large-movies/checkpoint-52000] due to args.save_total_limit\n",
      "{'loss': 1.0196, 'learning_rate': 1.8135502323918486e-05, 'epoch': 19.13}       \n",
      " 64%|█████████████████████            | 53500/83910 [2:54:23<1:30:15,  5.62it/s][INFO|trainer.py:722] 2023-07-17 17:52:13,702 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:52:13,704 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:52:13,704 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:52:13,704 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 47.72it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 37.34it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:00<00:00, 36.29it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:00<00:00, 38.73it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 39.25it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 40.06it/s]\u001b[A07/17/2023 17:52:14 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.234621524810791, 'eval_accuracy': 0.747275204359673, 'eval_runtime': 0.8361, 'eval_samples_per_second': 597.997, 'eval_steps_per_second': 38.272, 'epoch': 19.13}\n",
      " 64%|█████████████████████            | 53500/83910 [2:54:24<1:30:15,  5.62it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.06it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:52:14,542 >> Saving model checkpoint to roberta-large-movies/checkpoint-53500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:52:14,543 >> Configuration saved in roberta-large-movies/checkpoint-53500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:52:16,193 >> Model weights saved in roberta-large-movies/checkpoint-53500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:52:16,194 >> tokenizer config file saved in roberta-large-movies/checkpoint-53500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:52:16,194 >> Special tokens file saved in roberta-large-movies/checkpoint-53500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:52:19,908 >> Deleting older checkpoint [roberta-large-movies/checkpoint-47500] due to args.save_total_limit\n",
      "{'loss': 1.012, 'learning_rate': 1.7837564056727445e-05, 'epoch': 19.31}        \n",
      " 64%|█████████████████████▏           | 54000/83910 [2:56:01<1:30:08,  5.53it/s][INFO|trainer.py:722] 2023-07-17 17:53:52,034 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:53:52,036 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:53:52,036 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:53:52,036 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 42.59it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 42.92it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.20it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.54it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.99it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.35it/s]\u001b[A07/17/2023 17:53:52 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3665757179260254, 'eval_accuracy': 0.7338292367399741, 'eval_runtime': 0.8157, 'eval_samples_per_second': 612.938, 'eval_steps_per_second': 39.228, 'epoch': 19.31}\n",
      " 64%|█████████████████████▏           | 54000/83910 [2:56:02<1:30:08,  5.53it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.35it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:53:52,853 >> Saving model checkpoint to roberta-large-movies/checkpoint-54000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:53:52,854 >> Configuration saved in roberta-large-movies/checkpoint-54000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:53:54,534 >> Model weights saved in roberta-large-movies/checkpoint-54000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:53:54,534 >> tokenizer config file saved in roberta-large-movies/checkpoint-54000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:53:54,535 >> Special tokens file saved in roberta-large-movies/checkpoint-54000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:53:58,372 >> Deleting older checkpoint [roberta-large-movies/checkpoint-52500] due to args.save_total_limit\n",
      "{'loss': 1.0256, 'learning_rate': 1.753962578953641e-05, 'epoch': 19.49}        \n",
      " 65%|█████████████████████▍           | 54500/83910 [2:57:39<1:31:14,  5.37it/s][INFO|trainer.py:722] 2023-07-17 17:55:29,464 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:55:29,466 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:55:29,466 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:55:29,466 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.88it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.69it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.80it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.65it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.12it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.40it/s]\u001b[A07/17/2023 17:55:30 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3140363693237305, 'eval_accuracy': 0.7364842991259307, 'eval_runtime': 0.7949, 'eval_samples_per_second': 628.974, 'eval_steps_per_second': 40.254, 'epoch': 19.49}\n",
      " 65%|█████████████████████▍           | 54500/83910 [2:57:40<1:31:14,  5.37it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.40it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:55:30,263 >> Saving model checkpoint to roberta-large-movies/checkpoint-54500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:55:30,264 >> Configuration saved in roberta-large-movies/checkpoint-54500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:55:32,022 >> Model weights saved in roberta-large-movies/checkpoint-54500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:55:32,022 >> tokenizer config file saved in roberta-large-movies/checkpoint-54500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:55:32,023 >> Special tokens file saved in roberta-large-movies/checkpoint-54500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:55:35,868 >> Deleting older checkpoint [roberta-large-movies/checkpoint-53000] due to args.save_total_limit\n",
      "{'loss': 0.9824, 'learning_rate': 1.724168752234537e-05, 'epoch': 19.66}        \n",
      " 66%|█████████████████████▋           | 55000/83910 [2:59:16<1:25:23,  5.64it/s][INFO|trainer.py:722] 2023-07-17 17:57:07,028 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:57:07,029 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:57:07,029 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:57:07,029 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.68it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.97it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.14it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.90it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.14it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 40.47it/s]\u001b[A07/17/2023 17:57:07 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2764383554458618, 'eval_accuracy': 0.7416496250852079, 'eval_runtime': 0.8178, 'eval_samples_per_second': 611.417, 'eval_steps_per_second': 39.131, 'epoch': 19.66}\n",
      " 66%|█████████████████████▋           | 55000/83910 [2:59:17<1:25:23,  5.64it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.47it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:57:07,855 >> Saving model checkpoint to roberta-large-movies/checkpoint-55000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:57:07,861 >> Configuration saved in roberta-large-movies/checkpoint-55000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:57:09,574 >> Model weights saved in roberta-large-movies/checkpoint-55000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:57:09,575 >> tokenizer config file saved in roberta-large-movies/checkpoint-55000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:57:09,575 >> Special tokens file saved in roberta-large-movies/checkpoint-55000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:57:13,369 >> Deleting older checkpoint [roberta-large-movies/checkpoint-54000] due to args.save_total_limit\n",
      "{'loss': 1.0048, 'learning_rate': 1.6943749255154336e-05, 'epoch': 19.84}       \n",
      " 66%|█████████████████████▊           | 55500/83910 [3:00:54<1:25:25,  5.54it/s][INFO|trainer.py:722] 2023-07-17 17:58:44,933 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 17:58:44,934 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 17:58:44,934 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 17:58:44,934 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.80it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.61it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 40.67it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 39.54it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 39.31it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 40.19it/s]\u001b[A07/17/2023 17:58:45 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2514091730117798, 'eval_accuracy': 0.7487891507910881, 'eval_runtime': 0.8164, 'eval_samples_per_second': 612.474, 'eval_steps_per_second': 39.198, 'epoch': 19.84}\n",
      " 66%|█████████████████████▊           | 55500/83910 [3:00:55<1:25:25,  5.54it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.19it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 17:58:45,752 >> Saving model checkpoint to roberta-large-movies/checkpoint-55500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 17:58:45,753 >> Configuration saved in roberta-large-movies/checkpoint-55500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 17:58:47,439 >> Model weights saved in roberta-large-movies/checkpoint-55500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 17:58:47,439 >> tokenizer config file saved in roberta-large-movies/checkpoint-55500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 17:58:47,440 >> Special tokens file saved in roberta-large-movies/checkpoint-55500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 17:58:51,166 >> Deleting older checkpoint [roberta-large-movies/checkpoint-53500] due to args.save_total_limit\n",
      "{'loss': 0.9947, 'learning_rate': 1.6645810987963295e-05, 'epoch': 20.02}       \n",
      " 67%|██████████████████████           | 56000/83910 [3:02:31<1:24:55,  5.48it/s][INFO|trainer.py:722] 2023-07-17 18:00:22,118 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:00:22,119 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:00:22,119 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:00:22,119 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.89it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.66it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.09it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.97it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.22it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.43it/s]\u001b[A07/17/2023 18:00:22 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3350915908813477, 'eval_accuracy': 0.7431572246976448, 'eval_runtime': 0.7912, 'eval_samples_per_second': 631.988, 'eval_steps_per_second': 40.447, 'epoch': 20.02}\n",
      " 67%|██████████████████████           | 56000/83910 [3:02:32<1:24:55,  5.48it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.43it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:00:22,912 >> Saving model checkpoint to roberta-large-movies/checkpoint-56000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:00:22,913 >> Configuration saved in roberta-large-movies/checkpoint-56000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:00:24,641 >> Model weights saved in roberta-large-movies/checkpoint-56000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:00:24,642 >> tokenizer config file saved in roberta-large-movies/checkpoint-56000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:00:24,642 >> Special tokens file saved in roberta-large-movies/checkpoint-56000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:00:28,680 >> Deleting older checkpoint [roberta-large-movies/checkpoint-54500] due to args.save_total_limit\n",
      "{'loss': 0.977, 'learning_rate': 1.634846859730664e-05, 'epoch': 20.2}          \n",
      " 67%|██████████████████████▏          | 56500/83910 [3:04:10<1:25:25,  5.35it/s][INFO|trainer.py:722] 2023-07-17 18:02:00,230 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:02:00,232 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:02:00,232 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:02:00,232 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.65it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.08it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.79it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 35.95it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 36.63it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 38.16it/s]\u001b[A07/17/2023 18:02:01 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2854044437408447, 'eval_accuracy': 0.7451045469631596, 'eval_runtime': 0.8499, 'eval_samples_per_second': 588.28, 'eval_steps_per_second': 37.65, 'epoch': 20.2}\n",
      " 67%|██████████████████████▏          | 56500/83910 [3:04:10<1:25:25,  5.35it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.16it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:02:01,083 >> Saving model checkpoint to roberta-large-movies/checkpoint-56500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:02:01,084 >> Configuration saved in roberta-large-movies/checkpoint-56500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:02:02,776 >> Model weights saved in roberta-large-movies/checkpoint-56500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:02:02,777 >> tokenizer config file saved in roberta-large-movies/checkpoint-56500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:02:02,777 >> Special tokens file saved in roberta-large-movies/checkpoint-56500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:02:06,611 >> Deleting older checkpoint [roberta-large-movies/checkpoint-55000] due to args.save_total_limit\n",
      "{'loss': 0.9862, 'learning_rate': 1.60505303301156e-05, 'epoch': 20.38}         \n",
      " 68%|██████████████████████▍          | 57000/83910 [3:05:48<1:20:48,  5.55it/s][INFO|trainer.py:722] 2023-07-17 18:03:38,175 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:03:38,176 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:03:38,176 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:03:38,176 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.25it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.27it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.15it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.55it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 39.10it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 38.88it/s]\u001b[A07/17/2023 18:03:38 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.366584300994873, 'eval_accuracy': 0.7285475792988314, 'eval_runtime': 0.816, 'eval_samples_per_second': 612.774, 'eval_steps_per_second': 39.218, 'epoch': 20.38}\n",
      " 68%|██████████████████████▍          | 57000/83910 [3:05:48<1:20:48,  5.55it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.88it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:03:38,994 >> Saving model checkpoint to roberta-large-movies/checkpoint-57000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:03:38,995 >> Configuration saved in roberta-large-movies/checkpoint-57000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:03:40,691 >> Model weights saved in roberta-large-movies/checkpoint-57000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:03:40,692 >> tokenizer config file saved in roberta-large-movies/checkpoint-57000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:03:40,692 >> Special tokens file saved in roberta-large-movies/checkpoint-57000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:03:44,498 >> Deleting older checkpoint [roberta-large-movies/checkpoint-56000] due to args.save_total_limit\n",
      "{'loss': 0.9699, 'learning_rate': 1.5752592062924564e-05, 'epoch': 20.56}       \n",
      " 69%|██████████████████████▌          | 57500/83910 [3:07:25<1:21:11,  5.42it/s][INFO|trainer.py:722] 2023-07-17 18:05:15,698 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:05:15,699 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:05:15,699 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:05:15,699 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.85it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.51it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.73it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.72it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.89it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 42.01it/s]\u001b[A07/17/2023 18:05:16 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3123427629470825, 'eval_accuracy': 0.7347811780190853, 'eval_runtime': 0.7779, 'eval_samples_per_second': 642.731, 'eval_steps_per_second': 41.135, 'epoch': 20.56}\n",
      " 69%|██████████████████████▌          | 57500/83910 [3:07:26<1:21:11,  5.42it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 42.01it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:05:16,479 >> Saving model checkpoint to roberta-large-movies/checkpoint-57500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:05:16,479 >> Configuration saved in roberta-large-movies/checkpoint-57500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:05:18,152 >> Model weights saved in roberta-large-movies/checkpoint-57500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:05:18,153 >> tokenizer config file saved in roberta-large-movies/checkpoint-57500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:05:18,154 >> Special tokens file saved in roberta-large-movies/checkpoint-57500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:05:21,887 >> Deleting older checkpoint [roberta-large-movies/checkpoint-56500] due to args.save_total_limit\n",
      "{'loss': 0.977, 'learning_rate': 1.5454653795733526e-05, 'epoch': 20.74}        \n",
      " 69%|██████████████████████▊          | 58000/83910 [3:09:02<1:25:22,  5.06it/s][INFO|trainer.py:722] 2023-07-17 18:06:53,109 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:06:53,110 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:06:53,110 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:06:53,110 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.21it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.62it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.54it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.59it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.85it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 39.19it/s]\u001b[A07/17/2023 18:06:53 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3425793647766113, 'eval_accuracy': 0.7254770672915969, 'eval_runtime': 0.8285, 'eval_samples_per_second': 603.485, 'eval_steps_per_second': 38.623, 'epoch': 20.74}\n",
      " 69%|██████████████████████▊          | 58000/83910 [3:09:03<1:25:22,  5.06it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 39.19it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:06:53,943 >> Saving model checkpoint to roberta-large-movies/checkpoint-58000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:06:53,947 >> Configuration saved in roberta-large-movies/checkpoint-58000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:06:55,622 >> Model weights saved in roberta-large-movies/checkpoint-58000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:06:55,622 >> tokenizer config file saved in roberta-large-movies/checkpoint-58000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:06:55,622 >> Special tokens file saved in roberta-large-movies/checkpoint-58000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:06:59,527 >> Deleting older checkpoint [roberta-large-movies/checkpoint-57000] due to args.save_total_limit\n",
      "{'loss': 0.9749, 'learning_rate': 1.5157311405076868e-05, 'epoch': 20.92}       \n",
      " 70%|███████████████████████          | 58500/83910 [3:10:40<1:15:18,  5.62it/s][INFO|trainer.py:722] 2023-07-17 18:08:30,897 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:08:30,898 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:08:30,898 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:08:30,898 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.84it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.07it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.39it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.48it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.71it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.69it/s]\u001b[A07/17/2023 18:08:31 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3763371706008911, 'eval_accuracy': 0.7296604740550929, 'eval_runtime': 0.7855, 'eval_samples_per_second': 636.556, 'eval_steps_per_second': 40.74, 'epoch': 20.92}\n",
      " 70%|███████████████████████          | 58500/83910 [3:10:41<1:15:18,  5.62it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.69it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:08:31,685 >> Saving model checkpoint to roberta-large-movies/checkpoint-58500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:08:31,686 >> Configuration saved in roberta-large-movies/checkpoint-58500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:08:33,437 >> Model weights saved in roberta-large-movies/checkpoint-58500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:08:33,437 >> tokenizer config file saved in roberta-large-movies/checkpoint-58500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:08:33,438 >> Special tokens file saved in roberta-large-movies/checkpoint-58500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:08:37,170 >> Deleting older checkpoint [roberta-large-movies/checkpoint-57500] due to args.save_total_limit\n",
      "{'loss': 0.9505, 'learning_rate': 1.4859373137885832e-05, 'epoch': 21.09}       \n",
      " 70%|███████████████████████▏         | 59000/83910 [3:12:19<1:18:01,  5.32it/s][INFO|trainer.py:722] 2023-07-17 18:10:09,159 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:10:09,161 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:10:09,162 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:10:09,162 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.14it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.96it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.80it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.76it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.02it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.33it/s]\u001b[A07/17/2023 18:10:09 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2372225522994995, 'eval_accuracy': 0.7434469200524246, 'eval_runtime': 0.7967, 'eval_samples_per_second': 627.592, 'eval_steps_per_second': 40.166, 'epoch': 21.09}\n",
      " 70%|███████████████████████▏         | 59000/83910 [3:12:19<1:18:01,  5.32it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.33it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:10:09,960 >> Saving model checkpoint to roberta-large-movies/checkpoint-59000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:10:09,961 >> Configuration saved in roberta-large-movies/checkpoint-59000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:10:11,601 >> Model weights saved in roberta-large-movies/checkpoint-59000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:10:11,602 >> tokenizer config file saved in roberta-large-movies/checkpoint-59000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:10:11,602 >> Special tokens file saved in roberta-large-movies/checkpoint-59000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:10:15,360 >> Deleting older checkpoint [roberta-large-movies/checkpoint-58000] due to args.save_total_limit\n",
      "{'loss': 0.9438, 'learning_rate': 1.4561434870694793e-05, 'epoch': 21.27}       \n",
      " 71%|███████████████████████▍         | 59500/83910 [3:13:55<1:13:45,  5.52it/s][INFO|trainer.py:722] 2023-07-17 18:11:46,116 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:11:46,118 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:11:46,118 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:11:46,118 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.54it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.79it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.70it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.87it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.14it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.29it/s]\u001b[A07/17/2023 18:11:46 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.433412790298462, 'eval_accuracy': 0.7159090909090909, 'eval_runtime': 0.7929, 'eval_samples_per_second': 630.567, 'eval_steps_per_second': 40.356, 'epoch': 21.27}\n",
      " 71%|███████████████████████▍         | 59500/83910 [3:13:56<1:13:45,  5.52it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.29it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:11:46,912 >> Saving model checkpoint to roberta-large-movies/checkpoint-59500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:11:46,913 >> Configuration saved in roberta-large-movies/checkpoint-59500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:11:48,643 >> Model weights saved in roberta-large-movies/checkpoint-59500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:11:48,644 >> tokenizer config file saved in roberta-large-movies/checkpoint-59500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:11:48,644 >> Special tokens file saved in roberta-large-movies/checkpoint-59500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:11:52,493 >> Deleting older checkpoint [roberta-large-movies/checkpoint-58500] due to args.save_total_limit\n",
      "{'loss': 0.944, 'learning_rate': 1.4263496603503754e-05, 'epoch': 21.45}        \n",
      " 72%|███████████████████████▌         | 60000/83910 [3:15:33<1:11:08,  5.60it/s][INFO|trainer.py:722] 2023-07-17 18:13:23,770 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:13:23,772 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:13:23,772 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:13:23,772 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.39it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.80it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.62it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.25it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.95it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 38.83it/s]\u001b[A07/17/2023 18:13:24 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.269033432006836, 'eval_accuracy': 0.7507936507936508, 'eval_runtime': 0.8274, 'eval_samples_per_second': 604.314, 'eval_steps_per_second': 38.676, 'epoch': 21.45}\n",
      " 72%|███████████████████████▌         | 60000/83910 [3:15:34<1:11:08,  5.60it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.83it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:13:24,602 >> Saving model checkpoint to roberta-large-movies/checkpoint-60000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:13:24,604 >> Configuration saved in roberta-large-movies/checkpoint-60000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:13:26,284 >> Model weights saved in roberta-large-movies/checkpoint-60000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:13:26,285 >> tokenizer config file saved in roberta-large-movies/checkpoint-60000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:13:26,285 >> Special tokens file saved in roberta-large-movies/checkpoint-60000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:13:30,230 >> Deleting older checkpoint [roberta-large-movies/checkpoint-55500] due to args.save_total_limit\n",
      "{'loss': 0.9427, 'learning_rate': 1.3965558336312718e-05, 'epoch': 21.63}       \n",
      " 72%|███████████████████████▊         | 60500/83910 [3:17:11<1:07:30,  5.78it/s][INFO|trainer.py:722] 2023-07-17 18:15:01,370 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:15:01,371 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:15:01,372 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:15:01,372 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.03it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.23it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.72it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.57it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.04it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.28it/s]\u001b[A07/17/2023 18:15:02 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2185914516448975, 'eval_accuracy': 0.7485941118094608, 'eval_runtime': 0.7923, 'eval_samples_per_second': 631.05, 'eval_steps_per_second': 40.387, 'epoch': 21.63}\n",
      " 72%|███████████████████████▊         | 60500/83910 [3:17:12<1:07:30,  5.78it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.28it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:15:02,171 >> Saving model checkpoint to roberta-large-movies/checkpoint-60500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:15:02,174 >> Configuration saved in roberta-large-movies/checkpoint-60500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:15:03,941 >> Model weights saved in roberta-large-movies/checkpoint-60500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:15:03,942 >> tokenizer config file saved in roberta-large-movies/checkpoint-60500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:15:03,942 >> Special tokens file saved in roberta-large-movies/checkpoint-60500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:15:07,674 >> Deleting older checkpoint [roberta-large-movies/checkpoint-59000] due to args.save_total_limit\n",
      "{'loss': 0.9553, 'learning_rate': 1.3667620069121679e-05, 'epoch': 21.81}       \n",
      " 73%|███████████████████████▉         | 61000/83910 [3:18:48<1:11:25,  5.35it/s][INFO|trainer.py:722] 2023-07-17 18:16:38,943 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:16:38,944 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:16:38,945 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:16:38,945 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.21it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.18it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.28it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.20it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.21it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.34it/s]\u001b[A07/17/2023 18:16:39 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3940554857254028, 'eval_accuracy': 0.726882430647292, 'eval_runtime': 0.7961, 'eval_samples_per_second': 628.083, 'eval_steps_per_second': 40.197, 'epoch': 21.81}\n",
      " 73%|███████████████████████▉         | 61000/83910 [3:18:49<1:11:25,  5.35it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.34it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:16:39,744 >> Saving model checkpoint to roberta-large-movies/checkpoint-61000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:16:39,764 >> Configuration saved in roberta-large-movies/checkpoint-61000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:16:41,447 >> Model weights saved in roberta-large-movies/checkpoint-61000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:16:41,447 >> tokenizer config file saved in roberta-large-movies/checkpoint-61000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:16:41,448 >> Special tokens file saved in roberta-large-movies/checkpoint-61000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:16:45,201 >> Deleting older checkpoint [roberta-large-movies/checkpoint-59500] due to args.save_total_limit\n",
      "{'loss': 0.9571, 'learning_rate': 1.3369681801930641e-05, 'epoch': 21.99}       \n",
      " 73%|████████████████████████▏        | 61500/83910 [3:20:27<1:08:27,  5.46it/s][INFO|trainer.py:722] 2023-07-17 18:18:17,443 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:18:17,444 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:18:17,444 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:18:17,444 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 47.06it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.60it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.14it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.12it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.44it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.45it/s]\u001b[A07/17/2023 18:18:18 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.4162867069244385, 'eval_accuracy': 0.7273940607273941, 'eval_runtime': 0.791, 'eval_samples_per_second': 632.128, 'eval_steps_per_second': 40.456, 'epoch': 21.99}\n",
      " 73%|████████████████████████▏        | 61500/83910 [3:20:28<1:08:27,  5.46it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.45it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:18:18,237 >> Saving model checkpoint to roberta-large-movies/checkpoint-61500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:18:18,238 >> Configuration saved in roberta-large-movies/checkpoint-61500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:18:19,957 >> Model weights saved in roberta-large-movies/checkpoint-61500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:18:19,958 >> tokenizer config file saved in roberta-large-movies/checkpoint-61500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:18:19,959 >> Special tokens file saved in roberta-large-movies/checkpoint-61500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:18:23,995 >> Deleting older checkpoint [roberta-large-movies/checkpoint-60500] due to args.save_total_limit\n",
      "{'loss': 0.932, 'learning_rate': 1.3071743534739602e-05, 'epoch': 22.17}        \n",
      " 74%|████████████████████████▍        | 62000/83910 [3:22:05<1:07:43,  5.39it/s][INFO|trainer.py:722] 2023-07-17 18:19:55,250 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:19:55,251 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:19:55,251 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:19:55,251 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.88it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.27it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.93it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.80it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.82it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 40.99it/s]\u001b[A07/17/2023 18:19:56 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2717351913452148, 'eval_accuracy': 0.7522727272727273, 'eval_runtime': 0.796, 'eval_samples_per_second': 628.103, 'eval_steps_per_second': 40.199, 'epoch': 22.17}\n",
      " 74%|████████████████████████▍        | 62000/83910 [3:22:05<1:07:43,  5.39it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.99it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:19:56,049 >> Saving model checkpoint to roberta-large-movies/checkpoint-62000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:19:56,050 >> Configuration saved in roberta-large-movies/checkpoint-62000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:19:57,739 >> Model weights saved in roberta-large-movies/checkpoint-62000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:19:57,740 >> tokenizer config file saved in roberta-large-movies/checkpoint-62000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:19:57,740 >> Special tokens file saved in roberta-large-movies/checkpoint-62000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:20:01,525 >> Deleting older checkpoint [roberta-large-movies/checkpoint-60000] due to args.save_total_limit\n",
      "{'loss': 0.9166, 'learning_rate': 1.2773805267548563e-05, 'epoch': 22.35}       \n",
      " 74%|████████████████████████▌        | 62500/83910 [3:23:42<1:06:24,  5.37it/s][INFO|trainer.py:722] 2023-07-17 18:21:32,953 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:21:32,954 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:21:32,954 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:21:32,954 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.66it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.02it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.28it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.04it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.35it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 38.94it/s]\u001b[A07/17/2023 18:21:33 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.217714786529541, 'eval_accuracy': 0.73956326268465, 'eval_runtime': 0.8289, 'eval_samples_per_second': 603.185, 'eval_steps_per_second': 38.604, 'epoch': 22.35}\n",
      " 74%|████████████████████████▌        | 62500/83910 [3:23:43<1:06:24,  5.37it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.94it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:21:33,787 >> Saving model checkpoint to roberta-large-movies/checkpoint-62500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:21:33,789 >> Configuration saved in roberta-large-movies/checkpoint-62500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:21:35,454 >> Model weights saved in roberta-large-movies/checkpoint-62500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:21:35,455 >> tokenizer config file saved in roberta-large-movies/checkpoint-62500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:21:35,455 >> Special tokens file saved in roberta-large-movies/checkpoint-62500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:21:39,268 >> Deleting older checkpoint [roberta-large-movies/checkpoint-61000] due to args.save_total_limit\n",
      "{'loss': 0.9301, 'learning_rate': 1.2475867000357526e-05, 'epoch': 22.52}       \n",
      " 75%|████████████████████████▊        | 63000/83910 [3:25:20<1:05:03,  5.36it/s][INFO|trainer.py:722] 2023-07-17 18:23:10,859 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:23:10,861 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:23:10,861 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:23:10,861 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.00it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.03it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 35.15it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:00<00:00, 36.19it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:00<00:00, 37.36it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 38.90it/s]\u001b[A07/17/2023 18:23:11 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3264496326446533, 'eval_accuracy': 0.7377950210151956, 'eval_runtime': 0.8524, 'eval_samples_per_second': 586.56, 'eval_steps_per_second': 37.54, 'epoch': 22.52}\n",
      " 75%|████████████████████████▊        | 63000/83910 [3:25:21<1:05:03,  5.36it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.90it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:23:11,715 >> Saving model checkpoint to roberta-large-movies/checkpoint-63000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:23:11,716 >> Configuration saved in roberta-large-movies/checkpoint-63000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:23:13,375 >> Model weights saved in roberta-large-movies/checkpoint-63000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:23:13,376 >> tokenizer config file saved in roberta-large-movies/checkpoint-63000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:23:13,377 >> Special tokens file saved in roberta-large-movies/checkpoint-63000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:23:17,102 >> Deleting older checkpoint [roberta-large-movies/checkpoint-61500] due to args.save_total_limit\n",
      "{'loss': 0.9351, 'learning_rate': 1.2177928733166488e-05, 'epoch': 22.7}        \n",
      " 76%|████████████████████████▉        | 63500/83910 [3:26:58<1:04:02,  5.31it/s][INFO|trainer.py:722] 2023-07-17 18:24:48,181 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:24:48,182 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:24:48,183 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:24:48,183 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.51it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.10it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.41it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.36it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.61it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.72it/s]\u001b[A07/17/2023 18:24:48 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2570440769195557, 'eval_accuracy': 0.752010292698617, 'eval_runtime': 0.785, 'eval_samples_per_second': 636.94, 'eval_steps_per_second': 40.764, 'epoch': 22.7}\n",
      " 76%|████████████████████████▉        | 63500/83910 [3:26:58<1:04:02,  5.31it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.72it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:24:48,969 >> Saving model checkpoint to roberta-large-movies/checkpoint-63500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:24:48,970 >> Configuration saved in roberta-large-movies/checkpoint-63500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:24:50,644 >> Model weights saved in roberta-large-movies/checkpoint-63500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:24:50,644 >> tokenizer config file saved in roberta-large-movies/checkpoint-63500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:24:50,645 >> Special tokens file saved in roberta-large-movies/checkpoint-63500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:24:54,397 >> Deleting older checkpoint [roberta-large-movies/checkpoint-62500] due to args.save_total_limit\n",
      "{'loss': 0.9211, 'learning_rate': 1.1879990465975451e-05, 'epoch': 22.88}       \n",
      " 76%|█████████████████████████▏       | 64000/83910 [3:28:35<1:02:46,  5.29it/s][INFO|trainer.py:722] 2023-07-17 18:26:25,834 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:26:25,835 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:26:25,836 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:26:25,836 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.93it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.08it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 39.86it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 34.47it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 34.82it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 36.77it/s]\u001b[A07/17/2023 18:26:26 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2638896703720093, 'eval_accuracy': 0.75, 'eval_runtime': 0.8753, 'eval_samples_per_second': 571.265, 'eval_steps_per_second': 36.561, 'epoch': 22.88}\n",
      " 76%|█████████████████████████▏       | 64000/83910 [3:28:36<1:02:46,  5.29it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 36.77it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:26:26,712 >> Saving model checkpoint to roberta-large-movies/checkpoint-64000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:26:26,713 >> Configuration saved in roberta-large-movies/checkpoint-64000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:26:28,391 >> Model weights saved in roberta-large-movies/checkpoint-64000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:26:28,392 >> tokenizer config file saved in roberta-large-movies/checkpoint-64000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:26:28,392 >> Special tokens file saved in roberta-large-movies/checkpoint-64000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:26:32,260 >> Deleting older checkpoint [roberta-large-movies/checkpoint-63000] due to args.save_total_limit\n",
      "{'loss': 0.9211, 'learning_rate': 1.1582052198784414e-05, 'epoch': 23.06}       \n",
      " 77%|██████████████████████████▉        | 64500/83910 [3:30:13<55:45,  5.80it/s][INFO|trainer.py:722] 2023-07-17 18:28:03,681 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:28:03,683 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:28:03,683 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:28:03,683 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.93it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.87it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.96it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.64it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.07it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.28it/s]\u001b[A07/17/2023 18:28:04 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2376515865325928, 'eval_accuracy': 0.7605543022881083, 'eval_runtime': 0.7946, 'eval_samples_per_second': 629.265, 'eval_steps_per_second': 40.273, 'epoch': 23.06}\n",
      " 77%|██████████████████████████▉        | 64500/83910 [3:30:14<55:45,  5.80it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.28it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:28:04,479 >> Saving model checkpoint to roberta-large-movies/checkpoint-64500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:28:04,480 >> Configuration saved in roberta-large-movies/checkpoint-64500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:28:06,232 >> Model weights saved in roberta-large-movies/checkpoint-64500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:28:06,233 >> tokenizer config file saved in roberta-large-movies/checkpoint-64500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:28:06,233 >> Special tokens file saved in roberta-large-movies/checkpoint-64500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:28:09,966 >> Deleting older checkpoint [roberta-large-movies/checkpoint-62000] due to args.save_total_limit\n",
      "{'loss': 0.9196, 'learning_rate': 1.1284113931593374e-05, 'epoch': 23.24}       \n",
      " 77%|███████████████████████████        | 65000/83910 [3:31:51<58:30,  5.39it/s][INFO|trainer.py:722] 2023-07-17 18:29:41,136 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:29:41,138 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:29:41,138 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:29:41,138 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.42it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.32it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.32it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.11it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.30it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 36.42it/s]\u001b[A07/17/2023 18:29:41 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2738728523254395, 'eval_accuracy': 0.7485168094924193, 'eval_runtime': 0.8576, 'eval_samples_per_second': 583.036, 'eval_steps_per_second': 37.314, 'epoch': 23.24}\n",
      " 77%|███████████████████████████        | 65000/83910 [3:31:51<58:30,  5.39it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 36.42it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:29:41,999 >> Saving model checkpoint to roberta-large-movies/checkpoint-65000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:29:42,001 >> Configuration saved in roberta-large-movies/checkpoint-65000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:29:43,907 >> Model weights saved in roberta-large-movies/checkpoint-65000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:29:43,908 >> tokenizer config file saved in roberta-large-movies/checkpoint-65000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:29:43,908 >> Special tokens file saved in roberta-large-movies/checkpoint-65000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:29:47,767 >> Deleting older checkpoint [roberta-large-movies/checkpoint-63500] due to args.save_total_limit\n",
      "{'loss': 0.9062, 'learning_rate': 1.098677154093672e-05, 'epoch': 23.42}        \n",
      " 78%|███████████████████████████▎       | 65500/83910 [3:33:28<56:36,  5.42it/s][INFO|trainer.py:722] 2023-07-17 18:31:18,981 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:31:18,982 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:31:18,982 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:31:18,982 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.15it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.58it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.48it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.63it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.30it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 37.55it/s]\u001b[A07/17/2023 18:31:19 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3262896537780762, 'eval_accuracy': 0.7365366010964205, 'eval_runtime': 0.8401, 'eval_samples_per_second': 595.164, 'eval_steps_per_second': 38.09, 'epoch': 23.42}\n",
      " 78%|███████████████████████████▎       | 65500/83910 [3:33:29<56:36,  5.42it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 37.55it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:31:19,824 >> Saving model checkpoint to roberta-large-movies/checkpoint-65500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:31:19,825 >> Configuration saved in roberta-large-movies/checkpoint-65500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:31:21,503 >> Model weights saved in roberta-large-movies/checkpoint-65500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:31:21,503 >> tokenizer config file saved in roberta-large-movies/checkpoint-65500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:31:21,504 >> Special tokens file saved in roberta-large-movies/checkpoint-65500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:31:25,392 >> Deleting older checkpoint [roberta-large-movies/checkpoint-64000] due to args.save_total_limit\n",
      "{'loss': 0.8965, 'learning_rate': 1.068883327374568e-05, 'epoch': 23.6}         \n",
      " 79%|███████████████████████████▌       | 66000/83910 [3:35:06<51:36,  5.78it/s][INFO|trainer.py:722] 2023-07-17 18:32:56,329 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:32:56,330 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:32:56,331 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:32:56,331 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.88it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.70it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.67it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.70it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 42.05it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 42.10it/s]\u001b[A07/17/2023 18:32:57 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2814128398895264, 'eval_accuracy': 0.7455209024552091, 'eval_runtime': 0.778, 'eval_samples_per_second': 642.691, 'eval_steps_per_second': 41.132, 'epoch': 23.6}\n",
      " 79%|███████████████████████████▌       | 66000/83910 [3:35:06<51:36,  5.78it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 42.10it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:32:57,119 >> Saving model checkpoint to roberta-large-movies/checkpoint-66000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:32:57,121 >> Configuration saved in roberta-large-movies/checkpoint-66000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:32:58,875 >> Model weights saved in roberta-large-movies/checkpoint-66000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:32:58,876 >> tokenizer config file saved in roberta-large-movies/checkpoint-66000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:32:58,876 >> Special tokens file saved in roberta-large-movies/checkpoint-66000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:33:03,276 >> Deleting older checkpoint [roberta-large-movies/checkpoint-65000] due to args.save_total_limit\n",
      "{'loss': 0.9004, 'learning_rate': 1.0392086759623406e-05, 'epoch': 23.78}       \n",
      " 79%|███████████████████████████▋       | 66500/83910 [3:36:44<51:32,  5.63it/s][INFO|trainer.py:722] 2023-07-17 18:34:34,981 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:34:34,983 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:34:34,983 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:34:34,983 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.93it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.11it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.18it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 36.06it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 34.59it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 36.44it/s]\u001b[A07/17/2023 18:34:35 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2108628749847412, 'eval_accuracy': 0.7561779242174629, 'eval_runtime': 0.8669, 'eval_samples_per_second': 576.736, 'eval_steps_per_second': 36.911, 'epoch': 23.78}\n",
      " 79%|███████████████████████████▋       | 66500/83910 [3:36:45<51:32,  5.63it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 36.44it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:34:35,851 >> Saving model checkpoint to roberta-large-movies/checkpoint-66500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:34:35,852 >> Configuration saved in roberta-large-movies/checkpoint-66500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:34:37,529 >> Model weights saved in roberta-large-movies/checkpoint-66500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:34:37,530 >> tokenizer config file saved in roberta-large-movies/checkpoint-66500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:34:37,530 >> Special tokens file saved in roberta-large-movies/checkpoint-66500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:34:41,268 >> Deleting older checkpoint [roberta-large-movies/checkpoint-65500] due to args.save_total_limit\n",
      "{'loss': 0.9094, 'learning_rate': 1.0094148492432369e-05, 'epoch': 23.95}       \n",
      " 80%|███████████████████████████▉       | 67000/83910 [3:38:22<51:06,  5.51it/s][INFO|trainer.py:722] 2023-07-17 18:36:12,982 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:36:12,983 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:36:12,984 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:36:12,984 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.12it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.94it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.02it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 40.33it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 35.32it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 35.87it/s]\u001b[A07/17/2023 18:36:13 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2629289627075195, 'eval_accuracy': 0.7528089887640449, 'eval_runtime': 0.8653, 'eval_samples_per_second': 577.859, 'eval_steps_per_second': 36.983, 'epoch': 23.95}\n",
      " 80%|███████████████████████████▉       | 67000/83910 [3:38:23<51:06,  5.51it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 35.87it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:36:13,852 >> Saving model checkpoint to roberta-large-movies/checkpoint-67000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:36:13,857 >> Configuration saved in roberta-large-movies/checkpoint-67000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:36:15,589 >> Model weights saved in roberta-large-movies/checkpoint-67000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:36:15,590 >> tokenizer config file saved in roberta-large-movies/checkpoint-67000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:36:15,590 >> Special tokens file saved in roberta-large-movies/checkpoint-67000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:36:19,339 >> Deleting older checkpoint [roberta-large-movies/checkpoint-66000] due to args.save_total_limit\n",
      "{'loss': 0.8937, 'learning_rate': 9.79621022524133e-06, 'epoch': 24.13}         \n",
      " 80%|████████████████████████████▏      | 67500/83910 [3:40:00<49:11,  5.56it/s][INFO|trainer.py:722] 2023-07-17 18:37:50,719 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:37:50,720 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:37:50,720 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:37:50,720 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.07it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.66it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.09it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.15it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 38.22it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 36.30it/s]\u001b[A07/17/2023 18:37:51 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2770532369613647, 'eval_accuracy': 0.7375168690958165, 'eval_runtime': 0.8492, 'eval_samples_per_second': 588.814, 'eval_steps_per_second': 37.684, 'epoch': 24.13}\n",
      " 80%|████████████████████████████▏      | 67500/83910 [3:40:01<49:11,  5.56it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 36.30it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:37:51,571 >> Saving model checkpoint to roberta-large-movies/checkpoint-67500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:37:51,572 >> Configuration saved in roberta-large-movies/checkpoint-67500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:37:53,205 >> Model weights saved in roberta-large-movies/checkpoint-67500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:37:53,206 >> tokenizer config file saved in roberta-large-movies/checkpoint-67500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:37:53,206 >> Special tokens file saved in roberta-large-movies/checkpoint-67500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:37:56,891 >> Deleting older checkpoint [roberta-large-movies/checkpoint-66500] due to args.save_total_limit\n",
      "{'loss': 0.8711, 'learning_rate': 9.498271958050292e-06, 'epoch': 24.31}        \n",
      " 81%|████████████████████████████▎      | 68000/83910 [3:41:37<48:51,  5.43it/s][INFO|trainer.py:722] 2023-07-17 18:39:27,890 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:39:27,891 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:39:27,892 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:39:27,892 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 47.98it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.64it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.28it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.23it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.26it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.40it/s]\u001b[A07/17/2023 18:39:28 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3746039867401123, 'eval_accuracy': 0.7353233830845771, 'eval_runtime': 0.7929, 'eval_samples_per_second': 630.629, 'eval_steps_per_second': 40.36, 'epoch': 24.31}\n",
      " 81%|████████████████████████████▎      | 68000/83910 [3:41:38<48:51,  5.43it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.40it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:39:28,686 >> Saving model checkpoint to roberta-large-movies/checkpoint-68000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:39:28,687 >> Configuration saved in roberta-large-movies/checkpoint-68000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:39:30,416 >> Model weights saved in roberta-large-movies/checkpoint-68000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:39:30,417 >> tokenizer config file saved in roberta-large-movies/checkpoint-68000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:39:30,417 >> Special tokens file saved in roberta-large-movies/checkpoint-68000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:39:34,128 >> Deleting older checkpoint [roberta-large-movies/checkpoint-67000] due to args.save_total_limit\n",
      "{'loss': 0.8972, 'learning_rate': 9.200333690859255e-06, 'epoch': 24.49}        \n",
      " 82%|████████████████████████████▌      | 68500/83910 [3:43:15<45:10,  5.68it/s][INFO|trainer.py:722] 2023-07-17 18:41:05,437 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:41:05,438 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:41:05,438 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:41:05,438 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.15it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.77it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.92it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.86it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.08it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 37.71it/s]\u001b[A07/17/2023 18:41:06 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2529133558273315, 'eval_accuracy': 0.7453750420450723, 'eval_runtime': 0.8497, 'eval_samples_per_second': 588.462, 'eval_steps_per_second': 37.662, 'epoch': 24.49}\n",
      " 82%|████████████████████████████▌      | 68500/83910 [3:43:16<45:10,  5.68it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 37.71it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:41:06,293 >> Saving model checkpoint to roberta-large-movies/checkpoint-68500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:41:06,296 >> Configuration saved in roberta-large-movies/checkpoint-68500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:41:07,976 >> Model weights saved in roberta-large-movies/checkpoint-68500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:41:07,977 >> tokenizer config file saved in roberta-large-movies/checkpoint-68500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:41:07,977 >> Special tokens file saved in roberta-large-movies/checkpoint-68500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:41:11,610 >> Deleting older checkpoint [roberta-large-movies/checkpoint-67500] due to args.save_total_limit\n",
      "{'loss': 0.8863, 'learning_rate': 8.902395423668217e-06, 'epoch': 24.67}        \n",
      " 82%|████████████████████████████▊      | 69000/83910 [3:44:53<45:10,  5.50it/s][INFO|trainer.py:722] 2023-07-17 18:42:43,239 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:42:43,240 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:42:43,240 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:42:43,240 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.97it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.12it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.09it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 40.41it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 39.06it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 39.79it/s]\u001b[A07/17/2023 18:42:44 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3219196796417236, 'eval_accuracy': 0.7359154929577465, 'eval_runtime': 0.8149, 'eval_samples_per_second': 613.598, 'eval_steps_per_second': 39.27, 'epoch': 24.67}\n",
      " 82%|████████████████████████████▊      | 69000/83910 [3:44:53<45:10,  5.50it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 39.79it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:42:44,078 >> Saving model checkpoint to roberta-large-movies/checkpoint-69000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:42:44,079 >> Configuration saved in roberta-large-movies/checkpoint-69000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:42:45,768 >> Model weights saved in roberta-large-movies/checkpoint-69000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:42:45,769 >> tokenizer config file saved in roberta-large-movies/checkpoint-69000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:42:45,769 >> Special tokens file saved in roberta-large-movies/checkpoint-69000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:42:49,573 >> Deleting older checkpoint [roberta-large-movies/checkpoint-68000] due to args.save_total_limit\n",
      "{'loss': 0.8823, 'learning_rate': 8.604457156477178e-06, 'epoch': 24.85}        \n",
      " 83%|████████████████████████████▉      | 69500/83910 [3:46:31<44:08,  5.44it/s][INFO|trainer.py:722] 2023-07-17 18:44:21,317 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:44:21,319 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:44:21,319 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:44:21,319 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 44.86it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 38.14it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:00<00:00, 36.63it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:00<00:00, 38.84it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 39.49it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 40.50it/s]\u001b[A07/17/2023 18:44:22 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.313620924949646, 'eval_accuracy': 0.7367235275185066, 'eval_runtime': 0.8311, 'eval_samples_per_second': 601.621, 'eval_steps_per_second': 38.504, 'epoch': 24.85}\n",
      " 83%|████████████████████████████▉      | 69500/83910 [3:46:32<44:08,  5.44it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.50it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:44:22,151 >> Saving model checkpoint to roberta-large-movies/checkpoint-69500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:44:22,152 >> Configuration saved in roberta-large-movies/checkpoint-69500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:44:23,828 >> Model weights saved in roberta-large-movies/checkpoint-69500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:44:23,829 >> tokenizer config file saved in roberta-large-movies/checkpoint-69500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:44:23,829 >> Special tokens file saved in roberta-large-movies/checkpoint-69500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:44:27,595 >> Deleting older checkpoint [roberta-large-movies/checkpoint-68500] due to args.save_total_limit\n",
      "{'loss': 0.8759, 'learning_rate': 8.306518889286139e-06, 'epoch': 25.03}        \n",
      " 83%|█████████████████████████████▏     | 70000/83910 [3:48:08<42:22,  5.47it/s][INFO|trainer.py:722] 2023-07-17 18:45:59,001 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:45:59,003 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:45:59,003 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:45:59,003 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.19it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.49it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.57it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.62it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.82it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.27it/s]\u001b[A07/17/2023 18:45:59 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3151708841323853, 'eval_accuracy': 0.7427812811151676, 'eval_runtime': 0.7986, 'eval_samples_per_second': 626.093, 'eval_steps_per_second': 40.07, 'epoch': 25.03}\n",
      " 83%|█████████████████████████████▏     | 70000/83910 [3:48:09<42:22,  5.47it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.27it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:45:59,807 >> Saving model checkpoint to roberta-large-movies/checkpoint-70000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:45:59,812 >> Configuration saved in roberta-large-movies/checkpoint-70000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:46:01,515 >> Model weights saved in roberta-large-movies/checkpoint-70000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:46:01,516 >> tokenizer config file saved in roberta-large-movies/checkpoint-70000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:46:01,516 >> Special tokens file saved in roberta-large-movies/checkpoint-70000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:46:05,338 >> Deleting older checkpoint [roberta-large-movies/checkpoint-69000] due to args.save_total_limit\n",
      "{'loss': 0.8722, 'learning_rate': 8.008580622095102e-06, 'epoch': 25.21}        \n",
      " 84%|█████████████████████████████▍     | 70500/83910 [3:49:46<40:42,  5.49it/s][INFO|trainer.py:722] 2023-07-17 18:47:36,976 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:47:36,978 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:47:36,978 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:47:36,978 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 19%|████████▎                                   | 6/32 [00:00<00:00, 49.13it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 11/32 [00:00<00:00, 44.00it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 16/32 [00:00<00:00, 40.63it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 21/32 [00:00<00:00, 38.46it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 38.87it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 38.86it/s]\u001b[A07/17/2023 18:47:37 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3108021020889282, 'eval_accuracy': 0.7569644572526417, 'eval_runtime': 0.8281, 'eval_samples_per_second': 603.782, 'eval_steps_per_second': 38.642, 'epoch': 25.21}\n",
      " 84%|█████████████████████████████▍     | 70500/83910 [3:49:47<40:42,  5.49it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.86it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:47:37,807 >> Saving model checkpoint to roberta-large-movies/checkpoint-70500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:47:37,808 >> Configuration saved in roberta-large-movies/checkpoint-70500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:47:39,505 >> Model weights saved in roberta-large-movies/checkpoint-70500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:47:39,505 >> tokenizer config file saved in roberta-large-movies/checkpoint-70500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:47:39,506 >> Special tokens file saved in roberta-large-movies/checkpoint-70500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:47:43,218 >> Deleting older checkpoint [roberta-large-movies/checkpoint-69500] due to args.save_total_limit\n",
      "{'loss': 0.8548, 'learning_rate': 7.710642354904064e-06, 'epoch': 25.38}        \n",
      " 85%|█████████████████████████████▌     | 71000/83910 [3:51:24<38:13,  5.63it/s][INFO|trainer.py:722] 2023-07-17 18:49:14,305 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:49:14,306 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:49:14,307 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:49:14,307 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.17it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.77it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.16it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.13it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.50it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.49it/s]\u001b[A07/17/2023 18:49:15 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3503183126449585, 'eval_accuracy': 0.7367716008037508, 'eval_runtime': 0.7871, 'eval_samples_per_second': 635.233, 'eval_steps_per_second': 40.655, 'epoch': 25.38}\n",
      " 85%|█████████████████████████████▌     | 71000/83910 [3:51:24<38:13,  5.63it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.49it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:49:15,095 >> Saving model checkpoint to roberta-large-movies/checkpoint-71000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:49:15,096 >> Configuration saved in roberta-large-movies/checkpoint-71000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:49:16,874 >> Model weights saved in roberta-large-movies/checkpoint-71000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:49:16,875 >> tokenizer config file saved in roberta-large-movies/checkpoint-71000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:49:16,875 >> Special tokens file saved in roberta-large-movies/checkpoint-71000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:49:20,628 >> Deleting older checkpoint [roberta-large-movies/checkpoint-70000] due to args.save_total_limit\n",
      "{'loss': 0.8728, 'learning_rate': 7.412704087713027e-06, 'epoch': 25.56}        \n",
      " 85%|█████████████████████████████▊     | 71500/83910 [3:53:01<38:12,  5.41it/s][INFO|trainer.py:722] 2023-07-17 18:50:51,956 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:50:51,958 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:50:51,958 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:50:51,958 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.42it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.71it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.94it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.37it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.31it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 37.15it/s]\u001b[A07/17/2023 18:50:52 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3091211318969727, 'eval_accuracy': 0.7402768622280818, 'eval_runtime': 0.8581, 'eval_samples_per_second': 582.712, 'eval_steps_per_second': 37.294, 'epoch': 25.56}\n",
      " 85%|█████████████████████████████▊     | 71500/83910 [3:53:02<38:12,  5.41it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 37.15it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:50:52,820 >> Saving model checkpoint to roberta-large-movies/checkpoint-71500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:50:52,823 >> Configuration saved in roberta-large-movies/checkpoint-71500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:50:54,484 >> Model weights saved in roberta-large-movies/checkpoint-71500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:50:54,484 >> tokenizer config file saved in roberta-large-movies/checkpoint-71500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:50:54,484 >> Special tokens file saved in roberta-large-movies/checkpoint-71500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:50:58,310 >> Deleting older checkpoint [roberta-large-movies/checkpoint-70500] due to args.save_total_limit\n",
      "{'loss': 0.8633, 'learning_rate': 7.114765820521989e-06, 'epoch': 25.74}        \n",
      " 86%|██████████████████████████████     | 72000/83910 [3:54:39<35:19,  5.62it/s][INFO|trainer.py:722] 2023-07-17 18:52:29,870 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:52:29,871 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:52:29,872 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:52:29,872 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.17it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.37it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.79it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 39.65it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 36.54it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 36.57it/s]\u001b[A07/17/2023 18:52:30 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2952070236206055, 'eval_accuracy': 0.7416481069042317, 'eval_runtime': 0.8515, 'eval_samples_per_second': 587.213, 'eval_steps_per_second': 37.582, 'epoch': 25.74}\n",
      " 86%|██████████████████████████████     | 72000/83910 [3:54:40<35:19,  5.62it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 36.57it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:52:30,725 >> Saving model checkpoint to roberta-large-movies/checkpoint-72000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:52:30,726 >> Configuration saved in roberta-large-movies/checkpoint-72000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:52:32,438 >> Model weights saved in roberta-large-movies/checkpoint-72000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:52:32,439 >> tokenizer config file saved in roberta-large-movies/checkpoint-72000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:52:32,439 >> Special tokens file saved in roberta-large-movies/checkpoint-72000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:52:36,136 >> Deleting older checkpoint [roberta-large-movies/checkpoint-71000] due to args.save_total_limit\n",
      "{'loss': 0.8612, 'learning_rate': 6.816827553330949e-06, 'epoch': 25.92}        \n",
      " 86%|██████████████████████████████▏    | 72500/83910 [3:56:16<35:33,  5.35it/s][INFO|trainer.py:722] 2023-07-17 18:54:07,069 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:54:07,071 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:54:07,071 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:54:07,071 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.92it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.84it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.02it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.57it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.94it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.05it/s]\u001b[A07/17/2023 18:54:07 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.1612097024917603, 'eval_accuracy': 0.7719072164948454, 'eval_runtime': 0.7967, 'eval_samples_per_second': 627.618, 'eval_steps_per_second': 40.168, 'epoch': 25.92}\n",
      " 86%|██████████████████████████████▏    | 72500/83910 [3:56:17<35:33,  5.35it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.05it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:54:07,873 >> Saving model checkpoint to roberta-large-movies/checkpoint-72500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:54:07,874 >> Configuration saved in roberta-large-movies/checkpoint-72500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:54:09,566 >> Model weights saved in roberta-large-movies/checkpoint-72500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:54:09,567 >> tokenizer config file saved in roberta-large-movies/checkpoint-72500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:54:09,567 >> Special tokens file saved in roberta-large-movies/checkpoint-72500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:54:13,398 >> Deleting older checkpoint [roberta-large-movies/checkpoint-64500] due to args.save_total_limit\n",
      "{'loss': 0.8677, 'learning_rate': 6.5194851626742935e-06, 'epoch': 26.1}        \n",
      " 87%|██████████████████████████████▍    | 73000/83910 [3:57:54<32:54,  5.52it/s][INFO|trainer.py:722] 2023-07-17 18:55:44,791 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:55:44,793 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:55:44,793 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:55:44,793 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.63it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.65it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.70it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.60it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.78it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 39.23it/s]\u001b[A07/17/2023 18:55:45 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2855061292648315, 'eval_accuracy': 0.7449731903485255, 'eval_runtime': 0.8112, 'eval_samples_per_second': 616.391, 'eval_steps_per_second': 39.449, 'epoch': 26.1}\n",
      " 87%|██████████████████████████████▍    | 73000/83910 [3:57:55<32:54,  5.52it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 39.23it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:55:45,607 >> Saving model checkpoint to roberta-large-movies/checkpoint-73000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:55:45,610 >> Configuration saved in roberta-large-movies/checkpoint-73000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:55:47,260 >> Model weights saved in roberta-large-movies/checkpoint-73000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:55:47,261 >> tokenizer config file saved in roberta-large-movies/checkpoint-73000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:55:47,261 >> Special tokens file saved in roberta-large-movies/checkpoint-73000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:55:51,204 >> Deleting older checkpoint [roberta-large-movies/checkpoint-71500] due to args.save_total_limit\n",
      "{'loss': 0.8526, 'learning_rate': 6.2221427720176384e-06, 'epoch': 26.28}       \n",
      " 88%|██████████████████████████████▋    | 73500/83910 [3:59:33<31:33,  5.50it/s][INFO|trainer.py:722] 2023-07-17 18:57:23,170 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:57:23,172 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:57:23,172 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:57:23,172 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 37.16it/s]\u001b[A\n",
      " 28%|████████████▍                               | 9/32 [00:00<00:00, 37.07it/s]\u001b[A\n",
      " 41%|█████████████████▍                         | 13/32 [00:00<00:00, 37.29it/s]\u001b[A\n",
      " 56%|████████████████████████▏                  | 18/32 [00:00<00:00, 38.85it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 22/32 [00:00<00:00, 39.11it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 26/32 [00:00<00:00, 39.07it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 31/32 [00:00<00:00, 40.22it/s]\u001b[A07/17/2023 18:57:24 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.297914981842041, 'eval_accuracy': 0.7544929396662388, 'eval_runtime': 0.8472, 'eval_samples_per_second': 590.203, 'eval_steps_per_second': 37.773, 'epoch': 26.28}\n",
      " 88%|██████████████████████████████▋    | 73500/83910 [3:59:33<31:33,  5.50it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.22it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:57:24,021 >> Saving model checkpoint to roberta-large-movies/checkpoint-73500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:57:24,022 >> Configuration saved in roberta-large-movies/checkpoint-73500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:57:25,700 >> Model weights saved in roberta-large-movies/checkpoint-73500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:57:25,701 >> tokenizer config file saved in roberta-large-movies/checkpoint-73500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:57:25,701 >> Special tokens file saved in roberta-large-movies/checkpoint-73500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:57:29,470 >> Deleting older checkpoint [roberta-large-movies/checkpoint-72000] due to args.save_total_limit\n",
      "{'loss': 0.8594, 'learning_rate': 5.9242045048266e-06, 'epoch': 26.46}          \n",
      " 88%|██████████████████████████████▊    | 74000/83910 [4:01:10<30:06,  5.49it/s][INFO|trainer.py:722] 2023-07-17 18:59:00,419 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 18:59:00,421 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 18:59:00,421 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 18:59:00,421 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 47.77it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.76it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.05it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.26it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.31it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.28it/s]\u001b[A07/17/2023 18:59:01 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2569819688796997, 'eval_accuracy': 0.7598070739549839, 'eval_runtime': 0.7923, 'eval_samples_per_second': 631.066, 'eval_steps_per_second': 40.388, 'epoch': 26.46}\n",
      " 88%|██████████████████████████████▊    | 74000/83910 [4:01:11<30:06,  5.49it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.28it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 18:59:01,215 >> Saving model checkpoint to roberta-large-movies/checkpoint-74000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 18:59:01,216 >> Configuration saved in roberta-large-movies/checkpoint-74000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 18:59:02,889 >> Model weights saved in roberta-large-movies/checkpoint-74000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 18:59:02,890 >> tokenizer config file saved in roberta-large-movies/checkpoint-74000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 18:59:02,890 >> Special tokens file saved in roberta-large-movies/checkpoint-74000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 18:59:06,349 >> Deleting older checkpoint [roberta-large-movies/checkpoint-73000] due to args.save_total_limit\n",
      "{'loss': 0.8481, 'learning_rate': 5.626266237635562e-06, 'epoch': 26.64}        \n",
      " 89%|███████████████████████████████    | 74500/83910 [4:02:47<28:27,  5.51it/s][INFO|trainer.py:722] 2023-07-17 19:00:37,736 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 19:00:37,738 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 19:00:37,738 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 19:00:37,738 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.84it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.00it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 35.91it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:00<00:00, 34.68it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:00<00:00, 35.88it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 37.95it/s]\u001b[A07/17/2023 19:00:38 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2336714267730713, 'eval_accuracy': 0.7491992312620115, 'eval_runtime': 0.8668, 'eval_samples_per_second': 576.839, 'eval_steps_per_second': 36.918, 'epoch': 26.64}\n",
      " 89%|███████████████████████████████    | 74500/83910 [4:02:48<28:27,  5.51it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 37.95it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 19:00:38,606 >> Saving model checkpoint to roberta-large-movies/checkpoint-74500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 19:00:38,607 >> Configuration saved in roberta-large-movies/checkpoint-74500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 19:00:40,464 >> Model weights saved in roberta-large-movies/checkpoint-74500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 19:00:40,464 >> tokenizer config file saved in roberta-large-movies/checkpoint-74500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 19:00:40,465 >> Special tokens file saved in roberta-large-movies/checkpoint-74500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 19:00:44,212 >> Deleting older checkpoint [roberta-large-movies/checkpoint-73500] due to args.save_total_limit\n",
      "{'loss': 0.855, 'learning_rate': 5.3283279704445245e-06, 'epoch': 26.81}        \n",
      " 89%|███████████████████████████████▎   | 75000/83910 [4:04:25<28:27,  5.22it/s][INFO|trainer.py:722] 2023-07-17 19:02:15,286 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 19:02:15,287 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 19:02:15,287 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 19:02:15,287 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.29it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.72it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.91it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.97it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.22it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.39it/s]\u001b[A07/17/2023 19:02:16 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2874828577041626, 'eval_accuracy': 0.7443507588532884, 'eval_runtime': 0.7926, 'eval_samples_per_second': 630.803, 'eval_steps_per_second': 40.371, 'epoch': 26.81}\n",
      " 89%|███████████████████████████████▎   | 75000/83910 [4:04:25<28:27,  5.22it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.39it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 19:02:16,081 >> Saving model checkpoint to roberta-large-movies/checkpoint-75000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 19:02:16,082 >> Configuration saved in roberta-large-movies/checkpoint-75000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 19:02:17,736 >> Model weights saved in roberta-large-movies/checkpoint-75000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 19:02:17,737 >> tokenizer config file saved in roberta-large-movies/checkpoint-75000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 19:02:17,737 >> Special tokens file saved in roberta-large-movies/checkpoint-75000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 19:02:21,664 >> Deleting older checkpoint [roberta-large-movies/checkpoint-74000] due to args.save_total_limit\n",
      "{'loss': 0.835, 'learning_rate': 5.030389703253486e-06, 'epoch': 26.99}         \n",
      " 90%|███████████████████████████████▍   | 75500/83910 [4:06:03<24:41,  5.68it/s][INFO|trainer.py:722] 2023-07-17 19:03:53,205 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 19:03:53,206 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 19:03:53,206 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 19:03:53,206 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.68it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 42.48it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 39.33it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 40.03it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 39.73it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 40.37it/s]\u001b[A07/17/2023 19:03:54 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2270281314849854, 'eval_accuracy': 0.7584731819677526, 'eval_runtime': 0.8172, 'eval_samples_per_second': 611.826, 'eval_steps_per_second': 39.157, 'epoch': 26.99}\n",
      " 90%|███████████████████████████████▍   | 75500/83910 [4:06:03<24:41,  5.68it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.37it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 19:03:54,025 >> Saving model checkpoint to roberta-large-movies/checkpoint-75500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 19:03:54,026 >> Configuration saved in roberta-large-movies/checkpoint-75500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 19:03:55,714 >> Model weights saved in roberta-large-movies/checkpoint-75500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 19:03:55,715 >> tokenizer config file saved in roberta-large-movies/checkpoint-75500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 19:03:55,715 >> Special tokens file saved in roberta-large-movies/checkpoint-75500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 19:03:59,535 >> Deleting older checkpoint [roberta-large-movies/checkpoint-74500] due to args.save_total_limit\n",
      "{'loss': 0.8309, 'learning_rate': 4.732451436062448e-06, 'epoch': 27.17}        \n",
      " 91%|███████████████████████████████▋   | 76000/83910 [4:07:40<23:28,  5.62it/s][INFO|trainer.py:722] 2023-07-17 19:05:30,882 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 19:05:30,883 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 19:05:30,883 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 19:05:30,883 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.90it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.06it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.22it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.32it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 38.49it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 37.14it/s]\u001b[A07/17/2023 19:05:31 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2539992332458496, 'eval_accuracy': 0.7389322916666666, 'eval_runtime': 0.8357, 'eval_samples_per_second': 598.292, 'eval_steps_per_second': 38.291, 'epoch': 27.17}\n",
      " 91%|███████████████████████████████▋   | 76000/83910 [4:07:41<23:28,  5.62it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 37.14it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 19:05:31,720 >> Saving model checkpoint to roberta-large-movies/checkpoint-76000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 19:05:31,721 >> Configuration saved in roberta-large-movies/checkpoint-76000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 19:05:33,377 >> Model weights saved in roberta-large-movies/checkpoint-76000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 19:05:33,378 >> tokenizer config file saved in roberta-large-movies/checkpoint-76000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 19:05:33,378 >> Special tokens file saved in roberta-large-movies/checkpoint-76000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 19:05:37,217 >> Deleting older checkpoint [roberta-large-movies/checkpoint-75000] due to args.save_total_limit\n",
      "{'loss': 0.8326, 'learning_rate': 4.43451316887141e-06, 'epoch': 27.35}         \n",
      " 91%|███████████████████████████████▉   | 76500/83910 [4:09:18<22:37,  5.46it/s][INFO|trainer.py:722] 2023-07-17 19:07:08,367 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 19:07:08,368 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 19:07:08,368 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 19:07:08,368 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.02it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.87it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.70it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.67it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.07it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.17it/s]\u001b[A07/17/2023 19:07:09 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3610546588897705, 'eval_accuracy': 0.7374631268436578, 'eval_runtime': 0.7953, 'eval_samples_per_second': 628.676, 'eval_steps_per_second': 40.235, 'epoch': 27.35}\n",
      " 91%|███████████████████████████████▉   | 76500/83910 [4:09:19<22:37,  5.46it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.17it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 19:07:09,165 >> Saving model checkpoint to roberta-large-movies/checkpoint-76500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 19:07:09,166 >> Configuration saved in roberta-large-movies/checkpoint-76500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 19:07:10,901 >> Model weights saved in roberta-large-movies/checkpoint-76500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 19:07:10,902 >> tokenizer config file saved in roberta-large-movies/checkpoint-76500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 19:07:10,902 >> Special tokens file saved in roberta-large-movies/checkpoint-76500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 19:07:14,598 >> Deleting older checkpoint [roberta-large-movies/checkpoint-75500] due to args.save_total_limit\n",
      "{'loss': 0.8398, 'learning_rate': 4.136574901680372e-06, 'epoch': 27.53}        \n",
      " 92%|████████████████████████████████   | 77000/83910 [4:10:56<20:31,  5.61it/s][INFO|trainer.py:722] 2023-07-17 19:08:46,415 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 19:08:46,416 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 19:08:46,416 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 19:08:46,416 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 41.53it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 34.84it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 14/32 [00:00<00:00, 35.69it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:00<00:00, 37.88it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:00<00:00, 38.46it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 39.85it/s]\u001b[A07/17/2023 19:08:47 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2247506380081177, 'eval_accuracy': 0.7504918032786885, 'eval_runtime': 0.859, 'eval_samples_per_second': 582.099, 'eval_steps_per_second': 37.254, 'epoch': 27.53}\n",
      " 92%|████████████████████████████████   | 77000/83910 [4:10:57<20:31,  5.61it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 39.85it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 19:08:47,278 >> Saving model checkpoint to roberta-large-movies/checkpoint-77000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 19:08:47,279 >> Configuration saved in roberta-large-movies/checkpoint-77000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 19:08:48,876 >> Model weights saved in roberta-large-movies/checkpoint-77000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 19:08:48,877 >> tokenizer config file saved in roberta-large-movies/checkpoint-77000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 19:08:48,877 >> Special tokens file saved in roberta-large-movies/checkpoint-77000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 19:08:52,596 >> Deleting older checkpoint [roberta-large-movies/checkpoint-76000] due to args.save_total_limit\n",
      "{'loss': 0.8304, 'learning_rate': 3.838636634489334e-06, 'epoch': 27.71}        \n",
      " 92%|████████████████████████████████▎  | 77500/83910 [4:12:33<19:48,  5.40it/s][INFO|trainer.py:722] 2023-07-17 19:10:24,071 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 19:10:24,073 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 19:10:24,073 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 19:10:24,073 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.03it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.24it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.37it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 32.97it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 30.82it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 32.32it/s]\u001b[A07/17/2023 19:10:25 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2403171062469482, 'eval_accuracy': 0.7607282184655396, 'eval_runtime': 0.9471, 'eval_samples_per_second': 527.922, 'eval_steps_per_second': 33.787, 'epoch': 27.71}\n",
      " 92%|████████████████████████████████▎  | 77500/83910 [4:12:34<19:48,  5.40it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 32.32it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 19:10:25,022 >> Saving model checkpoint to roberta-large-movies/checkpoint-77500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 19:10:25,023 >> Configuration saved in roberta-large-movies/checkpoint-77500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 19:10:26,711 >> Model weights saved in roberta-large-movies/checkpoint-77500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 19:10:26,712 >> tokenizer config file saved in roberta-large-movies/checkpoint-77500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 19:10:26,712 >> Special tokens file saved in roberta-large-movies/checkpoint-77500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 19:10:30,382 >> Deleting older checkpoint [roberta-large-movies/checkpoint-76500] due to args.save_total_limit\n",
      "{'loss': 0.8373, 'learning_rate': 3.5406983672982957e-06, 'epoch': 27.89}       \n",
      " 93%|████████████████████████████████▌  | 78000/83910 [4:14:11<17:22,  5.67it/s][INFO|trainer.py:722] 2023-07-17 19:12:01,862 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 19:12:01,863 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 19:12:01,864 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 19:12:01,864 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.99it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.30it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.62it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.59it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.72it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 40.32it/s]\u001b[A07/17/2023 19:12:02 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.1708660125732422, 'eval_accuracy': 0.7611295681063123, 'eval_runtime': 0.8284, 'eval_samples_per_second': 603.609, 'eval_steps_per_second': 38.631, 'epoch': 27.89}\n",
      " 93%|████████████████████████████████▌  | 78000/83910 [4:14:12<17:22,  5.67it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.32it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 19:12:02,694 >> Saving model checkpoint to roberta-large-movies/checkpoint-78000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 19:12:02,696 >> Configuration saved in roberta-large-movies/checkpoint-78000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 19:12:04,419 >> Model weights saved in roberta-large-movies/checkpoint-78000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 19:12:04,420 >> tokenizer config file saved in roberta-large-movies/checkpoint-78000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 19:12:04,420 >> Special tokens file saved in roberta-large-movies/checkpoint-78000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 19:12:08,324 >> Deleting older checkpoint [roberta-large-movies/checkpoint-77000] due to args.save_total_limit\n",
      "{'loss': 0.8462, 'learning_rate': 3.2427601001072583e-06, 'epoch': 28.07}       \n",
      " 94%|████████████████████████████████▋  | 78500/83910 [4:15:49<16:13,  5.56it/s][INFO|trainer.py:722] 2023-07-17 19:13:39,574 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 19:13:39,575 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 19:13:39,576 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 19:13:39,576 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.19it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.92it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.49it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.93it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 37.49it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 36.68it/s]\u001b[A07/17/2023 19:13:40 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.289104700088501, 'eval_accuracy': 0.7508185985592666, 'eval_runtime': 0.8603, 'eval_samples_per_second': 581.16, 'eval_steps_per_second': 37.194, 'epoch': 28.07}\n",
      " 94%|████████████████████████████████▋  | 78500/83910 [4:15:50<16:13,  5.56it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 36.68it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 19:13:40,437 >> Saving model checkpoint to roberta-large-movies/checkpoint-78500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 19:13:40,439 >> Configuration saved in roberta-large-movies/checkpoint-78500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 19:13:42,229 >> Model weights saved in roberta-large-movies/checkpoint-78500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 19:13:42,230 >> tokenizer config file saved in roberta-large-movies/checkpoint-78500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 19:13:42,230 >> Special tokens file saved in roberta-large-movies/checkpoint-78500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 19:13:46,245 >> Deleting older checkpoint [roberta-large-movies/checkpoint-77500] due to args.save_total_limit\n",
      "{'loss': 0.8259, 'learning_rate': 2.945417709450602e-06, 'epoch': 28.24}        \n",
      " 94%|████████████████████████████████▉  | 79000/83910 [4:17:27<14:46,  5.54it/s][INFO|trainer.py:722] 2023-07-17 19:15:17,644 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 19:15:17,645 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 19:15:17,645 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 19:15:17,645 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.04it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.90it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.09it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.28it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.64it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.78it/s]\u001b[A07/17/2023 19:15:18 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2452012300491333, 'eval_accuracy': 0.7500814597588791, 'eval_runtime': 0.8046, 'eval_samples_per_second': 621.394, 'eval_steps_per_second': 39.769, 'epoch': 28.24}\n",
      " 94%|████████████████████████████████▉  | 79000/83910 [4:17:28<14:46,  5.54it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.78it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 19:15:18,463 >> Saving model checkpoint to roberta-large-movies/checkpoint-79000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 19:15:18,472 >> Configuration saved in roberta-large-movies/checkpoint-79000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 19:15:20,177 >> Model weights saved in roberta-large-movies/checkpoint-79000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 19:15:20,177 >> tokenizer config file saved in roberta-large-movies/checkpoint-79000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 19:15:20,178 >> Special tokens file saved in roberta-large-movies/checkpoint-79000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 19:15:23,911 >> Deleting older checkpoint [roberta-large-movies/checkpoint-78000] due to args.save_total_limit\n",
      "{'loss': 0.8334, 'learning_rate': 2.647479442259564e-06, 'epoch': 28.42}        \n",
      " 95%|█████████████████████████████████▏ | 79500/83910 [4:19:05<12:53,  5.70it/s][INFO|trainer.py:722] 2023-07-17 19:16:55,645 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 19:16:55,647 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 19:16:55,647 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 19:16:55,647 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 47.92it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.76it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 36.38it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:00<00:00, 33.85it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:00<00:00, 32.81it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▎      | 27/32 [00:00<00:00, 33.19it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 36.63it/s]\u001b[A07/17/2023 19:16:56 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2985996007919312, 'eval_accuracy': 0.746810598626104, 'eval_runtime': 0.9197, 'eval_samples_per_second': 543.676, 'eval_steps_per_second': 34.795, 'epoch': 28.42}\n",
      " 95%|█████████████████████████████████▏ | 79500/83910 [4:19:06<12:53,  5.70it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 36.63it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 19:16:56,568 >> Saving model checkpoint to roberta-large-movies/checkpoint-79500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 19:16:56,569 >> Configuration saved in roberta-large-movies/checkpoint-79500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 19:16:58,241 >> Model weights saved in roberta-large-movies/checkpoint-79500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 19:16:58,241 >> tokenizer config file saved in roberta-large-movies/checkpoint-79500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 19:16:58,242 >> Special tokens file saved in roberta-large-movies/checkpoint-79500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 19:17:02,055 >> Deleting older checkpoint [roberta-large-movies/checkpoint-78500] due to args.save_total_limit\n",
      "{'loss': 0.8115, 'learning_rate': 2.349541175068526e-06, 'epoch': 28.6}         \n",
      " 95%|█████████████████████████████████▎ | 80000/83910 [4:20:43<12:22,  5.27it/s][INFO|trainer.py:722] 2023-07-17 19:18:33,186 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 19:18:33,187 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 19:18:33,187 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 19:18:33,187 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.04it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.06it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.15it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.54it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 40.84it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 40.81it/s]\u001b[A07/17/2023 19:18:33 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2879589796066284, 'eval_accuracy': 0.7514638906961614, 'eval_runtime': 0.7986, 'eval_samples_per_second': 626.129, 'eval_steps_per_second': 40.072, 'epoch': 28.6}\n",
      " 95%|█████████████████████████████████▎ | 80000/83910 [4:20:43<12:22,  5.27it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.81it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 19:18:33,987 >> Saving model checkpoint to roberta-large-movies/checkpoint-80000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 19:18:33,988 >> Configuration saved in roberta-large-movies/checkpoint-80000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 19:18:35,855 >> Model weights saved in roberta-large-movies/checkpoint-80000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 19:18:35,856 >> tokenizer config file saved in roberta-large-movies/checkpoint-80000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 19:18:35,857 >> Special tokens file saved in roberta-large-movies/checkpoint-80000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 19:18:39,834 >> Deleting older checkpoint [roberta-large-movies/checkpoint-79000] due to args.save_total_limit\n",
      "{'loss': 0.8205, 'learning_rate': 2.0516029078774876e-06, 'epoch': 28.78}       \n",
      " 96%|█████████████████████████████████▌ | 80500/83910 [4:22:21<10:50,  5.24it/s][INFO|trainer.py:722] 2023-07-17 19:20:11,501 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 19:20:11,502 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 19:20:11,502 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 19:20:11,502 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.19it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.68it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 37.28it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:00<00:00, 34.27it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 23/32 [00:00<00:00, 35.84it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 37.78it/s]\u001b[A07/17/2023 19:20:12 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2727956771850586, 'eval_accuracy': 0.75615359369872, 'eval_runtime': 0.8652, 'eval_samples_per_second': 577.899, 'eval_steps_per_second': 36.986, 'epoch': 28.78}\n",
      " 96%|█████████████████████████████████▌ | 80500/83910 [4:22:22<10:50,  5.24it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 37.78it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 19:20:12,369 >> Saving model checkpoint to roberta-large-movies/checkpoint-80500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 19:20:12,370 >> Configuration saved in roberta-large-movies/checkpoint-80500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 19:20:14,038 >> Model weights saved in roberta-large-movies/checkpoint-80500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 19:20:14,039 >> tokenizer config file saved in roberta-large-movies/checkpoint-80500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 19:20:14,039 >> Special tokens file saved in roberta-large-movies/checkpoint-80500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 19:20:17,967 >> Deleting older checkpoint [roberta-large-movies/checkpoint-79500] due to args.save_total_limit\n",
      "{'loss': 0.8261, 'learning_rate': 1.7536646406864498e-06, 'epoch': 28.96}       \n",
      " 97%|█████████████████████████████████▊ | 81000/83910 [4:23:58<09:10,  5.28it/s][INFO|trainer.py:722] 2023-07-17 19:21:48,857 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 19:21:48,858 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 19:21:48,858 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 19:21:48,858 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.20it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.29it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.35it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.86it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.41it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.53it/s]\u001b[A07/17/2023 19:21:49 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2660555839538574, 'eval_accuracy': 0.7523561910952227, 'eval_runtime': 0.7893, 'eval_samples_per_second': 633.494, 'eval_steps_per_second': 40.544, 'epoch': 28.96}\n",
      " 97%|█████████████████████████████████▊ | 81000/83910 [4:23:59<09:10,  5.28it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.53it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 19:21:49,649 >> Saving model checkpoint to roberta-large-movies/checkpoint-81000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 19:21:49,650 >> Configuration saved in roberta-large-movies/checkpoint-81000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 19:21:51,372 >> Model weights saved in roberta-large-movies/checkpoint-81000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 19:21:51,373 >> tokenizer config file saved in roberta-large-movies/checkpoint-81000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 19:21:51,373 >> Special tokens file saved in roberta-large-movies/checkpoint-81000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 19:21:55,225 >> Deleting older checkpoint [roberta-large-movies/checkpoint-80000] due to args.save_total_limit\n",
      "{'loss': 0.8299, 'learning_rate': 1.4563222500297937e-06, 'epoch': 29.14}       \n",
      " 97%|█████████████████████████████████▉ | 81500/83910 [4:25:36<07:07,  5.64it/s][INFO|trainer.py:722] 2023-07-17 19:23:26,757 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 19:23:26,758 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 19:23:26,758 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 19:23:26,758 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.71it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.76it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 41.87it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 36.79it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 35.65it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 29/32 [00:00<00:00, 37.57it/s]\u001b[A07/17/2023 19:23:27 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.25924813747406, 'eval_accuracy': 0.7486106570774763, 'eval_runtime': 0.8513, 'eval_samples_per_second': 587.342, 'eval_steps_per_second': 37.59, 'epoch': 29.14}\n",
      " 97%|█████████████████████████████████▉ | 81500/83910 [4:25:37<07:07,  5.64it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 37.57it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 19:23:27,612 >> Saving model checkpoint to roberta-large-movies/checkpoint-81500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 19:23:27,614 >> Configuration saved in roberta-large-movies/checkpoint-81500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 19:23:29,306 >> Model weights saved in roberta-large-movies/checkpoint-81500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 19:23:29,307 >> tokenizer config file saved in roberta-large-movies/checkpoint-81500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 19:23:29,307 >> Special tokens file saved in roberta-large-movies/checkpoint-81500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 19:23:33,156 >> Deleting older checkpoint [roberta-large-movies/checkpoint-80500] due to args.save_total_limit\n",
      "{'loss': 0.8276, 'learning_rate': 1.1583839828387559e-06, 'epoch': 29.32}       \n",
      " 98%|██████████████████████████████████▏| 82000/83910 [4:27:14<05:52,  5.43it/s][INFO|trainer.py:722] 2023-07-17 19:25:04,922 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 19:25:04,924 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 19:25:04,924 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 19:25:04,924 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 48.75it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 44.57it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 36.85it/s]\u001b[A\n",
      " 59%|█████████████████████████▌                 | 19/32 [00:00<00:00, 35.63it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 24/32 [00:00<00:00, 37.16it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 28/32 [00:00<00:00, 37.96it/s]\u001b[A07/17/2023 19:25:05 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2325080633163452, 'eval_accuracy': 0.7529644268774703, 'eval_runtime': 0.8587, 'eval_samples_per_second': 582.291, 'eval_steps_per_second': 37.267, 'epoch': 29.32}\n",
      " 98%|██████████████████████████████████▏| 82000/83910 [4:27:15<05:52,  5.43it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 37.96it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 19:25:05,784 >> Saving model checkpoint to roberta-large-movies/checkpoint-82000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 19:25:05,785 >> Configuration saved in roberta-large-movies/checkpoint-82000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 19:25:07,521 >> Model weights saved in roberta-large-movies/checkpoint-82000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 19:25:07,521 >> tokenizer config file saved in roberta-large-movies/checkpoint-82000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 19:25:07,522 >> Special tokens file saved in roberta-large-movies/checkpoint-82000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 19:25:11,280 >> Deleting older checkpoint [roberta-large-movies/checkpoint-81000] due to args.save_total_limit\n",
      "{'loss': 0.8112, 'learning_rate': 8.604457156477178e-07, 'epoch': 29.5}         \n",
      " 98%|██████████████████████████████████▍| 82500/83910 [4:28:52<04:18,  5.46it/s][INFO|trainer.py:722] 2023-07-17 19:26:42,583 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 19:26:42,585 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 19:26:42,585 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 19:26:42,585 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.71it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.77it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 43.02it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 42.74it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 41.95it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 38.76it/s]\u001b[A07/17/2023 19:26:43 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3154096603393555, 'eval_accuracy': 0.7477890599410416, 'eval_runtime': 0.8166, 'eval_samples_per_second': 612.267, 'eval_steps_per_second': 39.185, 'epoch': 29.5}\n",
      " 98%|██████████████████████████████████▍| 82500/83910 [4:28:53<04:18,  5.46it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 38.76it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 19:26:43,408 >> Saving model checkpoint to roberta-large-movies/checkpoint-82500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 19:26:43,413 >> Configuration saved in roberta-large-movies/checkpoint-82500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 19:26:45,063 >> Model weights saved in roberta-large-movies/checkpoint-82500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 19:26:45,064 >> tokenizer config file saved in roberta-large-movies/checkpoint-82500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 19:26:45,064 >> Special tokens file saved in roberta-large-movies/checkpoint-82500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 19:26:48,775 >> Deleting older checkpoint [roberta-large-movies/checkpoint-81500] due to args.save_total_limit\n",
      "{'loss': 0.8111, 'learning_rate': 5.625074484566799e-07, 'epoch': 29.67}        \n",
      " 99%|██████████████████████████████████▌| 83000/83910 [4:30:30<02:48,  5.42it/s][INFO|trainer.py:722] 2023-07-17 19:28:20,344 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 19:28:20,345 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 19:28:20,345 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 19:28:20,346 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.36it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.07it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 42.26it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 41.26it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 39.77it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 40.04it/s]\u001b[A07/17/2023 19:28:21 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.3342524766921997, 'eval_accuracy': 0.740531561461794, 'eval_runtime': 0.8076, 'eval_samples_per_second': 619.083, 'eval_steps_per_second': 39.621, 'epoch': 29.67}\n",
      " 99%|██████████████████████████████████▌| 83000/83910 [4:30:31<02:48,  5.42it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.04it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 19:28:21,155 >> Saving model checkpoint to roberta-large-movies/checkpoint-83000\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 19:28:21,156 >> Configuration saved in roberta-large-movies/checkpoint-83000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 19:28:22,838 >> Model weights saved in roberta-large-movies/checkpoint-83000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 19:28:22,838 >> tokenizer config file saved in roberta-large-movies/checkpoint-83000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 19:28:22,839 >> Special tokens file saved in roberta-large-movies/checkpoint-83000/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 19:28:26,664 >> Deleting older checkpoint [roberta-large-movies/checkpoint-82000] due to args.save_total_limit\n",
      "{'loss': 0.8148, 'learning_rate': 2.645691812656418e-07, 'epoch': 29.85}        \n",
      "100%|██████████████████████████████████▊| 83500/83910 [4:32:08<01:13,  5.55it/s][INFO|trainer.py:722] 2023-07-17 19:29:58,287 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 19:29:58,289 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 19:29:58,289 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 19:29:58,289 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|██████▉                                     | 5/32 [00:00<00:00, 49.24it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 10/32 [00:00<00:00, 45.10it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 15/32 [00:00<00:00, 39.63it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 20/32 [00:00<00:00, 39.64it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 25/32 [00:00<00:00, 39.92it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 40.67it/s]\u001b[A07/17/2023 19:29:59 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 1.2806158065795898, 'eval_accuracy': 0.7484622855292975, 'eval_runtime': 0.8122, 'eval_samples_per_second': 615.596, 'eval_steps_per_second': 39.398, 'epoch': 29.85}\n",
      "100%|██████████████████████████████████▊| 83500/83910 [4:32:08<01:13,  5.55it/s]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 40.67it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2640] 2023-07-17 19:29:59,112 >> Saving model checkpoint to roberta-large-movies/checkpoint-83500\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 19:29:59,129 >> Configuration saved in roberta-large-movies/checkpoint-83500/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 19:30:00,766 >> Model weights saved in roberta-large-movies/checkpoint-83500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 19:30:00,767 >> tokenizer config file saved in roberta-large-movies/checkpoint-83500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 19:30:00,767 >> Special tokens file saved in roberta-large-movies/checkpoint-83500/special_tokens_map.json\n",
      "[INFO|trainer.py:2718] 2023-07-17 19:30:04,532 >> Deleting older checkpoint [roberta-large-movies/checkpoint-82500] due to args.save_total_limit\n",
      "100%|███████████████████████████████████| 83910/83910 [4:33:29<00:00,  5.74it/s][INFO|trainer.py:1850] 2023-07-17 19:31:19,212 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:1942] 2023-07-17 19:31:19,212 >> Loading best model from roberta-large-movies/checkpoint-72500 (score: 0.7719072164948454).\n",
      "{'train_runtime': 16410.0948, 'train_samples_per_second': 163.619, 'train_steps_per_second': 5.113, 'train_loss': 1.1746184680817338, 'epoch': 30.0}\n",
      "100%|███████████████████████████████████| 83910/83910 [4:33:30<00:00,  5.11it/s]\n",
      "[INFO|trainer.py:2640] 2023-07-17 19:31:20,199 >> Saving model checkpoint to roberta-large-movies\n",
      "[INFO|configuration_utils.py:451] 2023-07-17 19:31:20,200 >> Configuration saved in roberta-large-movies/config.json\n",
      "[INFO|modeling_utils.py:1566] 2023-07-17 19:31:21,914 >> Model weights saved in roberta-large-movies/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2023-07-17 19:31:21,915 >> tokenizer config file saved in roberta-large-movies/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2023-07-17 19:31:21,916 >> Special tokens file saved in roberta-large-movies/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       30.0\n",
      "  train_loss               =     1.1746\n",
      "  train_runtime            = 4:33:30.09\n",
      "  train_samples            =      89500\n",
      "  train_samples_per_second =    163.619\n",
      "  train_steps_per_second   =      5.113\n",
      "07/17/2023 19:31:22 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:722] 2023-07-17 19:31:22,016 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2891] 2023-07-17 19:31:22,018 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2893] 2023-07-17 19:31:22,018 >>   Num examples = 500\n",
      "[INFO|trainer.py:2896] 2023-07-17 19:31:22,018 >>   Batch size = 16\n",
      " 94%|████████████████████████████████████████▎  | 30/32 [00:00<00:00, 41.67it/s]07/17/2023 19:31:22 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "100%|███████████████████████████████████████████| 32/32 [00:00<00:00, 41.85it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =       30.0\n",
      "  eval_accuracy           =     0.7375\n",
      "  eval_loss               =     1.3261\n",
      "  eval_runtime            = 0:00:00.78\n",
      "  eval_samples            =        500\n",
      "  eval_samples_per_second =    635.414\n",
      "  eval_steps_per_second   =     40.666\n",
      "  perplexity              =     3.7663\n",
      "[INFO|modelcard.py:467] 2023-07-17 19:31:22,937 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Masked Language Modeling', 'type': 'fill-mask'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.737508327781479}]}\n"
     ]
    }
   ],
   "source": [
    "!python run_mlm.py \\\n",
    "    --model_name_or_path roberta-large \\\n",
    "    --train_file data/movies_text_tr.txt \\\n",
    "    --validation_file data/movies_text_dev.txt \\\n",
    "    --per_device_train_batch_size 32 \\\n",
    "    --per_device_eval_batch_size 16 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --fp16 True \\\n",
    "    --max_seq_len 128 \\\n",
    "    --num_train_epochs 30\\\n",
    "    --line_by_line True \\\n",
    "    --pad_to_max_length False \\\n",
    "    --evaluation_strategy 'steps' \\\n",
    "    --metric_for_best_model 'eval_accuracy' \\\n",
    "    --save_total_limit 3 \\\n",
    "    --logging_steps 500 \\\n",
    "    --eval_steps 500 \\\n",
    "    --load_best_model_at_end True \\\n",
    "    --output_dir 'roberta-large-movies'\\\n",
    "    --overwrite_output_dir True \\\n",
    "    \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1835fe-e090-4f20-91fc-ef73d1602868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df821a0-b9e7-4ccf-8b91-f76709060f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
